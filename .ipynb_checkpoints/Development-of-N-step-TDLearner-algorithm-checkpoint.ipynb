{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development of N-step TD learner algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from platform import python_version\n",
    "python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on Chapter 7: n-step Bootstrapping from Sutton & Barto\n",
    "\n",
    "### n-step TD\n",
    "\n",
    "<IMG SRC=\"tdlambda.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gamelearner import GameController, RandomPlayer, TDLearner\n",
    "from randomwalk import RandomWalkGame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 7.1 Random walk\n",
    "\n",
    "For verification purposes, we want to replicate the results below which should match Figure 7.2 from the book which looks like this:\n",
    "\n",
    "<IMG SRC=\"images/fig_7_2.png\">    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Official code from authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "1 2\n",
      "2 3\n",
      "3 4\n",
      "4 5\n",
      "0 1\n",
      "1 2\n",
      "2 3\n",
      "3 4\n",
      "4 5\n"
     ]
    }
   ],
   "source": [
    "steps = [1, 2, 3, 4, 5]\n",
    "for step_ind, step in zip(range(len(steps)), steps):\n",
    "    print(step_ind, step)\n",
    "for step_ind, step in enumerate(steps):\n",
    "    print(step_ind, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016-2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)             #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from tqdm import tqdm\n",
    "\n",
    "# all states\n",
    "N_STATES = 19\n",
    "\n",
    "# discount\n",
    "GAMMA = 1\n",
    "\n",
    "# all states but terminal states\n",
    "STATES = np.arange(1, N_STATES + 1)\n",
    "\n",
    "# start from the middle state\n",
    "START_STATE = 10\n",
    "\n",
    "# two terminal states\n",
    "# an action leading to the left terminal state has reward -1\n",
    "# an action leading to the right terminal state has reward 1\n",
    "END_STATES = [0, N_STATES + 1]\n",
    "\n",
    "# true state value from bellman equation\n",
    "TRUE_VALUE = np.arange(-20, 22, 2) / 20.0\n",
    "TRUE_VALUE[0] = TRUE_VALUE[-1] = 0\n",
    "\n",
    "# n-steps TD method\n",
    "# @value: values for each state, will be updated\n",
    "# @n: # of steps\n",
    "# @alpha: # step size\n",
    "def temporal_difference(value, n, alpha):\n",
    "    # initial starting state\n",
    "    state = START_STATE\n",
    "\n",
    "    # arrays to store states and rewards for an episode\n",
    "    # space isn't a major consideration, so I didn't use the mod trick\n",
    "    states = [state]\n",
    "    rewards = [0]\n",
    "\n",
    "    # track the time\n",
    "    time = 0\n",
    "\n",
    "    # the length of this episode\n",
    "    T = float('inf')\n",
    "    while True:\n",
    "        # go to next time step\n",
    "        time += 1\n",
    "\n",
    "        if time < T:\n",
    "            # choose an action randomly\n",
    "            if np.random.binomial(1, 0.5) == 1:\n",
    "                next_state = state + 1\n",
    "            else:\n",
    "                next_state = state - 1\n",
    "\n",
    "            if next_state == 0:\n",
    "                reward = -1\n",
    "            elif next_state == 20:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = 0\n",
    "\n",
    "            # store new state and new reward\n",
    "            states.append(next_state)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            if next_state in END_STATES:\n",
    "                T = time\n",
    "\n",
    "        # get the time of the state to update\n",
    "        update_time = time - n\n",
    "        if update_time >= 0:\n",
    "            returns = 0.0\n",
    "            # calculate corresponding rewards\n",
    "            for t in range(update_time + 1, min(T, update_time + n) + 1):\n",
    "                returns += pow(GAMMA, t - update_time - 1) * rewards[t]\n",
    "            # add state value to the return\n",
    "            if update_time + n <= T:\n",
    "                returns += pow(GAMMA, n) * value[states[(update_time + n)]]\n",
    "            state_to_update = states[update_time]\n",
    "            # update the state value\n",
    "            if not state_to_update in END_STATES:\n",
    "                value[state_to_update] += alpha * (returns - value[state_to_update])\n",
    "        if update_time == T - 1:\n",
    "            break\n",
    "        state = next_state\n",
    "\n",
    "# Figure 7.2, it will take quite a while\n",
    "def figure7_2():\n",
    "    # all possible steps\n",
    "    steps = np.power(2, np.arange(0, 10))\n",
    "\n",
    "    # all possible alphas\n",
    "    alphas = np.arange(0, 1.1, 0.1)\n",
    "\n",
    "    # each run has 10 episodes\n",
    "    episodes = 10\n",
    "\n",
    "    # perform 100 independent runs\n",
    "    runs = 100\n",
    "\n",
    "    # track the errors for each (step, alpha) combination\n",
    "    errors = np.zeros((len(steps), len(alphas)))\n",
    "    for run in tqdm(range(0, runs)):\n",
    "        for step_ind, step in zip(range(len(steps)), steps):\n",
    "            for alpha_ind, alpha in zip(range(len(alphas)), alphas):\n",
    "                # print('run:', run, 'step:', step, 'alpha:', alpha)\n",
    "                value = np.zeros(N_STATES + 2)\n",
    "                for ep in range(0, episodes):\n",
    "                    temporal_difference(value, step, alpha)\n",
    "                    # calculate the RMS error\n",
    "                    errors[step_ind, alpha_ind] += np.sqrt(np.sum(np.power(value - TRUE_VALUE, 2)) / N_STATES)\n",
    "    # take average\n",
    "    errors /= episodes * runs\n",
    "\n",
    "    for i in range(0, len(steps)):\n",
    "        plt.plot(alphas, errors[i, :], label='n = %d' % (steps[i]))\n",
    "    plt.xlabel('alpha')\n",
    "    plt.ylabel('RMS error')\n",
    "    plt.ylim([0.25, 0.55])\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig('images/figure_7_2.png')\n",
    "    plt.close()\n",
    "\n",
    "# Uncomment this to re-generate the image below\n",
    "#figure7_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<IMG SRC=\"images/figure_7_2.png\">   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMS Error Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5477225575051662"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From official code above\n",
    "value = np.zeros(N_STATES + 2)\n",
    "np.sqrt(np.sum(np.power(value - TRUE_VALUE, 2)) / N_STATES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1  0.   0.1  0.2  0.3  0.4\n",
      "  0.5  0.6  0.7  0.8  0.9]\n"
     ]
    }
   ],
   "source": [
    "def calculate_true_values(game):\n",
    "    \"\"\"Returns a list of the true values of states in a\n",
    "    RandomWalk game.\n",
    "    \"\"\"\n",
    "\n",
    "    xp = [0, game.size+1]\n",
    "    fp = [-1.0, 1.0]\n",
    "\n",
    "    true_values = np.interp(np.arange(game.size + 2), xp, fp)[1:-1]\n",
    "    \n",
    "    return true_values\n",
    "\n",
    "\n",
    "def calculate_rms_error(values, true_values):\n",
    "    \"\"\"Root-mean-squared error of values compared to true values.\n",
    "    \"\"\"\n",
    "\n",
    "    return np.sqrt(np.sum((np.array(values) - \n",
    "                           np.array(true_values))**2)/len(values))\n",
    "\n",
    "size = 19\n",
    "game = RandomWalkGame(size=size, \n",
    "                      terminal_rewards={'T1': -1.0, 'T2': 1.0})\n",
    "true_values = calculate_true_values(game)\n",
    "\n",
    "values = np.zeros(19)\n",
    "error = calculate_rms_error(values, true_values)\n",
    "\n",
    "assert error == 0.5477225575051662  # See benchmark code above\n",
    "\n",
    "print(true_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.9, -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2, -0.1,  0. ,  0.1,\n",
       "        0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0. , -0.9, -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2, -0.1,  0. ,\n",
       "        0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  0. ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRUE_VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter number of repetitions (1-100). Warning: 100 repetitions takes about 7 mins\n",
      "Press enter for default (1):1\n",
      "Execution time: 9.2s\n",
      "Produced results for n = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n"
     ]
    }
   ],
   "source": [
    "# Allow user to set number of repetitions (default 1)\n",
    "print(\"Enter number of repetitions (1-100). Warning: 100 repetitions takes about 7 mins\")\n",
    "default_input = '1'\n",
    "message = input(f\"Press enter for default ({default_input}):\")\n",
    "\n",
    "try:\n",
    "    repetitions = int(message or default_input)\n",
    "except ValueError as err:\n",
    "    print(\"Didn't understand that. Please try again.\")\n",
    "    raise err\n",
    "\n",
    "# Environment parameters\n",
    "size = 19\n",
    "terminal_rewards = {'T1': -1.0, 'T2': 1.0}\n",
    "game = RandomWalkGame(size=size, terminal_rewards=terminal_rewards)\n",
    "n_iters = 10\n",
    "\n",
    "# Agent parameters\n",
    "initial_value = 0.0\n",
    "gamma = 1.0\n",
    "\n",
    "# Iterate over learning_rates and n-values\n",
    "learning_rates = np.logspace(-2, 0.0, 50)\n",
    "n_values = [2**i for i in range(10)]\n",
    "\n",
    "avg_rms_error_sets = {}\n",
    "\n",
    "t0 = time.time()\n",
    "for n in n_values:\n",
    "\n",
    "    # TEMPORARY OVERRIDE FOR TESTING\n",
    "    #n = 10\n",
    "    rms_error_sets = []\n",
    "\n",
    "    rng1 = random.Random(5)\n",
    "\n",
    "    for repetition in range(repetitions):\n",
    "    \n",
    "        rms_errors = []\n",
    "        seed = repetition\n",
    "\n",
    "        for learning_rate in learning_rates:\n",
    "\n",
    "            # Initialise value function\n",
    "            value = np.zeros(size + 2)\n",
    "            \n",
    "            for iteration in range(n_iters):\n",
    "                temporal_difference(value, n, learning_rate)\n",
    "\n",
    "            rms_errors.append(calculate_rms_error(value[1:size+1], true_values))\n",
    "            #break\n",
    "        \n",
    "        rms_error_sets.append(rms_errors)\n",
    "\n",
    "    avg_rms_error_sets[n] = np.vstack(rms_error_sets).mean(axis=0)\n",
    "    \n",
    "    # OVERRIDE HERE\n",
    "    #break\n",
    "\n",
    "avg_rms_error_sets_master = avg_rms_error_sets.copy()\n",
    "\n",
    "t1 = time.time()\n",
    "total_time = t1 - t0\n",
    "print(f\"Execution time: {total_time:.1f}s\")\n",
    "\n",
    "print(f\"Produced results for n = {list(avg_rms_error_sets.keys())}\")\n",
    "#print(n, rms_results[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/billtubbs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Plot results\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for n, rms_errors in avg_rms_error_sets.items():\n",
    "    ax.plot(learning_rates, rms_errors, label=f'n = {n:d}')\n",
    "plt.title('n-step TD value error on random walk (%d states)' % game.size)\n",
    "ax.set_xlabel('Learning rate (alpha)')\n",
    "ax.set_ylabel('RMS Error')\n",
    "ax.set_ylim(0.0, 0.55)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(f\"random-walk-n-step-td-{game.size}-{repetitions}.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter number of repetitions (1-100). Warning: 100 repetitions takes about 7 mins\n",
      "Press enter for default (1):1\n",
      "Execution time: 9.2s\n",
      "Produced results for n = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n"
     ]
    }
   ],
   "source": [
    "# Allow user to set number of repetitions (default 1)\n",
    "print(\"Enter number of repetitions (1-100). Warning: 100 repetitions takes about 7 mins\")\n",
    "default_input = '1'\n",
    "message = input(f\"Press enter for default ({default_input}):\")\n",
    "\n",
    "try:\n",
    "    repetitions = int(message or default_input)\n",
    "except ValueError as err:\n",
    "    print(\"Didn't understand that. Please try again.\")\n",
    "    raise err\n",
    "\n",
    "# Environment parameters\n",
    "size = 19\n",
    "terminal_rewards = {'T1': -1.0, 'T2': 1.0}\n",
    "game = RandomWalkGame(size=size, terminal_rewards=terminal_rewards)\n",
    "n_iters = 10\n",
    "\n",
    "# Agent parameters\n",
    "initial_value = 0.0\n",
    "gamma = 1.0\n",
    "\n",
    "# Iterate over learning_rates and n-values\n",
    "learning_rates = np.logspace(-2, 0.0, 50)\n",
    "n_values = [2**i for i in range(10)]\n",
    "\n",
    "avg_rms_error_sets = {}\n",
    "\n",
    "t0 = time.time()\n",
    "for n in n_values:\n",
    "\n",
    "    # TEMPORARY OVERRIDE FOR TESTING\n",
    "    #n = 10\n",
    "    rms_error_sets = []\n",
    "\n",
    "    rng1 = random.Random(5)\n",
    "\n",
    "    for repetition in range(repetitions):\n",
    "    \n",
    "        rms_errors = []\n",
    "        seed = repetition\n",
    "\n",
    "        for learning_rate in learning_rates:\n",
    "\n",
    "            # Initialise value function\n",
    "            value_function = {\n",
    "                s: 0.0 if s in game.terminal_states else initial_value\n",
    "                for s in game.states\n",
    "            }\n",
    "\n",
    "            rng = random.Random(seed)\n",
    "\n",
    "            for iteration in range(n_iters):\n",
    "\n",
    "                past_states = [game.state]\n",
    "                past_rewards = [None]\n",
    "\n",
    "                T = 999999\n",
    "                t = 0\n",
    "                while True:\n",
    "\n",
    "                    # Behaviour policy\n",
    "                    if t < T:\n",
    "                        move = rng.choice(game.available_moves())\n",
    "                        game.make_move([1, move])\n",
    "                        past_states.append(game.state)\n",
    "                        if not game.game_over:\n",
    "                            reward = game.get_rewards()[1]\n",
    "                        else:\n",
    "                            reward = game.get_terminal_rewards()[1]\n",
    "                        past_rewards.append(reward)\n",
    "                        if game.game_over:\n",
    "                            T = t + 1\n",
    "\n",
    "                    # State to be updated\n",
    "                    tau = t - n + 1\n",
    "\n",
    "                    # TD(λ) update\n",
    "                    if tau >= 0:\n",
    "\n",
    "                        g = 0\n",
    "                        for i in range(tau + 1, min(tau + n, T) + 1):\n",
    "                            g += gamma**(i - tau - 1)*past_rewards[i]\n",
    "                        if tau + n < T:\n",
    "                            g += gamma**n*value_function[past_states[tau + n]]\n",
    "\n",
    "                        value_function[past_states[tau]] += \\\n",
    "                                    learning_rate*(g - value_function[past_states[tau]])\n",
    "\n",
    "                    t += 1\n",
    "                    if tau == T - 1:\n",
    "                        break\n",
    "\n",
    "                game.reset()\n",
    "                #break\n",
    "\n",
    "            values = np.array(list(value_function.values())[1:size+1])\n",
    "            rms_errors.append(calculate_rms_error(values, true_values))\n",
    "            #break\n",
    "        \n",
    "        rms_error_sets.append(rms_errors)\n",
    "\n",
    "    avg_rms_error_sets[n] = np.vstack(rms_error_sets).mean(axis=0)\n",
    "    \n",
    "    # OVERRIDE HERE\n",
    "    #break\n",
    "\n",
    "avg_rms_error_sets_master = avg_rms_error_sets.copy()\n",
    "\n",
    "t1 = time.time()\n",
    "total_time = t1 - t0\n",
    "print(f\"Execution time: {total_time:.1f}s\")\n",
    "\n",
    "print(f\"Produced results for n = {list(avg_rms_error_sets.keys())}\")\n",
    "#print(n, rms_results[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/billtubbs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Plot results\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for n, rms_errors in avg_rms_error_sets.items():\n",
    "    ax.plot(learning_rates, rms_errors, label=f'n = {n:d}')\n",
    "plt.title('n-step TD value error on random walk (%d states)' % game.size)\n",
    "ax.set_xlabel('Learning rate (alpha)')\n",
    "ax.set_ylabel('RMS Error')\n",
    "ax.set_ylim(0.0, 0.55)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(f\"random-walk-n-step-td-{game.size}-{repetitions}.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this dictionary to accumulate different results\n",
    "rms_errors = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(('n', 10),\n",
       "  ('learning_rate', 0.01),\n",
       "  ('gamma', 1.0),\n",
       "  ('n_iters', 10)): 0.49279075198489264}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Benchmark code\n",
    "\n",
    "size = 19\n",
    "terminal_rewards = {'T1': -1.0, 'T2': 1.0}\n",
    "\n",
    "game = RandomWalkGame(size=size, terminal_rewards=terminal_rewards)\n",
    "\n",
    "# Initialization of parameters\n",
    "initial_value = 0.0\n",
    "gamma = 1.0\n",
    "n = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Initialise value function\n",
    "value_function = {\n",
    "    s: 0.0 if s in game.terminal_states else initial_value\n",
    "    for s in game.states\n",
    "}\n",
    "\n",
    "rng = random.Random(5)\n",
    "\n",
    "n_iters = 10\n",
    "for iteration in range(n_iters):\n",
    "\n",
    "    past_states = [game.state]\n",
    "    past_rewards = [None]\n",
    "\n",
    "    T = 999999\n",
    "    t = 0\n",
    "    while True:\n",
    "\n",
    "        # Behaviour policy\n",
    "        if t < T:\n",
    "            move = rng.choice(game.available_moves())\n",
    "            game.make_move([1, move])\n",
    "            past_states.append(game.state)\n",
    "            if not game.game_over:\n",
    "                reward = game.get_rewards()[1]\n",
    "            else:\n",
    "                reward = game.get_terminal_rewards()[1]\n",
    "            past_rewards.append(reward)\n",
    "            if game.game_over:\n",
    "                T = t + 1\n",
    "\n",
    "        # State to be updated\n",
    "        tau = t - n + 1\n",
    "\n",
    "        # TD(λ) update\n",
    "        if tau >= 0:\n",
    "\n",
    "            g = 0\n",
    "            for i in range(tau + 1, min(tau + n, T) + 1):\n",
    "                g += gamma**(i - tau - 1)*past_rewards[i]\n",
    "            if tau + n < T:\n",
    "                g += gamma**n*value_function[past_states[tau + n]]\n",
    "\n",
    "            value_function[past_states[tau]] += \\\n",
    "                        learning_rate*(g - value_function[past_states[tau]])\n",
    "\n",
    "        t += 1\n",
    "        if tau == T - 1:\n",
    "            break\n",
    "\n",
    "    game.reset()\n",
    "    #break\n",
    "\n",
    "values = np.array(list(value_function.values())[1:size+1])\n",
    "rms_error = calculate_rms_error(values, true_values)\n",
    "\n",
    "# Save results in this dictionary\n",
    "params = {\n",
    "    'n': n,\n",
    "    'learning_rate': learning_rate,\n",
    "    'gamma': gamma,\n",
    "    'n_iters': n_iters\n",
    "}\n",
    "rms_errors[tuple(params.items())] = rms_error\n",
    "\n",
    "rms_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check correct result produced\n",
    "assert rms_errors[\n",
    "    (('n', 10), \n",
    "     ('learning_rate', 0.01), \n",
    "     ('gamma', 1.0), \n",
    "     ('n_iters', 10))] == 0.49279075198489264"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06793465, -0.07726219, -0.06760227, -0.07391731, -0.07595744,\n",
       "       -0.06291249, -0.02331332, -0.02287681, -0.0063066 ,  0.00689905,\n",
       "        0.01194015,  0.01518813,  0.02818066,  0.05353457,  0.05502057,\n",
       "        0.07147919,  0.07271629,  0.08409261,  0.06737659])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use this benchmark code\n",
    "\n",
    "Source: https://github.com/ShangtongZhang/reinforcement-learning-an-introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all states\n",
    "N_STATES = 19\n",
    "\n",
    "# all states but terminal states\n",
    "STATES = np.arange(1, N_STATES + 1)\n",
    "\n",
    "# start from the middle state\n",
    "START_STATE = 10\n",
    "\n",
    "# two terminal states\n",
    "# an action leading to the left terminal state has reward -1\n",
    "# an action leading to the right terminal state has reward 1\n",
    "END_STATES = [0, N_STATES + 1]\n",
    "\n",
    "# true state value from bellman equation\n",
    "TRUE_VALUE = np.arange(-20, 22, 2) / 20.0\n",
    "TRUE_VALUE[0] = TRUE_VALUE[-1] = 0\n",
    "\n",
    "def temporal_difference_test(actions, value, n, alpha, gamma):\n",
    "    # initial starting state\n",
    "    state = START_STATE\n",
    "\n",
    "    # arrays to store states and rewards for an episode\n",
    "    # space isn't a major consideration, so I didn't use the mod trick\n",
    "    states = [state]\n",
    "    rewards = [0]\n",
    "\n",
    "    # track the time\n",
    "    time = 0\n",
    "\n",
    "    # the length of this episode\n",
    "    T = float('inf')\n",
    "    while True:\n",
    "        # go to next time step\n",
    "        time += 1\n",
    "\n",
    "        if time < T:\n",
    "            # take next action from list\n",
    "            action = actions[time - 1]\n",
    "            assert action in [-1, 1], \"Actions must be -1 or +1\"\n",
    "            next_state = state + action\n",
    "\n",
    "            if next_state == 0:\n",
    "                reward = -1\n",
    "            elif next_state == 20:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = 0\n",
    "\n",
    "            # store new state and new reward\n",
    "            states.append(next_state)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            if next_state in END_STATES:\n",
    "                T = time\n",
    "\n",
    "        # get the time of the state to update\n",
    "        update_time = time - n\n",
    "        if update_time >= 0:\n",
    "            returns = 0.0\n",
    "            # calculate corresponding rewards\n",
    "            for t in range(update_time + 1, min(T, update_time + n) + 1):\n",
    "                returns += pow(gamma, t - update_time - 1) * rewards[t]\n",
    "            # add state value to the return\n",
    "            if update_time + n <= T:\n",
    "                returns += pow(gamma, n) * value[states[(update_time + n)]]\n",
    "            state_to_update = states[update_time]\n",
    "            # update the state value\n",
    "            if not state_to_update in END_STATES:\n",
    "                value[state_to_update] += alpha * (returns - value[state_to_update])\n",
    "        if update_time == T - 1:\n",
    "            break\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0. , -0.9, -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2, -0.1,  0. ,\n",
       "        0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  0. ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRUE_VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5477225575051662"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = np.zeros(21)\n",
    "\n",
    "np.sqrt(np.sum(np.power(value - TRUE_VALUE, 2)) / N_STATES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-step TD - Classes for adding to game-learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "def discounted_sequence_generator(sequence, gamma):\n",
    "    \"\"\"Returns a generator that yields each item of sequence\n",
    "    discounted by gamma at each time-step.\n",
    "    \n",
    "    Example:\n",
    "    >>> list(discounted_sequence_generator([10.0]*5, 0.75))\n",
    "    [10.0, 7.5, 5.625, 4.21875, 3.1640625]\n",
    "    \"\"\"\n",
    "    \n",
    "    x = 1.0\n",
    "    for item in sequence:\n",
    "        yield x*item\n",
    "        x = x*gamma\n",
    "\n",
    "\n",
    "# Initialization of agent\n",
    "class Agent:\n",
    "    \n",
    "    def __init__(self, policy, name=None, initial_value=0.0, n=1,\n",
    "                 gamma=1.0, learning_rate=0.1):\n",
    "\n",
    "        self.name = name\n",
    "        self.policy = policy\n",
    "        self.initial_value = initial_value\n",
    "        self.n = n\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.saved_game_states = None\n",
    "        self.past_rewards = None\n",
    "        self.value_function = None\n",
    "        self.update_count = None\n",
    "        self.updates_on = True\n",
    "\n",
    "    def initialize(self, game, role):\n",
    "        \n",
    "        # Initialise value function\n",
    "        self.value_function = {}\n",
    "        self.update_count = 0\n",
    "        \n",
    "        self.saved_game_states = {}\n",
    "        self.past_rewards = {}  # TODO: Need to replace past_rewards\n",
    "        \n",
    "        # This must be called before starting the first game\n",
    "        self.reset_game_information(game, role)\n",
    "        \n",
    "        # Some behaviour policies need reseting (e.g. random seed)\n",
    "        self.policy.reset()\n",
    "\n",
    "    def get_value(self, state_key):\n",
    "        \"\"\"Returns a value from TDLearner's value_function for the\n",
    "        game state represented by state_key. If there is no item for\n",
    "        that state, returns the initial_value instead.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.value_function.get(state_key, self.initial_value)\n",
    "\n",
    "    def save_state(self, game, state_key):\n",
    "        \"\"\"Adds state_key to a list of keys stored in dictionary\n",
    "        self.saved_game_states for each game being played.\n",
    "        \"\"\"\n",
    "\n",
    "        self.saved_game_states.get(game, []).append(state_key)\n",
    "\n",
    "    def decide_next_move(self, game, role, show=False):\n",
    "        \"\"\"Returns the agent's next move in game (using its policy)\n",
    "        and assuming they are playing this role.\n",
    "\n",
    "        Args:\n",
    "            game (Game): Game being played.\n",
    "            role (object): Role that the player is playing.\n",
    "            show (bool): Print messages if True.\n",
    "\n",
    "        Returns:\n",
    "            move (tuple): Tuple containing (role, action).\n",
    "        \"\"\"\n",
    "\n",
    "        action = self.policy.choose_action(game, role)\n",
    "        move = (role, action)\n",
    "\n",
    "        # Save chosen after-state for learning updates later\n",
    "        # TODO: Should save timestep with it\n",
    "        next_state = game.next_state(game.state, move)\n",
    "        next_state_key = game.generate_state_key(next_state, role)\n",
    "        self.save_state(game, next_state_key)\n",
    "\n",
    "        if show:\n",
    "            move_format = game.help_text['Move format']\n",
    "            print(\"%s's turn (%s): %s\" % (self.name, move_format,\n",
    "                                          str(position)))\n",
    "\n",
    "        return move\n",
    "\n",
    "    def make_move(self, game, role, show=False):\n",
    "        \"\"\"Ask player to make a move.  This method will call\n",
    "        the player's decide_next_move method and will then\n",
    "        execute the resulting move in the game.\n",
    "\n",
    "        Args:\n",
    "            game (Game): Game that player is playing.\n",
    "            role (int): Role that the player is playing.\n",
    "            show (bool): Print messages if True.\n",
    "        \"\"\"\n",
    "\n",
    "        assert not game.game_over, \"Can't make move. Game is over.\"\n",
    "\n",
    "        # Player needs to trigger game start if method exists\n",
    "        if len(game.moves) == 0:\n",
    "            try:\n",
    "                game.start()\n",
    "            except AttributeError:\n",
    "                pass\n",
    "\n",
    "        move = self.decide_next_move(game, role, show)\n",
    "        game.make_move(move)\n",
    "        \n",
    "        #self.t += 1  # TODO: Need to figure out when to do this\n",
    "\n",
    "    def update(self, game, reward, show=False, debug=False):\n",
    "        \"\"\"Update TDLearner's value function based on current reward \n",
    "        from game.  This gets called by GameController during a game\n",
    "        whenever rewards are distributed and the player has received\n",
    "        one.\n",
    "\n",
    "        Args:\n",
    "            game (Game): Game that player is playing.\n",
    "            role (int): Player role.\n",
    "            reward (float): Reward value.\n",
    "            show (bool): Print a message if True.\n",
    "        \n",
    "        TODO: Should add a 'terminal' or 'done' argument to signal\n",
    "        the terminal state reached (gameover).\n",
    "        \"\"\"\n",
    "\n",
    "        # Save the current state and reward for future value\n",
    "        # updates.  TODO: Needs to be generalized to multiple games\n",
    "        self.past_rewards.append(reward)\n",
    "\n",
    "        if self.updates_on:\n",
    "            prev_states = self.saved_game_states[game]\n",
    "            prev_rewards = self.past_rewards\n",
    "            \n",
    "            # TODO: Do we need to track and update self.t?\n",
    "            # Or can we just use prev_states\n",
    "            #t = len(prev_states) - 1\n",
    "\n",
    "            if not game.game_over:\n",
    "                \n",
    "                tau = self.t - self.n + 1\n",
    "                self.td_lambda_update(prev_states, prev_rewards, tau,\n",
    "                                      debug=debug)\n",
    "\n",
    "            else:\n",
    "                # Complete final state-value updates for timesteps tau\n",
    "                # to current (terminal) timestep\n",
    "                T = self.t + 1\n",
    "                for tau in range(self.t - self.n + 1, T):\n",
    "                    self.td_lambda_update(prev_states, prev_rewards, tau=tau, \n",
    "                                          t_max=T, debug=debug)\n",
    "\n",
    "        self.t += 1   # TODO: Need to figure out when to do this\n",
    "                      # on 2-player games (works here)\n",
    "                      # It's connected to the question of when\n",
    "                      # to add the state and reward to saved_states\n",
    "                      # Better idea is to base it on len(prev_states)\n",
    "                      # maybe\n",
    "\n",
    "        if show:\n",
    "            print(\"%s got %s reward.\" % (self.name, reward))\n",
    "\n",
    "    def td_lambda_update(self, prev_states, prev_rewards, tau, \n",
    "                         t_max=np.inf, debug=False):\n",
    "        \"\"\"Updates the value for the past state that occurred\n",
    "        in timestep tau using the Bellman equation.  If tau is \n",
    "        not provided, then it is calculated as follows:\n",
    "        \n",
    "        tau = t - n + 1\n",
    "        \n",
    "        Where:\n",
    "            t is the current timestep (self.t)\n",
    "            n is the lambda value (self.n)  \n",
    "        \n",
    "        If tau < 0 no update is possible so none is made.\n",
    "        \n",
    "        Args:\n",
    "            tau (int): Previous time-step to be updated.\n",
    "            t_max (int or np.inf): The maximum timestep to include.  \n",
    "                Set to infinity or a high number if you want to\n",
    "                include all timesteps from tau to tau + n.  Set\n",
    "                to t if the game has eneded.\n",
    "        \"\"\"\n",
    "\n",
    "        # Set tau, the timestep of the state that will have its\n",
    "        # value updated\n",
    "        \n",
    "        assert len(prev_states) == len(prev_rewards)\n",
    "        \n",
    "        #if debug and self.t == 177:\n",
    "        #    breakpoint()\n",
    "        #    show = True\n",
    "        #else:\n",
    "        show = False\n",
    "\n",
    "        if tau >= 0:\n",
    "\n",
    "            assert tau < len(prev_states), \"Not enough past states.\"\n",
    "\n",
    "            n = self.n\n",
    "            #g = 0\n",
    "            #for i in range(tau + 1, min(tau + n, t_max) + 1):\n",
    "            #    # TODO: Can this be vectorized?\n",
    "            #    g += self.gamma**(i - tau - 1)*prev_rewards[i]\n",
    "            \n",
    "            discounted_rewards = discounted_sequence_generator(\n",
    "                prev_rewards[tau+1:min(tau + n, t_max) + 1], \n",
    "                self.gamma\n",
    "            )\n",
    "            g = sum(list(discounted_rewards))\n",
    "            \n",
    "            if tau + n < t_max:\n",
    "                g += self.gamma**n*self.get_value(prev_states[tau + n])\n",
    "\n",
    "            # Update value of state at timestep tau\n",
    "            state_key = prev_states[tau]\n",
    "            state_value = self.get_value(state_key)\n",
    "            \n",
    "            if show:\n",
    "                print(f\"prev_states[{tau}]: {state_value}\")\n",
    "    \n",
    "            agent.value_function[state_key] = state_value + \\\n",
    "                self.learning_rate*(g - state_value)\n",
    "        \n",
    "            if show:\n",
    "                print(f\"value_function[{state_key.__repr__()}]: {self.get_value(state_key)}\")\n",
    "\n",
    "    def reset_game_information(self, game, role):\n",
    "        \"\"\"Delete state and reward history for specified game\n",
    "        and set timestep (t) to zero.\n",
    "        \"\"\"\n",
    "\n",
    "        state_key = game.generate_state_key(game.state, role)\n",
    "        self.saved_game_states[game] = [state_key]\n",
    "        self.past_rewards = [None]\n",
    "        self.t = 0\n",
    "\n",
    "    def gameover(self, game):\n",
    "\n",
    "        pass\n",
    "\n",
    "class Policy(ABC):\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        \n",
    "        self.name = name\n",
    "    \n",
    "    @abstractmethod\n",
    "    def choose_action(self, game, role):\n",
    "        \n",
    "        pass\n",
    "\n",
    "\n",
    "class RandomPolicy(Policy):\n",
    "    \n",
    "    def __init__(self, name=\"RANDOM\", seed=None):\n",
    "        \n",
    "        super().__init__(name)\n",
    "        \n",
    "        # Independent random number generator for sole use\n",
    "        # by this instance\n",
    "        self.seed = seed\n",
    "        self.reset()\n",
    "\n",
    "    def choose_action(self, game, role):\n",
    "        \n",
    "        available_moves = game.available_moves()\n",
    "        \n",
    "        action = self.rng.choice(available_moves)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        self.rng = random.Random(self.seed)\n",
    "\n",
    "        \n",
    "class PredefinedActions(Policy):\n",
    "    \n",
    "    def __init__(self, actions, name=\"PREDEFINED\"):\n",
    "        \n",
    "        super().__init__(name)\n",
    "        \n",
    "        self.actions = actions\n",
    "        self.t = 0\n",
    "\n",
    "    def choose_action(self, game, role):\n",
    "        \n",
    "        available_moves = game.available_moves()\n",
    "        \n",
    "        action = self.actions[self.t]\n",
    "        \n",
    "        assert action in game.available_moves()\n",
    "\n",
    "        self.t += 1\n",
    "\n",
    "        return action\n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        self.t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter number of repetitions (1-100). Warning: 100 repetitions takes about 7 mins\n",
      "Press enter for default (1):1\n",
      "Execution time: 6.1s\n",
      "512 0.5371095068993652\n"
     ]
    }
   ],
   "source": [
    "# Allow user to set number of repetitions (default 1)\n",
    "print(\"Enter number of repetitions (1-100). Warning: 100 repetitions takes about 7 mins\")\n",
    "default_input = '1'\n",
    "message = input(f\"Press enter for default ({default_input}):\")\n",
    "\n",
    "try:\n",
    "    repetitions = int(message or default_input)\n",
    "except ValueError as err:\n",
    "    print(\"Didn't understand that. Please try again.\")\n",
    "    raise err\n",
    "\n",
    "# Environment parameters\n",
    "size = 19\n",
    "terminal_rewards = {'T1': -1.0, 'T2': 1.0}\n",
    "game = RandomWalkGame(size=size, terminal_rewards=terminal_rewards)\n",
    "n_iters = 10\n",
    "\n",
    "# Agent parameters\n",
    "initial_value = 0.0\n",
    "gamma = 1.0\n",
    "seed = 5\n",
    "agent = Agent(RandomPolicy(seed=seed), name=\"Random\", \n",
    "              initial_value=initial_value, \n",
    "              gamma=gamma)\n",
    "\n",
    "# Iterate over learning_rates and n-values\n",
    "learning_rates = np.logspace(-2, 0.0, 50)\n",
    "n_values = [2**i for i in range(10)]\n",
    "\n",
    "avg_rms_error_sets = {}\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# TODO: Should be repeating the test 100 times for each\n",
    "# pair of n, alpha values and averaging the results.\n",
    "\n",
    "for n in n_values:\n",
    "\n",
    "    # TODO: Should be repeating the test 100 times for each\n",
    "    # pair of n, alpha values and averaging the results.\n",
    "    \n",
    "    # OVERRIDE\n",
    "    #n = 10\n",
    "    agent.n = n\n",
    "\n",
    "    rms_error_sets = []\n",
    "\n",
    "    for repetition in range(repetitions):\n",
    "    \n",
    "        rms_errors = []\n",
    "        seed = repetition\n",
    "\n",
    "        for learning_rate in learning_rates:\n",
    "\n",
    "            agent.learning_rate = learning_rate\n",
    "\n",
    "            # Initialize agent with empty value function\n",
    "            agent.initialize(game, role=1)\n",
    "\n",
    "            # Testing policy rng was reset\n",
    "            some_moves = [agent.decide_next_move(game, 1) for _ in range(5)]\n",
    "            agent.initialize(game, role=1)\n",
    "            some_moves2 = [agent.decide_next_move(game, 1) for _ in range(5)]\n",
    "            assert some_moves == some_moves2\n",
    "\n",
    "            agent.initialize(game, role=1)\n",
    "            # Testing value function is zero\n",
    "            value_function = {\n",
    "                s: 0.0 if s in game.terminal_states else agent.initial_value\n",
    "                for s in game.states\n",
    "            }\n",
    "            agents_v = {s: agent.get_value(s) for s in game.states}\n",
    "            assert agents_v == value_function\n",
    "\n",
    "            for iteration in range(n_iters):\n",
    "\n",
    "                # Reset agent's game information\n",
    "                agent.reset_game_information(game, role=1)\n",
    "\n",
    "                # Testing\n",
    "                #assert agent.t == 0\n",
    "                #assert agent.saved_game_states[game] == [game.state]\n",
    "                #assert agent.past_rewards == [None]\n",
    "\n",
    "                while not game.game_over:\n",
    "\n",
    "                    # Behaviour policy\n",
    "                    move = agent.decide_next_move(game, 1)\n",
    "                    game.make_move(move)\n",
    "\n",
    "                    # Get rewards\n",
    "                    #reward = game.get_rewards()[1]\n",
    "                    if not game.game_over:\n",
    "                        reward = game.get_rewards()[1]\n",
    "                    else:\n",
    "                        reward = game.get_terminal_rewards()[1]\n",
    "\n",
    "                    debug = True if iteration == n_iters - 1 else False\n",
    "\n",
    "                    # Update the value for state in timestep tau,\n",
    "                    # where tau = t - n + 1\n",
    "                    agent.update(game, reward, debug=debug)\n",
    "\n",
    "                # TODO: Should this be where final final TD(λ) updates happen?\n",
    "                agent.gameover(game)  # Currently redundant\n",
    "                game.reset()\n",
    "\n",
    "                # OVERRIDE\n",
    "                #break\n",
    "\n",
    "            values = pd.Series(agent.value_function, index=game.states).fillna(agent.initial_value)\n",
    "            rms_errors.append(calculate_rms_error(values[1:game.size+1], true_values))\n",
    "            #break\n",
    "\n",
    "        rms_error_sets.append(rms_errors)\n",
    "\n",
    "    avg_rms_error_sets[n] = np.vstack(rms_error_sets).mean(axis=0)\n",
    "    \n",
    "    # OVERRIDE HERE\n",
    "    #break\n",
    "\n",
    "t1 = time.time()\n",
    "total_time = t1 - t0\n",
    "print(f\"Execution time: {total_time:.1f}s\")\n",
    "\n",
    "print(n, rms_errors[0])\n",
    "# For n=1\n",
    "# 1 [0.5392683523771096, 0.5311550178396927, 0.5233241063497756]\n",
    "\n",
    "# For n=10, t = 11\n",
    "# 10 [0.49279075198489264, 0.4444604816372286, 0.4014578835834512]\n",
    "# assert rms_results[0] == 0.49279075198489264\n",
    "# prev_states[2]: 0.009388775398326584\n",
    "# value_function['J']: 0.009081736826626132\n",
    "\n",
    "# For n=128\n",
    "# 128 [0.5067675989278085, 0.5346220655366686, 0.5882351606556061]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/billtubbs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Plot results\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for n, rms_errors in avg_rms_error_sets.items():\n",
    "    ax.plot(learning_rates, rms_errors, label='n = %d' % n)\n",
    "plt.title('n-step TD value error on random walk (%d states)' % game.size)\n",
    "ax.set_xlabel('Learning rate (alpha)')\n",
    "ax.set_ylabel('RMS Error')\n",
    "ax.set_ylim(0.0, 0.55)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(f\"random-walk-n-step-td-{game.size}-{repetitions}-classes.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False,  True])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(avg_rms_error_sets[1] == avg_rms_error_sets_master[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5384547846703097, 0.5384495420268675)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_rms_error_sets[1][1], avg_rms_error_sets_master[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.53926366, 0.53844954, 0.53755899, 0.5365852 , 0.5355208 ,\n",
       "       0.53435785, 0.53308781, 0.53170148, 0.530189  , 0.52853979,\n",
       "       0.5267425 , 0.52478504, 0.52265447, 0.52033701, 0.51781796,\n",
       "       0.51508172, 0.51211168, 0.50889018, 0.50539845, 0.50161653,\n",
       "       0.49752314, 0.49309554, 0.48830937, 0.48313842, 0.4775543 ,\n",
       "       0.4715261 , 0.46501992, 0.45799828, 0.45041941, 0.44223645,\n",
       "       0.43339645, 0.42383934, 0.41349679, 0.40229106, 0.390134  ,\n",
       "       0.37692637, 0.36255776, 0.34690764, 0.32984838, 0.31125188,\n",
       "       0.29100326, 0.2690287 , 0.24535403, 0.22023336, 0.19444371,\n",
       "       0.16997238, 0.1515009 , 0.14856947, 0.17659995, 0.29912152])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_rms_error_sets_master[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, False, False, False, False, False, False, False, False]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    np.allclose(avg_rms_error_sets[item], avg_rms_error_sets_master[item])\n",
    "    for item in avg_rms_error_sets_master\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple One-Episode Tests of Value Function Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reference Values</th>\n",
       "      <th>Test Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>T1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>J</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Reference Values  Test Values\n",
       "T1               0.0          NaN\n",
       "A                0.0          NaN\n",
       "B                0.0          NaN\n",
       "C                0.0          NaN\n",
       "D                0.0          NaN\n",
       "E                0.0          NaN\n",
       "F                0.0          NaN\n",
       "G                0.0          NaN\n",
       "H                0.0          NaN\n",
       "I                0.0          NaN\n",
       "J                0.1          0.1\n",
       "K                0.1          0.1\n",
       "L                0.1          0.1\n",
       "M                0.1          0.1\n",
       "N                0.1          0.1\n",
       "O                0.1          0.1\n",
       "P                0.1          0.1\n",
       "Q                0.1          0.1\n",
       "R                0.1          0.1\n",
       "S                0.1          0.1\n",
       "T2               0.0          NaN"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 1.0\n",
    "n = 10\n",
    "learning_rate = 0.1\n",
    "actions = ['r']*10\n",
    "\n",
    "game = RandomWalkGame(size=19, terminal_rewards={'T1': -1.0, 'T2': 1.0})\n",
    "\n",
    "agent = Agent(PredefinedActions(actions), name=\"Test 1\", initial_value=0.0, \n",
    "              gamma=gamma)\n",
    "agent.n = n\n",
    "agent.learning_rate = learning_rate\n",
    "agent.initialize(game, role=1)\n",
    "\n",
    "while not game.game_over:\n",
    "\n",
    "    # Behaviour policy\n",
    "    move = agent.decide_next_move(game, 1)\n",
    "    game.make_move(move)\n",
    "\n",
    "    # Get rewards\n",
    "    #reward = game.get_rewards()[1]\n",
    "    if not game.game_over:\n",
    "        reward = game.get_rewards()[1]\n",
    "    else:\n",
    "        reward = game.get_terminal_rewards()[1]\n",
    "\n",
    "    debug = True if iteration == n_iters - 1 else False\n",
    "\n",
    "    # Update the value for state in timestep tau,\n",
    "    # where tau = t - n + 1\n",
    "    agent.update(game, reward, debug=debug)\n",
    "\n",
    "agent.gameover(game)\n",
    "\n",
    "# Convert actions to (1, -1)\n",
    "action_map = {'l': -1, 'r': 1}\n",
    "actions = [action_map[a] for role, a in game.moves]\n",
    "\n",
    "# Reference values from benchmark codetest_values = np.zeros(N_STATES + 2)\n",
    "test_values = np.zeros(N_STATES + 2) \n",
    "temporal_difference_test(actions, test_values, n, learning_rate, gamma=gamma)\n",
    "\n",
    "comparison = pd.DataFrame({'Reference Values': test_values, \n",
    "                           'Test Values': agent.value_function}, \n",
    "                          index=game.states)\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(comparison.dropna()['Reference Values'] == \n",
    "              comparison.dropna()['Test Values'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reference Values</th>\n",
       "      <th>Test Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>T1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>J</th>\n",
       "      <td>0.387420</td>\n",
       "      <td>0.387420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K</th>\n",
       "      <td>0.430467</td>\n",
       "      <td>0.430467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L</th>\n",
       "      <td>0.478297</td>\n",
       "      <td>0.478297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>0.531441</td>\n",
       "      <td>0.531441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>0.590490</td>\n",
       "      <td>0.590490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>0.656100</td>\n",
       "      <td>0.656100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P</th>\n",
       "      <td>0.729000</td>\n",
       "      <td>0.729000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q</th>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Reference Values  Test Values\n",
       "T1          0.000000          NaN\n",
       "A           0.000000          NaN\n",
       "B           0.000000          NaN\n",
       "C           0.000000          NaN\n",
       "D           0.000000          NaN\n",
       "E           0.000000          NaN\n",
       "F           0.000000          NaN\n",
       "G           0.000000          NaN\n",
       "H           0.000000          NaN\n",
       "I           0.000000          NaN\n",
       "J           0.387420     0.387420\n",
       "K           0.430467     0.430467\n",
       "L           0.478297     0.478297\n",
       "M           0.531441     0.531441\n",
       "N           0.590490     0.590490\n",
       "O           0.656100     0.656100\n",
       "P           0.729000     0.729000\n",
       "Q           0.810000     0.810000\n",
       "R           0.900000     0.900000\n",
       "S           1.000000     1.000000\n",
       "T2          0.000000          NaN"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "n = 10\n",
    "learning_rate = 1.0\n",
    "actions = ['r']*11\n",
    "\n",
    "game = RandomWalkGame(size=19, terminal_rewards={'T1': -1.0, 'T2': 1.0})\n",
    "\n",
    "agent = Agent(PredefinedActions(actions), name=\"Test 2\", initial_value=0.0, \n",
    "              gamma=gamma)\n",
    "agent.n = n\n",
    "agent.learning_rate = learning_rate\n",
    "agent.initialize(game, role=1)\n",
    "\n",
    "while not game.game_over:\n",
    "\n",
    "    # Behaviour policy\n",
    "    move = agent.decide_next_move(game, 1)\n",
    "    game.make_move(move)\n",
    "\n",
    "    # Get rewards\n",
    "    #reward = game.get_rewards()[1]\n",
    "    if not game.game_over:\n",
    "        reward = game.get_rewards()[1]\n",
    "    else:\n",
    "        reward = game.get_terminal_rewards()[1]\n",
    "\n",
    "    debug = True if iteration == n_iters - 1 else False\n",
    "\n",
    "    # Update the value for state in timestep tau,\n",
    "    # where tau = t - n + 1\n",
    "    agent.update(game, reward, debug=debug)\n",
    "\n",
    "agent.gameover(game)\n",
    "\n",
    "# Convert actions to (1, -1)\n",
    "action_map = {'l': -1, 'r': 1}\n",
    "actions = [action_map[a] for role, a in game.moves]\n",
    "\n",
    "# Reference values from benchmark codetest_values = np.zeros(N_STATES + 2)\n",
    "test_values = np.zeros(N_STATES + 2) \n",
    "\n",
    "temporal_difference_test(actions, test_values, n, learning_rate, gamma=gamma)\n",
    "\n",
    "comparison = pd.DataFrame({'Reference Values': test_values, \n",
    "                           'Test Values': agent.value_function}, \n",
    "                          index=game.states)\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(comparison.dropna()['Reference Values'], comparison.dropna()['Test Values']).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reference Values</th>\n",
       "      <th>Test Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>T1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>-0.900</td>\n",
       "      <td>-0.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>-0.810</td>\n",
       "      <td>-0.810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>-0.729</td>\n",
       "      <td>-0.729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>J</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K</th>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L</th>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P</th>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q</th>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R</th>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Reference Values  Test Values\n",
       "T1             0.000          NaN\n",
       "A             -0.900       -0.900\n",
       "B             -0.810       -0.810\n",
       "C             -0.729       -0.729\n",
       "D              0.000        0.000\n",
       "E              0.000        0.000\n",
       "F              0.000        0.000\n",
       "G              0.000        0.000\n",
       "H              0.000        0.000\n",
       "I              0.000        0.000\n",
       "J              0.000        0.000\n",
       "K              0.000          NaN\n",
       "L              0.000          NaN\n",
       "M              0.000          NaN\n",
       "N              0.000          NaN\n",
       "O              0.000          NaN\n",
       "P              0.000          NaN\n",
       "Q              0.000          NaN\n",
       "R              0.000          NaN\n",
       "S              0.000          NaN\n",
       "T2             0.000          NaN"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "n = 3\n",
    "learning_rate = 0.9\n",
    "actions = ['l']*11\n",
    "\n",
    "game = RandomWalkGame(size=19, terminal_rewards={'T1': -1.0, 'T2': 1.0})\n",
    "\n",
    "agent = Agent(PredefinedActions(actions), name=\"Test 3\", initial_value=0.0, \n",
    "              gamma=gamma)\n",
    "agent.n = n\n",
    "agent.learning_rate = learning_rate\n",
    "agent.initialize(game, role=1)\n",
    "\n",
    "while not game.game_over:\n",
    "\n",
    "    # Behaviour policy\n",
    "    move = agent.decide_next_move(game, 1)\n",
    "    game.make_move(move)\n",
    "\n",
    "    # Get rewards\n",
    "    #reward = game.get_rewards()[1]\n",
    "    if not game.game_over:\n",
    "        reward = game.get_rewards()[1]\n",
    "    else:\n",
    "        reward = game.get_terminal_rewards()[1]\n",
    "\n",
    "    debug = True if iteration == n_iters - 1 else False\n",
    "\n",
    "    # Update the value for state in timestep tau,\n",
    "    # where tau = t - n + 1\n",
    "    agent.update(game, reward, debug=debug)\n",
    "\n",
    "agent.gameover(game)\n",
    "\n",
    "# Convert actions to (1, -1)\n",
    "action_map = {'l': -1, 'r': 1}\n",
    "actions = [action_map[a] for role, a in game.moves]\n",
    "\n",
    "# Reference values from benchmark codetest_values = np.zeros(N_STATES + 2)\n",
    "test_values = np.zeros(N_STATES + 2) \n",
    "temporal_difference_test(actions, test_values, n, learning_rate, gamma=gamma)\n",
    "\n",
    "comparison = pd.DataFrame({'Reference Values': test_values, \n",
    "                           'Test Values': agent.value_function}, \n",
    "                          index=game.states)\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(comparison.dropna()['Reference Values'], comparison.dropna()['Test Values']).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 made move 'l'\n",
      "Player 1 made move 'l'\n",
      "Player 1 made move 'l'\n",
      "Player 1 made move 'l'\n",
      "Player 1 made move 'l'\n",
      "Player 1 made move 'l'\n",
      "Player 1 made move 'r'\n",
      "Player 1 made move 'l'\n",
      "Player 1 made move 'l'\n",
      "Player 1 made move 'l'\n",
      "Player 1 made move 'r'\n",
      "Player 1 made move 'l'\n",
      "Player 1 made move 'l'\n",
      "Player 1 made move 'l'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reference Values</th>\n",
       "      <th>Test Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>T1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>-0.900000</td>\n",
       "      <td>-0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>-0.875610</td>\n",
       "      <td>-0.875610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>-0.788049</td>\n",
       "      <td>-0.788049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>-0.574488</td>\n",
       "      <td>-0.574488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E</th>\n",
       "      <td>-0.517039</td>\n",
       "      <td>-0.517039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>-0.348678</td>\n",
       "      <td>-0.348678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>J</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Reference Values  Test Values\n",
       "T1          0.000000          NaN\n",
       "A          -0.900000    -0.900000\n",
       "B          -0.875610    -0.875610\n",
       "C          -0.788049    -0.788049\n",
       "D          -0.574488    -0.574488\n",
       "E          -0.517039    -0.517039\n",
       "F          -0.348678    -0.348678\n",
       "G           0.000000     0.000000\n",
       "H           0.000000     0.000000\n",
       "I           0.000000     0.000000\n",
       "J           0.000000     0.000000\n",
       "K           0.000000          NaN\n",
       "L           0.000000          NaN\n",
       "M           0.000000          NaN\n",
       "N           0.000000          NaN\n",
       "O           0.000000          NaN\n",
       "P           0.000000          NaN\n",
       "Q           0.000000          NaN\n",
       "R           0.000000          NaN\n",
       "S           0.000000          NaN\n",
       "T2          0.000000          NaN"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "n = 10\n",
    "learning_rate = 0.9\n",
    "actions = ['l', 'l', 'l', 'l', 'l', 'l', 'r', 'l', 'l', 'l', 'r', 'l', 'l', 'l']\n",
    "\n",
    "game = RandomWalkGame(size=19, terminal_rewards={'T1': -1.0, 'T2': 1.0})\n",
    "\n",
    "agent = Agent(PredefinedActions(actions), name=\"Test 4\", initial_value=0.0, \n",
    "              gamma=gamma)\n",
    "agent.n = n\n",
    "agent.learning_rate = learning_rate\n",
    "agent.initialize(game, role=1)\n",
    "\n",
    "while not game.game_over:\n",
    "\n",
    "    # Behaviour policy\n",
    "    move = agent.decide_next_move(game, 1)\n",
    "    game.make_move(move, show=True)\n",
    "\n",
    "    # Get rewards\n",
    "    #reward = game.get_rewards()[1]\n",
    "    if not game.game_over:\n",
    "        reward = game.get_rewards()[1]\n",
    "    else:\n",
    "        reward = game.get_terminal_rewards()[1]\n",
    "\n",
    "    debug = True if iteration == n_iters - 1 else False\n",
    "\n",
    "    # Update the value for state in timestep tau,\n",
    "    # where tau = t - n + 1\n",
    "    agent.update(game, reward, debug=debug)\n",
    "\n",
    "agent.gameover(game)\n",
    "\n",
    "# Convert actions to (1, -1)\n",
    "action_map = {'l': -1, 'r': 1}\n",
    "actions = [action_map[a] for role, a in game.moves]\n",
    "\n",
    "# Reference values from benchmark codetest_values = np.zeros(N_STATES + 2)\n",
    "test_values = np.zeros(N_STATES + 2)\n",
    "temporal_difference_test(actions, test_values, n, learning_rate, gamma=gamma)\n",
    "\n",
    "comparison = pd.DataFrame({'Reference Values': test_values, \n",
    "                           'Test Values': agent.value_function}, \n",
    "                          index=game.states)\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(comparison.dropna()['Reference Values'], comparison.dropna()['Test Values']).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reference Values</th>\n",
       "      <th>Test Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>T1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>J</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L</th>\n",
       "      <td>0.348678</td>\n",
       "      <td>0.348678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>0.517039</td>\n",
       "      <td>0.517039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>0.574488</td>\n",
       "      <td>0.574488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>0.590490</td>\n",
       "      <td>0.590490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P</th>\n",
       "      <td>0.656100</td>\n",
       "      <td>0.656100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q</th>\n",
       "      <td>0.729000</td>\n",
       "      <td>0.729000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R</th>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Reference Values  Test Values\n",
       "T1          0.000000          NaN\n",
       "A           0.000000          NaN\n",
       "B           0.000000          NaN\n",
       "C           0.000000          NaN\n",
       "D           0.000000          NaN\n",
       "E           0.000000          NaN\n",
       "F           0.000000     0.000000\n",
       "G           0.000000     0.000000\n",
       "H           0.000000     0.000000\n",
       "I           0.000000     0.000000\n",
       "J           0.000000     0.000000\n",
       "K           0.000000     0.000000\n",
       "L           0.348678     0.348678\n",
       "M           0.517039     0.517039\n",
       "N           0.574488     0.574488\n",
       "O           0.590490     0.590490\n",
       "P           0.656100     0.656100\n",
       "Q           0.729000     0.729000\n",
       "R           0.810000     0.810000\n",
       "S           0.900000     0.900000\n",
       "T2          0.000000          NaN"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "n = 10\n",
    "learning_rate = 0.9\n",
    "seed = 2\n",
    "\n",
    "game = RandomWalkGame(size=19, terminal_rewards={'T1': -1.0, 'T2': 1.0})\n",
    "\n",
    "agent = Agent(RandomPolicy(seed=seed), name=\"Test 5\", initial_value=0.0, \n",
    "              gamma=gamma)\n",
    "agent.n = n\n",
    "agent.learning_rate = learning_rate\n",
    "agent.initialize(game, role=1)\n",
    "\n",
    "while not game.game_over:\n",
    "\n",
    "    # Behaviour policy\n",
    "    move = agent.decide_next_move(game, 1)\n",
    "    game.make_move(move)\n",
    "\n",
    "    # Get rewards\n",
    "    #reward = game.get_rewards()[1]\n",
    "    if not game.game_over:\n",
    "        reward = game.get_rewards()[1]\n",
    "    else:\n",
    "        reward = game.get_terminal_rewards()[1]\n",
    "\n",
    "    debug = True if iteration == n_iters - 1 else False\n",
    "\n",
    "    # Update the value for state in timestep tau,\n",
    "    # where tau = t - n + 1\n",
    "    agent.update(game, reward, debug=debug)\n",
    "\n",
    "agent.gameover(game)\n",
    "\n",
    "# Convert actions to (1, -1)\n",
    "action_map = {'l': -1, 'r': 1}\n",
    "actions = [action_map[a] for role, a in game.moves]\n",
    "\n",
    "# Reference values from benchmark codetest_values = np.zeros(N_STATES + 2)\n",
    "test_values = np.zeros(N_STATES + 2) \n",
    "temporal_difference_test(actions, test_values, n, learning_rate, gamma=gamma)\n",
    "\n",
    "comparison = pd.DataFrame({'Reference Values': test_values, \n",
    "                           'Test Values': agent.value_function}, \n",
    "                          index=game.states)\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(comparison.dropna()['Reference Values'], comparison.dropna()['Test Values']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_states = ['J', 'K', 'J', 'K', 'J', 'K', 'L', 'K', 'J', 'K', 'J', 'I', 'H']\n",
    "              \n",
    "prev_rewards = [None, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "n = 10\n",
    "gamma = 1.0\n",
    "initial_value = 0.0\n",
    "value_function = {\n",
    "    'J': 0.009388775398326584, 'K': 0.012724472068223015, 'L': 0.015790004710492304, \n",
    "    'I': -0.00403859530840691, 'H': -0.021315081771718508, 'G': -0.021496015967472096, \n",
    "    'F': -0.054275718752572394, 'E': -0.06948717697342521, 'D': -0.06598448738239086, \n",
    "    'C': -0.04893934900883692, 'B': -0.04901704716984233, 'A': -0.0490099501, \n",
    "    'M': 0.029488099958858597, 'N': 0.05430058175403064, 'O': 0.05502056748246712, \n",
    "    'P': 0.07147918642363447, 'Q': 0.07271629098862856, 'R': 0.0840926138566933, \n",
    "    'S': 0.06737658637280254\n",
    "}\n",
    "\n",
    "\n",
    "t_max = np.inf\n",
    "\n",
    "t = 11\n",
    "\n",
    "def n_step_td_update(prev_states, prev_rewards, t, n, gamma,\n",
    "                     value_function, t_max=np.inf):\n",
    "    \"\"\"Carries out an n-step TD update to the previous state-value\n",
    "    at timestep (t - n + 1) using the values from steps (t - n + 2)\n",
    "    to (t + 1) or t_max, whichever comes first.\n",
    "    \n",
    "    If (t - n + 1) < 0, no update is carried out.\n",
    "    \n",
    "    Note: Usually, when you call this function in timestep t, you\n",
    "    set tau to t - n + 1.\n",
    "    \n",
    "    If the episode is over (i.e. state t is a terminal state), then\n",
    "    set t_max to t and it will not look for a value in timestep \n",
    "    (t + 1).\n",
    "    \"\"\"\n",
    "\n",
    "    # Value for state in timestep tau will be updated\n",
    "    tau = t - n + 1\n",
    "\n",
    "    if tau >= 0 and len(prev_states) > 1:\n",
    "\n",
    "        # Original version\n",
    "        #g = sum([gamma**(i - tau - 1)*prev_rewards[i] \n",
    "        #         for i in range(tau + 1, min(tau + n, t_max) + 1)])\n",
    "\n",
    "        # This is the fastest\n",
    "        discounted_rewards = discounted_sequence_generator(\n",
    "            prev_rewards[tau+1:min(tau + n, t_max) + 1], \n",
    "            gamma\n",
    "        )\n",
    "        g = sum(list(discounted_rewards))\n",
    "        \n",
    "        print(g)\n",
    "        \n",
    "        # Vectorized version\n",
    "        # Note: This is only faster for n>25\n",
    "        #g = np.power(np.full((n), gamma), np.arange(min(n, t_max - tau)))*prev_rewards[tau+1:min(tau + n, t_max) + 1]\n",
    "\n",
    "        if tau + n < t_max:\n",
    "            g += gamma**n*value_function.get(prev_states[tau + n], initial_value)\n",
    "\n",
    "        state_key = prev_states[tau]\n",
    "        state_value = value_function.get(state_key, initial_value)\n",
    "\n",
    "        print(f\"prev_states[{tau}]: {state_value}\")\n",
    "\n",
    "        value_function[state_key] = state_value + learning_rate*(g - state_value)\n",
    "\n",
    "        print(f\"value_function[{state_key.__repr__()}]: {value_function[state_key]}\")\n",
    "\n",
    "#t += 1  # TODO: Need to figure out when to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "prev_states[2]: 0.009388775398326584\n",
      "value_function['J']: -0.018244696054713995\n"
     ]
    }
   ],
   "source": [
    "n_step_td_update(prev_states, prev_rewards, t, n, gamma,\n",
    "                 value_function, t_max=np.inf)\n",
    "\n",
    "# t = 9:\n",
    "#prev_states[1]: 0.012893795981118266\n",
    "#value_function['K']: 0.012724472068223015\n",
    "#10 0.49279075198489264\n",
    "\n",
    "\n",
    "# Test cases\n",
    "#prev_states = ['J', 'K', 'J', 'K', 'J', 'K', 'L', 'K', 'J', 'K', 'J', 'I', 'H']\n",
    "#prev_rewards = [None, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "#\n",
    "#n = 10\n",
    "#gamma = 1.0\n",
    "#initial_value = 0.0\n",
    "#value_function = {\n",
    "#    'J': 0.009388775398326584, 'K': 0.012724472068223015, 'L': 0.015790004710492304, \n",
    "#    'I': -0.00403859530840691, 'H': -0.021315081771718508, 'G': -0.021496015967472096, \n",
    "#    'F': -0.054275718752572394, 'E': -0.06948717697342521, 'D': -0.06598448738239086, \n",
    "#    'C': -0.04893934900883692, 'B': -0.04901704716984233, 'A': -0.0490099501, \n",
    "#    'M': 0.029488099958858597, 'N': 0.05430058175403064, 'O': 0.05502056748246712, \n",
    "#    'P': 0.07147918642363447, 'Q': 0.07271629098862856, 'R': 0.0840926138566933, \n",
    "#    'S': 0.06737658637280254\n",
    "#}\n",
    "# t = 11:\n",
    "#prev_states[2]: 0.009388775398326584\n",
    "#value_function['J']: 0.009081736826626132\n",
    "#10 0.49279075198489264\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test new n-step class on tictactoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import tictactoe\n",
    "reload(tictactoe)\n",
    "TicTacToeGame, TicTacToeExpert, test_player = tictactoe.TicTacToeGame, tictactoe.TicTacToeExpert, tictactoe.test_player\n",
    "\n",
    "import gamelearner\n",
    "reload(gamelearner)\n",
    "RandomPlayer, TDLearner, GameController = gamelearner.RandomPlayer, gamelearner.TDLearner, gamelearner.GameController"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tictactoe import TicTacToeGame, TicTacToeExpert, test_player\n",
    "from gamelearner import RandomPlayer, TDLearner, GameController"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ _ _\n",
      "_ _ _\n",
      "_ _ _\n"
     ]
    }
   ],
   "source": [
    "game = TicTacToeGame()\n",
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = RandomPlayer(seed=1)\n",
    "p2 = Agent(RandomPolicy(seed=5), name=\"Random\", initial_value=0.0, \n",
    "           gamma=1.0)\n",
    "p2.initialize(game, role=1)\n",
    "p2.t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ _ X\n",
      "_ _ _\n",
      "_ _ _\n"
     ]
    }
   ],
   "source": [
    "p1.make_move(game, 1)\n",
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 0.0}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards = game.get_rewards()\n",
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-534c5254ed1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-1e6ebb6203cf>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, game, reward, show, debug)\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0mtau\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 self.td_lambda_update(prev_states, prev_rewards, tau,\n\u001b[0;32m--> 152\u001b[0;31m                                       debug=debug)\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-1e6ebb6203cf>\u001b[0m in \u001b[0;36mtd_lambda_update\u001b[0;34m(self, prev_states, prev_rewards, tau, t_max, debug)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;31m# value updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_states\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;31m#if debug and self.t == 177:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "p2.update(game, rewards[2])\n",
    "p2.t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2.make_move(game, 2)\n",
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = game.get_rewards()\n",
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.make_move(game, 1)\n",
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = game.get_rewards()\n",
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pdb on\n",
    "p2.update(game, rewards[2])\n",
    "%pdb off\n",
    "p2.t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2.value_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2.make_move(game, 2)\n",
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = game.get_rewards()\n",
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.make_move(game, 1)\n",
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = game.get_rewards()\n",
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pdb on\n",
    "p2.update(game, rewards[2])\n",
    "%pdb off\n",
    "p2.t += 1\n",
    "p2.t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2.value_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2.make_move(game, 2)\n",
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed tests on discounting calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "prev_rewards = [None] + [random.random() for _ in range(t + 1)]\n",
    "\n",
    "n = 10\n",
    "gamma = 0.9\n",
    "t_max = np.inf\n",
    "t = 11\n",
    "\n",
    "tau = t - n + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pure Python version 1\n",
    "\n",
    "g = sum([gamma**(i - tau - 1)*prev_rewards[i] \n",
    "                 for i in range(tau + 1, min(tau + n, t_max) + 1)])\n",
    "\n",
    "assert g == 3.206433194301773"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy version 1\n",
    "\n",
    "g = np.sum(np.power(np.full((n), gamma), np.arange(min(n, t_max - tau)))*prev_rewards[tau+1:min(tau + n, t_max) + 1])\n",
    "\n",
    "assert np.isclose(g, 3.206433194301773) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pure Python version 2\n",
    "\n",
    "g = sum([gamma**(i)*prev_rewards[i + tau + 1] \n",
    "                 for i in range(0, min(n, t_max - tau))])\n",
    "\n",
    "assert g == 3.206433194301773"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pure Python version 3\n",
    "\n",
    "def discounted_sequence_generator(sequence, gamma):\n",
    "    \"\"\"Returns a generator that yields each item of sequence\n",
    "    discounted by gamma at each time-step.\n",
    "    \n",
    "    Example:\n",
    "    >>> list(discounted_sequence_generator([10.0]*5, 0.75))\n",
    "    [10.0, 7.5, 5.625, 4.21875, 3.1640625]\n",
    "    \"\"\"\n",
    "    \n",
    "    x = 1.0\n",
    "    for item in sequence:\n",
    "        yield x*item\n",
    "        x = x*gamma\n",
    "\n",
    "g = sum(list(discounted_sequence_generator(prev_rewards[tau+1:min(tau + n, t_max) + 1], gamma)))\n",
    "\n",
    "assert g == 3.206433194301773"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy version 2\n",
    "\n",
    "def sum_discounted_rewards(prev_rewards, tau, n, t_max, gamma):\n",
    "    \n",
    "    gamma_values = np.full(10, gamma)\n",
    "    gamma_values[0] = 1.0\n",
    "    rewards = prev_rewards[tau+1:min(tau + n, t_max) + 1]\n",
    "    \n",
    "    return sum(np.cumprod(gamma_values)*rewards)\n",
    "\n",
    "g = sum_discounted_rewards(prev_rewards, tau, n, t_max, gamma)\n",
    "\n",
    "assert np.isclose(g, 3.206433194301773) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit g = sum([gamma**(i - tau - 1)*prev_rewards[i] for i in range(tau + 1, min(tau + n, t_max) + 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit g = sum([gamma**(i)*prev_rewards[i + tau + 1] for i in range(0, min(n, t_max - tau))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit g = sum_discounted_rewards(prev_rewards, tau, n, t_max, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit g = sum(list(discounted_sequence_generator(prev_rewards[tau+1:min(tau + n, t_max) + 1], gamma)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit g = np.sum(np.power(np.full((n), gamma), np.arange(min(n, t_max - tau)))*prev_rewards[tau+1:min(tau + n, t_max) + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_sum_generator(sequence, gamma):\n",
    "    \n",
    "    x = 1.0\n",
    "    \n",
    "    for item in sequence:\n",
    "        \n",
    "        yield x*item\n",
    "        x = x*gamma\n",
    "\n",
    "list(discounted_sum_generator([1.0]*5, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Outputs should look like this before summing\n",
    "[gamma**(i - tau - 1)*prev_rewards[i] \n",
    "                 for i in range(tau + 1, min(tau + n, t_max) + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
