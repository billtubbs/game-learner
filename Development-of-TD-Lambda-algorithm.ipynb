{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding of TD($\\lambda$) learner algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from platform import python_version\n",
    "python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on Chapter 12: Eligibility Traces from Sutton & Barto\n",
    "\n",
    "### TD($\\lambda$)\n",
    "\n",
    "<IMG SRC=\"images/tdlambda.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same environment used in Chapter 7:\n",
    "\n",
    "### Example 7.1 Random walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from randomwalk import RandomWalkGame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = RandomWalkGame(size=19, terminal_rewards={'T1': -1.0, 'T2': 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1 _ _ _ _ _ _ _ _ _ J _ _ _ _ _ _ _ _ _ T2\n"
     ]
    }
   ],
   "source": [
    "# Initial state\n",
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1 _ _ _ _ _ _ _ _ _ _ K _ _ _ _ _ _ _ _ T2\n"
     ]
    }
   ],
   "source": [
    "# Moves are 'l' or 'r' (left or right)\n",
    "game.make_move((1, 'r'))\n",
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zero rewards...\n",
    "game.get_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ T2\n"
     ]
    }
   ],
   "source": [
    "# until terminal state reached\n",
    "while not game.state in game.terminal_states:\n",
    "    game.make_move((1, 'r'))\n",
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.get_terminal_rewards()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For verification purposes, let's try to replicate the results below which should match Figure 12.8 from the book which looks like this:\n",
    "\n",
    "<IMG SRC=\"images/fig_12_8.png\">    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recap: this is the n-step TD update calculation from chapter 7\n",
    "\n",
    "## N-Step TD Update - for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_sequence_generator(sequence, gamma):\n",
    "    \"\"\"Returns a generator that yields each item of sequence\n",
    "    discounted by gamma at each time-step.\n",
    "    \n",
    "    Example:\n",
    "    >>> list(discounted_sequence_generator([10.0]*5, 0.75))\n",
    "    [10.0, 7.5, 5.625, 4.21875, 3.1640625]\n",
    "    \"\"\"\n",
    "\n",
    "    x = 1.0\n",
    "    for item in sequence:\n",
    "        yield x * item\n",
    "        x = x * gamma\n",
    "\n",
    "\n",
    "def td_n_step_update(value_function, prev_states, prev_rewards, tau, gamma,\n",
    "                     learning_rate, t_max=np.inf, show=False):\n",
    "    \"\"\"Updates the value in value_function for the state\n",
    "    that occurred in past timestep tau using the n-step TD\n",
    "    algorithm.\n",
    "    \n",
    "    Note:\n",
    "    tau = t - n + 1\n",
    "\n",
    "    Where:\n",
    "        t is the current timestep\n",
    "        n is the number of steps to look forward  \n",
    "\n",
    "    If tau < 0 no update is possible so none is made.\n",
    "\n",
    "    Args:\n",
    "        value_function (dict): Dictionary of state values.\n",
    "        prev_states (list): List of previous states.\n",
    "        prev_rewards (list): List of previous rewards.\n",
    "        tau (int): The timestep of the state that will have\n",
    "            its value updated.\n",
    "        gamma (float): Discount rate.\n",
    "        learning_rate (float): Learning rate parameter.\n",
    "        t_max (int or np.inf): The maximum timestep to include.  \n",
    "            Set to np.inf or a high number if you want to\n",
    "            include all timesteps from tau to tau + n.  Set\n",
    "            to t if the game has eneded.\n",
    "        show (bool): Print messages if True.\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(prev_states) == len(prev_rewards)\n",
    "\n",
    "    if tau >= 0:\n",
    "\n",
    "        assert tau < len(prev_states), \"Not enough past states.\"\n",
    "\n",
    "        discounted_rewards = discounted_sequence_generator(\n",
    "            prev_rewards[tau+1:min(tau + n, t_max) + 1], \n",
    "            gamma\n",
    "        )\n",
    "        g = sum(list(discounted_rewards))\n",
    "\n",
    "        if tau + n < t_max:\n",
    "            g += gamma**n * value_function[prev_states[tau + n]]\n",
    "\n",
    "        # Update value of state at timestep tau\n",
    "        state_key = prev_states[tau]\n",
    "        state_value = value_function[state_key]\n",
    "\n",
    "        if show:\n",
    "            print(f\"prev_states[{tau}]: {state_value}\")\n",
    "\n",
    "        value_function[state_key] = state_value + \\\n",
    "            learning_rate * (g - state_value)\n",
    "\n",
    "        if show:\n",
    "            print(f\"value_function[{state_key.__repr__()}]: {value_function[state_key]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the equivalent TD($\\lambda$) update calculation\n",
    "\n",
    "## TD($\\lambda$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_lambda_weight_update(weights, value_current_state, value_prev_state, reward, \n",
    "                            dv_dw, z, lam, gamma, learning_rate):\n",
    "    \"\"\"Updates the weights of a value function for the \n",
    "    previous state using the TD-Lambda algorithm.\n",
    "    \n",
    "    Args:\n",
    "        weights (array): Array of weights (value function \n",
    "            parameters).\n",
    "        value_current_state (float): Value estimate for current state.\n",
    "        value_prev_state (float): Value estimate for previous state.\n",
    "        reward (float): Reward at current state.\n",
    "        dv_dw (array): Partial derivatives of value function\n",
    "            w.r.t. the weights at current state.\n",
    "        z (array): Eligibility trace vector.\n",
    "        lam (float): Lambda parameter.\n",
    "        gamma (float): Discount factor.\n",
    "        learning_rate (float): Learning rate parameter.\n",
    "    \"\"\"\n",
    "\n",
    "    assert dv_dw.shape == z.shape\n",
    "    z = gamma * lam * z + dv_dw\n",
    "    td_error = reward + gamma * value_current_state - value_prev_state\n",
    "    \n",
    "    return weights + learning_rate * td_error * z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to this code from here:\n",
    "# https://amreis.github.io/ml/reinf-learn/2017/11/02/reinforcement-learning-eligibility-traces.html\n",
    "#\n",
    "# td_error = reward + gamma * state_values[new_state] - state_values[state]\n",
    "# state_values = state_values + alpha * td_error * eligibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Approximation Functions for use with TD-$\\lambda$\n",
    "\n",
    "To use TD-$\\lambda$ we need a differentiable value function applicable to the problem (i.e. the environment's states and actions).  Here are a couple of general-purpose value function classes that might be useful for the RandomWalk environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplest possible value function - discrete values for each state\n",
    "state_values = np.zeros(n_states)  # initial guess = 0 value\n",
    "eligibility = np.zeros(n_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1  0.   0.1  0.2  0.3  0.4\n",
      "  0.5  0.6  0.7  0.8  0.9]\n"
     ]
    }
   ],
   "source": [
    "def calculate_true_values(game):\n",
    "    \"\"\"Returns a list of the true values of states in a\n",
    "    RandomWalk game.\n",
    "    \"\"\"\n",
    "\n",
    "    xp = [0, game.size+1]\n",
    "    fp = [-1.0, 1.0]\n",
    "\n",
    "    true_values = np.interp(np.arange(game.size + 2), xp, fp)[1:-1]\n",
    "    \n",
    "    return true_values\n",
    "\n",
    "\n",
    "def calculate_rms_error(values, true_values):\n",
    "    \"\"\"Root-mean-squared error of values compared to true values.\n",
    "    \"\"\"\n",
    "\n",
    "    return np.sqrt(np.sum((np.array(values) - \n",
    "                           np.array(true_values))**2)/len(values))\n",
    "\n",
    "size = 19\n",
    "game = RandomWalkGame(size=size, \n",
    "                      terminal_rewards={'T1': -1.0, 'T2': 1.0})\n",
    "true_values = calculate_true_values(game)\n",
    "\n",
    "values = np.zeros(19)\n",
    "error = calculate_rms_error(values, true_values)\n",
    "assert error == 0.5477225575051662\n",
    "\n",
    "print(true_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Parameter Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) N-Step TD Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter number of repetitions (1-100). Warning: 100 repetitions takes about 7 mins\n",
      "Press enter for default (1):1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'temporal_difference' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-303101fca4e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mtemporal_difference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mrms_errors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculate_rms_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'temporal_difference' is not defined"
     ]
    }
   ],
   "source": [
    "# Allow user to set number of repetitions (default 1)\n",
    "print(\"Enter number of repetitions (1-100). Warning: 100 repetitions takes about 7 mins\")\n",
    "default_input = '1'\n",
    "message = input(f\"Press enter for default ({default_input}):\")\n",
    "\n",
    "try:\n",
    "    repetitions = int(message or default_input)\n",
    "except ValueError as err:\n",
    "    print(\"Didn't understand that. Please try again.\")\n",
    "    raise err\n",
    "\n",
    "# Environment parameters\n",
    "size = 19\n",
    "terminal_rewards = {'T1': -1.0, 'T2': 1.0}\n",
    "game = RandomWalkGame(size=size, terminal_rewards=terminal_rewards)\n",
    "n_iters = 10\n",
    "true_values = calculate_true_values(game)\n",
    "\n",
    "# Agent parameters\n",
    "initial_value = 0.0\n",
    "gamma = 1.0\n",
    "\n",
    "# Iterate over learning_rates and n-values\n",
    "learning_rates = np.logspace(-2, 0.0, 50)\n",
    "n_values = [2**i for i in range(10)]\n",
    "\n",
    "avg_rms_error_sets = {}\n",
    "\n",
    "t0 = time.time()\n",
    "for n in n_values:\n",
    "\n",
    "    # TEMPORARY OVERRIDE FOR TESTING\n",
    "    #n = 10\n",
    "    rms_error_sets = []\n",
    "\n",
    "    rng1 = random.Random(5)\n",
    "\n",
    "    for repetition in range(repetitions):\n",
    "    \n",
    "        rms_errors = []\n",
    "        seed = repetition\n",
    "\n",
    "        for learning_rate in learning_rates:\n",
    "\n",
    "            # Initialise value function\n",
    "            value = np.zeros(size + 2)\n",
    "            \n",
    "            for iteration in range(n_iters):\n",
    "                temporal_difference(value, n, learning_rate)\n",
    "\n",
    "            rms_errors.append(calculate_rms_error(value[1:size+1], true_values))\n",
    "            #break\n",
    "        \n",
    "        rms_error_sets.append(rms_errors)\n",
    "\n",
    "    avg_rms_error_sets[n] = np.vstack(rms_error_sets).mean(axis=0)\n",
    "    \n",
    "    # OVERRIDE HERE TO SPEED UP\n",
    "    #break\n",
    "\n",
    "avg_rms_error_sets_master = avg_rms_error_sets.copy()\n",
    "\n",
    "t1 = time.time()\n",
    "total_time = t1 - t0\n",
    "print(f\"Execution time: {total_time:.1f}s\")\n",
    "\n",
    "print(f\"Produced results for n = {list(avg_rms_error_sets.keys())}\")\n",
    "#print(n, rms_results[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for n, rms_errors in avg_rms_error_sets.items():\n",
    "    ax.plot(learning_rates, rms_errors, label=f'n = {n:d}')\n",
    "plt.title('n-step TD value error on random walk (%d states)' % game.size)\n",
    "ax.set_xlabel('Learning rate (alpha)')\n",
    "ax.set_ylabel('RMS Error')\n",
    "ax.set_ylim(0.0, 0.55)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(f\"random-walk-n-step-td-{game.size}-{repetitions}.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we use or adapt scikit-learn models?\n",
    "\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(LinearRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class FunctionApproximator(ABC):\n",
    "    \"\"\"Abstract function approximation class.\"\"\"\n",
    "\n",
    "    def __init__(self, input_shape, d, output_shape=(1,),\n",
    "                 init_value=0):\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.d = d\n",
    "        self._weights = np.full(d, init_value)\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        return self._weights\n",
    "\n",
    "    @weights.setter\n",
    "    def weights(self, values):\n",
    "        self._weights[:] = values\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, x):\n",
    "        \"\"\"Returns the function estimate at x. x may\n",
    "        be a single point or sequence of points.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @abstractmethod\n",
    "    def dw(self, x):\n",
    "        \"\"\"Returns the partial derivatives w.r.t. the weights\n",
    "        (parameters) at x.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class LinearValueFunctionForDiscreteStates(FunctionApproximator):\n",
    "    \"\"\"Uses the following simple linear functon to approximate\n",
    "    the state-values of an environment:\n",
    "    \n",
    "    v_hat = w_0 + w_1 * x\n",
    "    \n",
    "    where x is a scalar input feature generated from the \n",
    "    environment's state.  For example, this might work well if \n",
    "    the state values are a one-dimensional discretized \n",
    "    representation of a linear one-dimensional state-space\n",
    "    where x represents the position.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, states, terminal_states=None, init_value=0):\n",
    "        \n",
    "        if terminal_states is None:\n",
    "            terminal_states = []\n",
    "            self.states = states\n",
    "        else:\n",
    "            self.states = [s for s in states if s not in terminal_states]\n",
    "        self.terminal_states = terminal_states\n",
    "        self.x = np.full(len(self.states), np.nan)\n",
    "        self.d = len(self.states)\n",
    "        input_shape = (1, )\n",
    "        super().__init__(input_shape, self.d, output_shape=(1,), init_value=init_value)\n",
    "        self.z = np.full_like(self.weights, 0.)\n",
    "\n",
    "    def __call__(self, state):\n",
    "        \"\"\"Returns the value estimate for the given state.\n",
    "        \"\"\"\n",
    "        if state in game.terminal_states:\n",
    "            return 0.\n",
    "        x = self.input_mapping(state)\n",
    "        return np.sum(self.weights * x)\n",
    "    \n",
    "    def input_mapping(self, state):\n",
    "        self.x[:] = 0\n",
    "        self.x[self.states.index(state)] = 1\n",
    "        return self.x\n",
    "    \n",
    "    def dw(self, state):\n",
    "        \"\"\"Returns the partial derivatives of the value \n",
    "        function w.r.t. the weights (parameters) at the\n",
    "        given state.\n",
    "        \"\"\"\n",
    "        if state in game.terminal_states:\n",
    "            # TODO: Is this the correct thing to do?\n",
    "            self.x[:] = 0\n",
    "            x = self.x\n",
    "        else:\n",
    "            x = self.input_mapping(state)\n",
    "        return x\n",
    "\n",
    "    def update_weights(self, current_state, prev_state, reward,\n",
    "                       lam, gamma, learning_rate):\n",
    "        \n",
    "        self.weights[:] = td_lambda_weight_update(\n",
    "            self.weights, \n",
    "            current_state_value=self(current_state), \n",
    "            prev_state_value=self(prev_state), \n",
    "            reward=reward,\n",
    "            dv_dw=self.dw(prev_state),\n",
    "            z=self.z,\n",
    "            lam=lam,\n",
    "            gamma=gamma,\n",
    "            learning_rate=learning_rate\n",
    "        )\n",
    "\n",
    "\n",
    "class ValueFunctionDiscreteStatesLinearApproximator(FunctionApproximator):\n",
    "    \"\"\"Uses the following simple linear functon to approximate\n",
    "    the state-values of an environment:\n",
    "    \n",
    "    v_hat = w_0 + w_1 * x\n",
    "    \n",
    "    where x is a scalar input feature that is generated from\n",
    "    the environment's state assuming all the state values are\n",
    "    a one-dimensional discretized representation of a linear\n",
    "    state-space.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, states):\n",
    "        self.states = states\n",
    "        self.n_states = len(self.states)\n",
    "        self.terminal_states = [s for s in game.terminal_states]\n",
    "        self.weights = np.zeros(2)  # Linear function approximator\n",
    "        self.z = np.full_like(self.weights, 0)  # Eligibility trace vector\n",
    "\n",
    "    def input_mapper(self, state_key):\n",
    "        \"\"\"Converts a discrete state into a (scalar) feature \n",
    "        value assuming all the state values represent a one-\n",
    "        dimensional linear state-space.\n",
    "        \n",
    "        returns:\n",
    "            x (float): (0 <= x <= 1).\n",
    "        \"\"\"\n",
    "        return self.states.index(state_key) / (self.n_states - 1)\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"Return values based on state key (or sequence of keys).\n",
    "        \"\"\"\n",
    "        \n",
    "        if isinstance(key, str):\n",
    "            return self.value(key)\n",
    "        if not isinstance(key, np.ndarray):\n",
    "            # Convert a simple index x[y] to an array\n",
    "            key = np.array(key)\n",
    "\n",
    "        # Handle the different dimensional cases\n",
    "        return np.fromiter((self.value(xi) for xi in key), key.dtype)\n",
    "        \n",
    "    def value(self, state_key):\n",
    "        \"\"\"Returns estimate of value of state.\n",
    "        \"\"\"\n",
    "        if state_key in self.terminal_states:\n",
    "            return 0\n",
    "        x = self.map_inputs(state_key)\n",
    "        return self.weights[0] + self.weights[1] * x\n",
    "\n",
    "    def dw(self, state_key):\n",
    "        \"\"\"Partial derivatives of value function w.r.t.\n",
    "        weights at given state.\n",
    "        \"\"\"\n",
    "        if state_key in self.terminal_states:\n",
    "            return np.array([1.0, 0.0])\n",
    "        x = self.map_inputs(state_key)\n",
    "        return np.array([1.0, x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ValueFunctionDiscreteStates class\n",
    "states = game.states\n",
    "terminal_states = game.terminal_states\n",
    "value_function = ValueFunctionDiscreteStates(states, terminal_states, \n",
    "                                             init_value=0.5)\n",
    "\n",
    "assert len(value_function.states) == 19\n",
    "assert value_function.terminal_states == ['T1', 'T2']\n",
    "assert value_function.d == 19\n",
    "assert value_function.input_shape == (1,)\n",
    "assert value_function.weights.shape == (19,)\n",
    "assert value_function.z.shape == (19,)\n",
    "assert value_function.x.shape == (19,)\n",
    "assert np.all(value_function.weights == 0.5)\n",
    "assert np.all(value_function.z == 0)\n",
    "assert all([value_function(s) == 0.5 for s in value_function.states])\n",
    "assert all([value_function(s) == 0.0 for s in \n",
    "            value_function.terminal_states])\n",
    "assert np.array_equal(\n",
    "    value_function.dw('D'),\n",
    "    np.array([0., 0., 0., 1., 0., 0., 0., 0., 0., \n",
    "              0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    ")\n",
    "value_function.weights = [0]*19\n",
    "assert np.array_equal(\n",
    "    value_function.weights,\n",
    "    np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., \n",
    "              0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    ")\n",
    "current_state = 'K'\n",
    "prev_state = 'J'\n",
    "reward = 1\n",
    "lam = 0.5\n",
    "gamma = 1.0\n",
    "learning_rate = 0.1\n",
    "value_function.update_weights(current_state, prev_state, reward,\n",
    "                              lam, gamma, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array_equal(\n",
    "    value_function.weights,\n",
    "    np.array([0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , \n",
    "              0.1, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ])\n",
    ")\n",
    "prev_state = current_state\n",
    "current_state = 'L'\n",
    "reward = 1\n",
    "lam = 0.5\n",
    "gamma = 1.0\n",
    "learning_rate = 0.1\n",
    "value_function.update_weights(current_state, prev_state, reward,\n",
    "                              lam, gamma, learning_rate)\n",
    "assert np.isclose(\n",
    "    value_function.weights,\n",
    "    np.array([0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.1, \n",
    "              0.1, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ])\n",
    ").all()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "value_function = ValueFunctionDiscreteStatesLinearApproximator(game)\n",
    "\n",
    "# Mapping states to a single input feature\n",
    "index = pd.Index(value_function.states, name='State')\n",
    "pd.DataFrame({\n",
    "    'x': {s: value_function.map_inputs(s) for s in value_function.states},\n",
    "    'Value estimate': value_function[value_function.states]\n",
    "}, index=index)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "value_function.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1 - Single Episode / Walk Right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) N-Step TD Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General TD parameters\n",
    "gamma = 1.0\n",
    "learning_rate = 0.1\n",
    "initial_value = 0.0\n",
    "\n",
    "# N-step TD parameters\n",
    "n = 10\n",
    "value_function = {state_key: initial_value for state_key in game.states}\n",
    "\n",
    "# Environment setup\n",
    "game = RandomWalkGame(size=19, terminal_rewards={'T1': -1.0, 'T2': 1.0})\n",
    "actions = ['r']*10\n",
    "role = 1\n",
    "\n",
    "prev_states = [game.start_state]\n",
    "prev_rewards = [None]\n",
    "t = 0\n",
    "while not game.game_over:\n",
    "\n",
    "    # Behaviour policy\n",
    "    move = (role, actions[t])\n",
    "    game.make_move(move)\n",
    "\n",
    "    # Get rewards\n",
    "    if not game.game_over:\n",
    "        reward = game.get_rewards()[role]\n",
    "    else:\n",
    "        reward = game.get_terminal_rewards()[role]\n",
    "\n",
    "    print(f\"{prev_states[-1]} -> {game.state}, {reward}\")\n",
    "\n",
    "    state_key = game.generate_state_key(game.state, role)\n",
    "    prev_states.append(state_key)\n",
    "    prev_rewards.append(reward)\n",
    "\n",
    "    if not game.game_over:\n",
    "        # Update the value for state in timestep tau\n",
    "        tau = t - n + 1\n",
    "        td_n_step_update(value_function, prev_states, prev_rewards, \n",
    "                         tau, gamma, learning_rate)\n",
    "\n",
    "    # Update timestep\n",
    "    t += 1\n",
    "\n",
    "assert game.game_over\n",
    "\n",
    "# Complete final state-value updates for timesteps tau\n",
    "# to current (terminal) timestep\n",
    "for tau in range(t - n, t):\n",
    "    td_n_step_update(value_function, prev_states, prev_rewards, \n",
    "                     tau, gamma, learning_rate, t_max=t)\n",
    "\n",
    "# Value function after 1 episode\n",
    "pd.Series(value_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) TD($\\lambda$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General TD parameters\n",
    "gamma = 1.0\n",
    "learning_rate = 0.1\n",
    "initial_value = 0.0\n",
    "\n",
    "# TD-Lambda parameters\n",
    "lam = 0.5\n",
    "value_function = ValueFunctionDiscreteStates(game.states, \n",
    "                                             game.terminal_states, \n",
    "                                             init_value=0.0)\n",
    "\n",
    "# Environment setup\n",
    "game = RandomWalkGame(size=19, terminal_rewards={'T1': -1.0, 'T2': 1.0})\n",
    "actions = ['r']*10\n",
    "role = 1\n",
    "\n",
    "prev_state = game.generate_state_key(game.start_state, role)\n",
    "prev_reward = None\n",
    "t = 0\n",
    "while not game.game_over:\n",
    "\n",
    "    # Behaviour policy\n",
    "    move = (role, actions[t])\n",
    "    game.make_move(move)\n",
    "\n",
    "    # Get rewards\n",
    "    if not game.game_over:\n",
    "        reward = game.get_rewards()[role]\n",
    "    else:\n",
    "        reward = game.get_terminal_rewards()[role]\n",
    "    \n",
    "    current_state = game.generate_state_key(game.state, role)\n",
    "    current_state_value = value_function(current_state)\n",
    "\n",
    "    print(f\"{prev_state} -> {current_state}, {reward}\")\n",
    "\n",
    "    # Update the value function weights\n",
    "    value_function.update_weights(current_state, prev_state, reward,\n",
    "                                  lam, gamma, learning_rate)\n",
    "\n",
    "    prev_state = current_state\n",
    "    prev_reward = reward\n",
    "\n",
    "    # Update timestep\n",
    "    t += 1\n",
    "\n",
    "# Value function after 1 episode\n",
    "values = {state: value_function(state) for state in game.states}\n",
    "pd.Series(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) TD($\\lambda$) With Simple Linear Approximation of State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General TD parameters\n",
    "gamma = 1.0\n",
    "learning_rate = 0.1\n",
    "initial_value = 0.0\n",
    "\n",
    "# TD-Lambda parameters\n",
    "lam = 0.5\n",
    "value_function = ValueApproxFunction(game)\n",
    "\n",
    "# Environment setup\n",
    "game = RandomWalkGame(size=19, terminal_rewards={'T1': -1.0, 'T2': 1.0})\n",
    "actions = ['r']*10\n",
    "role = 1\n",
    "\n",
    "prev_states = [game.start_state]\n",
    "prev_rewards = [None]\n",
    "t = 0\n",
    "while not game.game_over:\n",
    "\n",
    "    # Behaviour policy\n",
    "    move = (role, actions[t])\n",
    "    game.make_move(move)\n",
    "\n",
    "    # Get rewards\n",
    "    if not game.game_over:\n",
    "        reward = game.get_rewards()[role]\n",
    "    else:\n",
    "        reward = game.get_terminal_rewards()[role]\n",
    "\n",
    "    state_key = game.generate_state_key(game.state, role)\n",
    "    prev_state = prev_states[-1]\n",
    "    prev_states.append(state_key)\n",
    "    prev_rewards.append(reward)\n",
    "\n",
    "    if not game.game_over:\n",
    "        # Update the value for state in timestep tau\n",
    "        td_lambda_update(value_function, prev_state, state_key, \n",
    "                         reward, t, lam, gamma)\n",
    "\n",
    "    # Update timestep\n",
    "    t += 1\n",
    "\n",
    "assert game.game_over\n",
    "\n",
    "# Complete state-value update for final timestep (to terminal state)\n",
    "td_lambda_update(value_function, prev_state, state_key, reward, t, lam, gamma)\n",
    "\n",
    "# Value function after 1 episode\n",
    "values = {state: value_function.value(state) for state in game.states}\n",
    "pd.Series(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2 - 10 Episodes Alternating Right, Left, Right, ... etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) N-Step TD Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "\n",
    "# General TD parameters\n",
    "gamma = 1.0\n",
    "learning_rate = 0.1\n",
    "initial_value = 0.0\n",
    "\n",
    "# N-step TD parameters\n",
    "n = 10\n",
    "value_function = {state_key: initial_value for state_key in game.states}\n",
    "\n",
    "# Environment setup\n",
    "game = RandomWalkGame(size=19, terminal_rewards={'T1': -1.0, 'T2': 1.0})\n",
    "actions = ['r', 'l', 'r', 'l', 'r', 'l', 'r', 'l', 'r', 'l']\n",
    "role = 1\n",
    "\n",
    "saved_values = []\n",
    "for episode in range(0, n_runs):\n",
    "    game.reset()\n",
    "    action = actions[episode]  # Different action each episode\n",
    "\n",
    "    prev_states = [game.start_state]\n",
    "    prev_rewards = [None]\n",
    "    t = 0\n",
    "    while not game.game_over:\n",
    "\n",
    "        # Behaviour policy\n",
    "        move = (role, action)\n",
    "        game.make_move(move)\n",
    "\n",
    "        # Get rewards\n",
    "        if not game.game_over:\n",
    "            reward = game.get_rewards()[role]\n",
    "        else:\n",
    "            reward = game.get_terminal_rewards()[role]\n",
    "        if reward != 0:\n",
    "            print(f\"{prev_state} -> {current_state}, {reward}\")\n",
    "\n",
    "        state_key = game.generate_state_key(game.state, role)\n",
    "        prev_states.append(state_key)\n",
    "        prev_rewards.append(reward)\n",
    "\n",
    "        if not game.game_over:\n",
    "            # Update the value for state in timestep tau\n",
    "            tau = t - n + 1\n",
    "            td_n_step_update(value_function, prev_states, prev_rewards, \n",
    "                             tau, gamma, learning_rate)\n",
    "\n",
    "        # Update timestep\n",
    "        t += 1\n",
    "\n",
    "    assert game.game_over\n",
    "\n",
    "    # Complete final state-value updates for timesteps tau\n",
    "    # to current (terminal) timestep\n",
    "    for tau in range(t - n, t):\n",
    "        td_n_step_update(value_function, prev_states, prev_rewards, \n",
    "                         tau, gamma, learning_rate, t_max=t)\n",
    "    \n",
    "    saved_values.append(pd.Series(value_function, name=episode))\n",
    "\n",
    "# Final value estimates\n",
    "print(pd.Series(value_function))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_value_estimates(saved_values):\n",
    "    terminal_states = game.terminal_states\n",
    "    combined_results = pd.concat(saved_values, axis=1).drop(terminal_states)\n",
    "    combined_results.plot(style='o-', figsize=(8,4))\n",
    "    plt.grid()\n",
    "    plt.xlabel('State')\n",
    "    plt.ylabel('Value Estimate')\n",
    "    plt.title('State Value Estimates after Each Episode')\n",
    "    plt.legend(title='Episode', bbox_to_anchor=(1.04,1), \n",
    "               loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "plot_value_estimates(saved_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) TD(λ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "\n",
    "# General TD parameters\n",
    "gamma = 1.0\n",
    "learning_rate = 0.1\n",
    "initial_value = 0.0\n",
    "\n",
    "# TD-Lambda parameters\n",
    "lam = 1.0\n",
    "value_function = ValueFunctionDiscreteStates(game.states, \n",
    "                                             game.terminal_states, \n",
    "                                             init_value=0.0)\n",
    "\n",
    "# Environment setup\n",
    "game = RandomWalkGame(size=19, terminal_rewards={'T1': -1.0, 'T2': 1.0})\n",
    "actions = ['r', 'l', 'r', 'l', 'r', 'l', 'r', 'l', 'r', 'l']\n",
    "role = 1\n",
    "\n",
    "saved_values = []\n",
    "for episode in range(0, n_runs):\n",
    "    game.reset()\n",
    "    action = actions[episode]  # Different action each episode\n",
    "\n",
    "    prev_state = game.generate_state_key(game.start_state, role)\n",
    "    prev_reward = None\n",
    "    t = 0\n",
    "    while not game.game_over:\n",
    "\n",
    "        # Behaviour policy\n",
    "        move = (role, action)\n",
    "        game.make_move(move)\n",
    "\n",
    "        # Get rewards\n",
    "        if not game.game_over:\n",
    "            reward = game.get_rewards()[role]\n",
    "        else:\n",
    "            reward = game.get_terminal_rewards()[role]\n",
    "\n",
    "        current_state = game.generate_state_key(game.state, role)\n",
    "        current_state_value = value_function(current_state)\n",
    "        if reward != 0:\n",
    "            print(f\"{prev_state} -> {current_state}, {reward}\")\n",
    "\n",
    "        # Update the value function weights\n",
    "        value_function.update_weights(current_state, prev_state, reward,\n",
    "                                      lam, gamma, learning_rate)\n",
    "\n",
    "        prev_state = current_state\n",
    "        prev_reward = reward\n",
    "\n",
    "        # Update timestep\n",
    "        t += 1\n",
    "    \n",
    "    values = {state: value_function(state) for state in game.states}\n",
    "    saved_values.append(pd.Series(values, name=episode))\n",
    "\n",
    "# Final value estimates\n",
    "pd.Series(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_value_estimates(saved_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) TD($\\lambda$) With Linear Input Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "\n",
    "# General TD parameters\n",
    "gamma = 1.0\n",
    "learning_rate = 0.1\n",
    "initial_value = 0.0\n",
    "\n",
    "# TD-Lambda parameters\n",
    "value_function = ValueApproxFunction(game)\n",
    "lam = 0.5\n",
    "\n",
    "# Environment setup\n",
    "game = RandomWalkGame(size=19, terminal_rewards={'T1': -1.0, 'T2': 1.0})\n",
    "#actions = ['r']*10\n",
    "actions = ['r', 'l', 'r', 'l', 'r', 'l', 'r', 'l', 'r', 'l']\n",
    "role = 1\n",
    "\n",
    "saved_values = []\n",
    "for episode in range(0, n_runs):\n",
    "    game.reset()\n",
    "    action = actions[episode]  # Different action each episode\n",
    "\n",
    "    prev_states = [game.start_state]\n",
    "    prev_rewards = [None]\n",
    "    t = 0\n",
    "    while not game.game_over:\n",
    "\n",
    "        # Behaviour policy\n",
    "        move = (role, action)\n",
    "        game.make_move(move)\n",
    "\n",
    "        # Get rewards\n",
    "        if not game.game_over:\n",
    "            reward = game.get_rewards()[role]\n",
    "        else:\n",
    "            reward = game.get_terminal_rewards()[role]\n",
    "\n",
    "        state_key = game.generate_state_key(game.state, role)\n",
    "        prev_state = prev_states[-1]\n",
    "        prev_states.append(state_key)\n",
    "        prev_rewards.append(reward)\n",
    "\n",
    "        if not game.game_over:\n",
    "            # Update the value for state in timestep tau\n",
    "            td_lambda_update(value_function, prev_state, state_key, \n",
    "                             reward, t, lam, gamma)\n",
    "\n",
    "        # Update timestep\n",
    "        t += 1\n",
    "\n",
    "    assert game.game_over\n",
    "\n",
    "    # Complete state-value update for final timestep (to terminal state)\n",
    "    td_lambda_update(value_function, prev_state, state_key, \n",
    "                     reward, t, lam, gamma)\n",
    "\n",
    "    values = {state: value_function.value(state) for state in game.states}\n",
    "    saved_values.append(pd.Series(values, name=episode))\n",
    "\n",
    "# Final value estimates\n",
    "print(pd.Series(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_value_estimates(saved_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 3 - 10 Episodes with Random Walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) N-Step TD Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "\n",
    "# General TD parameters\n",
    "gamma = 1.0\n",
    "learning_rate = 0.1\n",
    "initial_value = 0.0\n",
    "\n",
    "# N-step TD parameters\n",
    "n = 10\n",
    "value_function = {state_key: initial_value for state_key in game.states}\n",
    "\n",
    "# Environment setup\n",
    "game = RandomWalkGame(size=19, terminal_rewards={'T1': -1.0, 'T2': 1.0})\n",
    "actions = ['r', 'l']\n",
    "role = 1\n",
    "\n",
    "saved_values = []\n",
    "for episode in range(0, n_runs):\n",
    "    game.reset()\n",
    "\n",
    "    prev_states = [game.start_state]\n",
    "    prev_rewards = [None]\n",
    "    t = 0\n",
    "    while not game.game_over:\n",
    "\n",
    "        # Behaviour policy\n",
    "        action = random.choice(actions)\n",
    "        move = (role, action)\n",
    "        game.make_move(move)\n",
    "\n",
    "        # Get rewards\n",
    "        if not game.game_over:\n",
    "            reward = game.get_rewards()[role]\n",
    "        else:\n",
    "            reward = game.get_terminal_rewards()[role]\n",
    "\n",
    "        state_key = game.generate_state_key(game.state, role)\n",
    "        prev_states.append(state_key)\n",
    "        prev_rewards.append(reward)\n",
    "\n",
    "        if not game.game_over:\n",
    "            # Update the value for state in timestep tau\n",
    "            tau = t - n + 1\n",
    "            td_n_step_update(value_function, prev_states, prev_rewards, \n",
    "                             tau, gamma)\n",
    "\n",
    "        # Update timestep\n",
    "        t += 1\n",
    "\n",
    "    assert game.game_over\n",
    "\n",
    "    # Complete final state-value updates for timesteps tau\n",
    "    # to current (terminal) timestep\n",
    "    for tau in range(t - n, t):\n",
    "        td_n_step_update(value_function, prev_states, prev_rewards, \n",
    "                         tau, gamma, t_max=t)\n",
    "    \n",
    "    saved_values.append(pd.Series(value_function, name=episode))\n",
    "\n",
    "# Final value estimates\n",
    "print(pd.Series(value_function))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_value_estimates(saved_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) TD($\\lambda$) With Linear Input Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "\n",
    "# General TD parameters\n",
    "gamma = 1.0\n",
    "learning_rate = 0.1\n",
    "initial_value = 0.0\n",
    "\n",
    "# TD-Lambda parameters\n",
    "value_function = ValueApproxFunction(game)\n",
    "lam = 0.5\n",
    "\n",
    "# Environment setup\n",
    "game = RandomWalkGame(size=19, terminal_rewards={'T1': -1.0, 'T2': 1.0})\n",
    "actions = ['r', 'l']\n",
    "role = 1\n",
    "\n",
    "saved_values = []\n",
    "for episode in range(0, n_runs):\n",
    "    game.reset()\n",
    "\n",
    "    prev_states = [game.start_state]\n",
    "    prev_rewards = [None]\n",
    "    t = 0\n",
    "    while not game.game_over:\n",
    "\n",
    "        # Behaviour policy\n",
    "        action = random.choice(actions)\n",
    "        move = (role, action)\n",
    "        game.make_move(move)\n",
    "\n",
    "        # Get rewards\n",
    "        if not game.game_over:\n",
    "            reward = game.get_rewards()[role]\n",
    "        else:\n",
    "            reward = game.get_terminal_rewards()[role]\n",
    "\n",
    "        state_key = game.generate_state_key(game.state, role)\n",
    "        prev_state = prev_states[-1]\n",
    "        prev_states.append(state_key)\n",
    "        prev_rewards.append(reward)\n",
    "\n",
    "        if not game.game_over:\n",
    "            # Update the value for state in timestep tau\n",
    "            td_lambda_update(value_function, prev_state, state_key, \n",
    "                             reward, t, lam, gamma)\n",
    "\n",
    "        # Update timestep\n",
    "        t += 1\n",
    "\n",
    "    assert game.game_over\n",
    "\n",
    "    # Complete state-value update for final timestep (to terminal state)\n",
    "    td_lambda_update(value_function, prev_state, state_key, \n",
    "                     reward, t, lam, gamma)\n",
    "\n",
    "    values = {state: value_function.value(state) for state in game.states}\n",
    "    saved_values.append(pd.Series(values, name=episode))\n",
    "\n",
    "# Final value estimates\n",
    "print(pd.Series(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_value_estimates(saved_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
