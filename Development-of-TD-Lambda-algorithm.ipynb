{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development of TD($\\lambda$) learner algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from platform import python_version\n",
    "python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on Chapter 12: Eligibility Traces from Sutton & Barto\n",
    "\n",
    "### TD($\\lambda$)\n",
    "\n",
    "<IMG SRC=\"images/tdlambda.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 7.1 Random walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from randomwalk import RandomWalkGame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = RandomWalkGame(size=19, terminal_rewards={'T1': -1.0, 'T2': 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1 _ _ _ _ _ _ _ _ _ J _ _ _ _ _ _ _ _ _ T2\n"
     ]
    }
   ],
   "source": [
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1 _ _ _ _ _ _ _ _ _ _ K _ _ _ _ _ _ _ _ T2\n"
     ]
    }
   ],
   "source": [
    "game.make_move((1, 'r'))\n",
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.get_rewards()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For verification purposes, we want to replicate the results below which should match Figure 12.8 from the book which looks like this:\n",
    "\n",
    "<IMG SRC=\"images/fig_12_8.png\">    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Step TD Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_sequence_generator(sequence, gamma):\n",
    "    \"\"\"Returns a generator that yields each item of sequence\n",
    "    discounted by gamma at each time-step.\n",
    "    \n",
    "    Example:\n",
    "    >>> list(discounted_sequence_generator([10.0]*5, 0.75))\n",
    "    [10.0, 7.5, 5.625, 4.21875, 3.1640625]\n",
    "    \"\"\"\n",
    "    \n",
    "    x = 1.0\n",
    "    for item in sequence:\n",
    "        yield x*item\n",
    "        x = x*gamma\n",
    "\n",
    "\n",
    "def td_n_step_update(value_function, prev_states, prev_rewards, tau, gamma,\n",
    "                     t_max=np.inf, show=False):\n",
    "    \"\"\"Updates the value in value_function for the past state\n",
    "    that occurred in timestep tau using the n-step TD\n",
    "    algorithm.\n",
    "    \n",
    "    Note:\n",
    "    tau = t - n + 1\n",
    "\n",
    "    Where:\n",
    "        t is the current timestep\n",
    "        n is the lambda value  \n",
    "\n",
    "    If tau < 0 no update is possible so none is made.\n",
    "\n",
    "    Args:\n",
    "        value_function (dict): Dictionary of state values.\n",
    "        prev_states (list): List of previous states.\n",
    "        prev_rewards (list): List of previous rewards.\n",
    "        tau (int): The timestep of the state that will have\n",
    "            its value updated.\n",
    "        gamma (float): Discount rate.\n",
    "        t_max (int or np.inf): The maximum timestep to include.  \n",
    "            Set to np.inf or a high number if you want to\n",
    "            include all timesteps from tau to tau + n.  Set\n",
    "            to t if the game has eneded.\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(prev_states) == len(prev_rewards)\n",
    "\n",
    "    if tau >= 0:\n",
    "\n",
    "        assert tau < len(prev_states), \"Not enough past states.\"\n",
    "\n",
    "        discounted_rewards = discounted_sequence_generator(\n",
    "            prev_rewards[tau+1:min(tau + n, t_max) + 1], \n",
    "            gamma\n",
    "        )\n",
    "        g = sum(list(discounted_rewards))\n",
    "\n",
    "        if tau + n < t_max:\n",
    "            g += gamma**n * value_function[prev_states[tau + n]]\n",
    "\n",
    "        # Update value of state at timestep tau\n",
    "        state_key = prev_states[tau]\n",
    "        state_value = value_function[state_key]\n",
    "\n",
    "        if show:\n",
    "            print(f\"prev_states[{tau}]: {state_value}\")\n",
    "\n",
    "        value_function[state_key] = state_value + \\\n",
    "            learning_rate * (g - state_value)\n",
    "\n",
    "        if show:\n",
    "            print(f\"value_function[{state_key.__repr__()}]: {value_function[state_key]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD($\\lambda$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A    0.000000\n",
       "B    0.055556\n",
       "C    0.111111\n",
       "D    0.166667\n",
       "E    0.222222\n",
       "F    0.277778\n",
       "G    0.333333\n",
       "H    0.388889\n",
       "I    0.444444\n",
       "J    0.500000\n",
       "K    0.555556\n",
       "L    0.611111\n",
       "M    0.666667\n",
       "N    0.722222\n",
       "O    0.777778\n",
       "P    0.833333\n",
       "Q    0.888889\n",
       "R    0.944444\n",
       "S    1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ValueApproxFunction():\n",
    "    \n",
    "    def __init__(self, game):\n",
    "\n",
    "        self.states = [s for s in game.states if not s in game.terminal_states]\n",
    "        self.terminal_states = game.terminal_states  # TODO: Should save state keys here\n",
    "        self.n_states = len(self.states)\n",
    "        self.weights = np.zeros(2)  # Linear function approximator\n",
    "        self.z = np.full_like(self.weights, 0)  # Eligibility trace vector\n",
    "\n",
    "    def input_value(self, state_key):\n",
    "        \"\"\"Convert discrete state into a linear feature\n",
    "        value\n",
    "        \n",
    "        returns:\n",
    "            x (float): (0 <= x <= 1).\n",
    "        \"\"\"\n",
    "        return self.states.index(state_key) / (self.n_states - 1)\n",
    "    \n",
    "    def value(self, state_key):\n",
    "        if state_key in self.terminal_states:\n",
    "            return 0.0\n",
    "        x = self.input_value(state_key)\n",
    "        return self.weights[0] + self.weights[1] * x\n",
    "\n",
    "    def dw(self, state_key):\n",
    "        \"\"\"Partial derivatives of value function w.r.t.\n",
    "        weights at given state.\"\"\"\n",
    "        if state_key in self.terminal_states:\n",
    "            return np.array([1.0, 0.0])        \n",
    "        x = self.input_value(state_key)\n",
    "        return np.array([1.0, x])\n",
    "\n",
    "value_function = ValueApproxFunction(game)\n",
    "pd.Series({s: value_function.input_value(s) for s in value_function.states})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_function.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_function.value('J')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1. , 0.5])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_function.dw('J')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_lambda_update(value_function, prev_state_key, state_key, reward, t, lam, gamma, show=False):\n",
    "    \"\"\"Updates the value in value_function for the past state\n",
    "    using the TD-Lambda algorithm.\n",
    "    \n",
    "    Args:\n",
    "        value_function (ValueApproxFunction): Value function approximator.\n",
    "        state (str): State key.\n",
    "        reward (float): Reward value.\n",
    "        z (array): Eligibility trace vector.\n",
    "        t (int): The timestep of the state that will have\n",
    "            its value updated.\n",
    "        gamma (float): Discount factor.\n",
    "    \"\"\"\n",
    "\n",
    "    value_function.z = gamma * lam * value_function.z + value_function.dw(state_key)\n",
    "    current_state_value = value_function.value(state_key)\n",
    "    prev_state_value = value_function.value(prev_state_key)\n",
    "    td_error = reward + gamma * prev_state_value - current_state_value\n",
    "    value_function.weights += learning_rate * td_error * value_function.z\n",
    "\n",
    "    if show:\n",
    "        print(f\"value_function.value({state_key.__repr__()}): {current_state_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1 - Walk Right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) N-Step TD Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T1    0.0\n",
       "A     0.0\n",
       "B     0.0\n",
       "C     0.0\n",
       "D     0.0\n",
       "E     0.0\n",
       "F     0.0\n",
       "G     0.0\n",
       "H     0.0\n",
       "I     0.0\n",
       "J     0.0\n",
       "K     0.1\n",
       "L     0.1\n",
       "M     0.1\n",
       "N     0.1\n",
       "O     0.1\n",
       "P     0.1\n",
       "Q     0.1\n",
       "R     0.1\n",
       "S     0.1\n",
       "T2    0.1\n",
       "dtype: float64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# General TD parameters\n",
    "gamma = 1.0\n",
    "learning_rate = 0.1\n",
    "initial_value = 0.0\n",
    "\n",
    "# N-step TD parameters\n",
    "n = 10\n",
    "value_function = {state_key: initial_value for state_key in game.states}\n",
    "\n",
    "# Initialize agent\n",
    "state_key = game.generate_state_key(game.state, role)\n",
    "prev_states = [state_key]\n",
    "prev_rewards = [None]\n",
    "\n",
    "# Environment setup\n",
    "game = RandomWalkGame(size=19, terminal_rewards={'T1': -1.0, 'T2': 1.0})\n",
    "actions = ['r']*10\n",
    "role = 1\n",
    "\n",
    "t = 0\n",
    "while not game.game_over:\n",
    "\n",
    "    # Behaviour policy\n",
    "    move = (role, actions[t])\n",
    "    \n",
    "    # Save chosen after-state for learning updates later\n",
    "    next_state = game.next_state(game.state, move)\n",
    "    next_state_key = game.generate_state_key(next_state, role)\n",
    "\n",
    "    game.make_move(move)\n",
    "    assert game.state == next_state\n",
    "    \n",
    "    # Get rewards\n",
    "    if not game.game_over:\n",
    "        reward = game.get_rewards()[role]\n",
    "    else:\n",
    "        reward = game.get_terminal_rewards()[role]\n",
    "\n",
    "    state_key = game.generate_state_key(game.state, role)\n",
    "    prev_states.append(state_key)\n",
    "    prev_rewards.append(reward)\n",
    "\n",
    "    if not game.game_over:\n",
    "        # Update the value for state in timestep tau\n",
    "        tau = t - n + 1\n",
    "        td_n_step_update(value_function, prev_states, prev_rewards, tau, gamma)\n",
    "\n",
    "    else:\n",
    "        # Complete final state-value updates for timesteps tau\n",
    "        # to current (terminal) timestep\n",
    "        T = t + 1\n",
    "        for tau in range(t - n + 1, T):\n",
    "            td_n_step_update(value_function, prev_states, prev_rewards, tau, gamma,\n",
    "                             t_max=T)\n",
    "\n",
    "    # Update timestep\n",
    "    t += 1\n",
    "\n",
    "\n",
    "assert game.game_over\n",
    "\n",
    "pd.Series(value_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) TD($\\lambda$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General TD parameters\n",
    "gamma = 1.0\n",
    "learning_rate = 0.1\n",
    "initial_value = 0.0\n",
    "\n",
    "# TD-Lambda parameters\n",
    "value_function = ValueApproxFunction(game)\n",
    "lam = 0.5\n",
    "\n",
    "# Initialize agent\n",
    "state_key = game.generate_state_key(game.state, role)\n",
    "prev_states = [state_key]\n",
    "prev_rewards = [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T1    0.000000\n",
       "A     0.223091\n",
       "B     0.227498\n",
       "C     0.231905\n",
       "D     0.236312\n",
       "E     0.240719\n",
       "F     0.245126\n",
       "G     0.249533\n",
       "H     0.253940\n",
       "I     0.258346\n",
       "J     0.262753\n",
       "K     0.267160\n",
       "L     0.271567\n",
       "M     0.275974\n",
       "N     0.280381\n",
       "O     0.284788\n",
       "P     0.289195\n",
       "Q     0.293602\n",
       "R     0.298008\n",
       "S     0.302415\n",
       "T2    0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Environment setup\n",
    "game = RandomWalkGame(size=19, terminal_rewards={'T1': -1.0, 'T2': 1.0})\n",
    "actions = ['r']*10\n",
    "role = 1\n",
    "\n",
    "t = 0\n",
    "while not game.game_over:\n",
    "\n",
    "    # Behaviour policy\n",
    "    move = (role, actions[t])\n",
    "    game.make_move(move)\n",
    "\n",
    "    # Get rewards\n",
    "    #reward = game.get_rewards()[1]\n",
    "    if not game.game_over:\n",
    "        reward = game.get_rewards()[role]\n",
    "    else:\n",
    "        reward = game.get_terminal_rewards()[role]\n",
    "\n",
    "    state = game.generate_state_key(game.state, role)\n",
    "    prev_state = prev_states[-1]\n",
    "    prev_states.append(state_key)\n",
    "    prev_rewards.append(reward)\n",
    "\n",
    "    if not game.game_over:\n",
    "        # Update the value for state in timestep tau\n",
    "        td_lambda_update(value_function, prev_state, state, reward, t, lam, gamma)\n",
    "\n",
    "    else:\n",
    "        # Complete state-value update for final timestep (to terminal state)\n",
    "        T = t + 1\n",
    "        td_lambda_update(value_function, prev_state, state, reward, T, lam, gamma)\n",
    "\n",
    "    # Update timestep\n",
    "    t += 1\n",
    "\n",
    "\n",
    "assert game.game_over\n",
    "\n",
    "values = {state: value_function.value(state) for state in game.states}\n",
    "pd.Series(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
