{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development of TD($\\lambda$) learner algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from platform import python_version\n",
    "python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on Chapter 12: Eligibility Traces from Sutton & Barto\n",
    "\n",
    "### TD($\\lambda$)\n",
    "\n",
    "<IMG SRC=\"images/tdlambda.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 7.1 Random walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from randomwalk import RandomWalkGame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = RandomWalkGame(size=19, terminal_rewards={'T1': -1.0, 'T2': 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1 _ _ _ _ _ _ _ _ _ J _ _ _ _ _ _ _ _ _ T2\n"
     ]
    }
   ],
   "source": [
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1 _ _ _ _ _ _ _ _ _ _ K _ _ _ _ _ _ _ _ T2\n"
     ]
    }
   ],
   "source": [
    "game.make_move((1, 'r'))\n",
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.get_rewards()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For verification purposes, we want to replicate the results below which should match Figure 12.8 from the book which looks like this:\n",
    "\n",
    "<IMG SRC=\"images/fig_12_8.png\">    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Step TD Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_sequence_generator(sequence, gamma):\n",
    "    \"\"\"Returns a generator that yields each item of sequence\n",
    "    discounted by gamma at each time-step.\n",
    "    \n",
    "    Example:\n",
    "    >>> list(discounted_sequence_generator([10.0]*5, 0.75))\n",
    "    [10.0, 7.5, 5.625, 4.21875, 3.1640625]\n",
    "    \"\"\"\n",
    "    \n",
    "    x = 1.0\n",
    "    for item in sequence:\n",
    "        yield x*item\n",
    "        x = x*gamma\n",
    "\n",
    "\n",
    "def td_n_step_update(value_function, prev_states, prev_rewards, tau, gamma,\n",
    "                     t_max=np.inf, show=False):\n",
    "    \"\"\"Updates the value in value_function for the past state\n",
    "    that occurred in timestep tau using the n-step TD\n",
    "    algorithm.\n",
    "    \n",
    "    Note:\n",
    "    tau = t - n + 1\n",
    "\n",
    "    Where:\n",
    "        t is the current timestep\n",
    "        n is the lambda value  \n",
    "\n",
    "    If tau < 0 no update is possible so none is made.\n",
    "\n",
    "    Args:\n",
    "        value_function (dict): Dictionary of state values.\n",
    "        prev_states (list): List of previous states.\n",
    "        prev_rewards (list): List of previous rewards.\n",
    "        tau (int): The timestep of the state that will have\n",
    "            its value updated.\n",
    "        gamma (float): Discount rate.\n",
    "        t_max (int or np.inf): The maximum timestep to include.  \n",
    "            Set to np.inf or a high number if you want to\n",
    "            include all timesteps from tau to tau + n.  Set\n",
    "            to t if the game has eneded.\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(prev_states) == len(prev_rewards)\n",
    "\n",
    "    if tau >= 0:\n",
    "\n",
    "        assert tau < len(prev_states), \"Not enough past states.\"\n",
    "\n",
    "        discounted_rewards = discounted_sequence_generator(\n",
    "            prev_rewards[tau+1:min(tau + n, t_max) + 1], \n",
    "            gamma\n",
    "        )\n",
    "        g = sum(list(discounted_rewards))\n",
    "\n",
    "        if tau + n < t_max:\n",
    "            g += gamma**n * value_function[prev_states[tau + n]]\n",
    "\n",
    "        # Update value of state at timestep tau\n",
    "        state_key = prev_states[tau]\n",
    "        state_value = value_function[state_key]\n",
    "\n",
    "        if show:\n",
    "            print(f\"prev_states[{tau}]: {state_value}\")\n",
    "\n",
    "        value_function[state_key] = state_value + \\\n",
    "            learning_rate * (g - state_value)\n",
    "\n",
    "        if show:\n",
    "            print(f\"value_function[{state_key.__repr__()}]: {value_function[state_key]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD($\\lambda$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A    0.000000\n",
       "B    0.055556\n",
       "C    0.111111\n",
       "D    0.166667\n",
       "E    0.222222\n",
       "F    0.277778\n",
       "G    0.333333\n",
       "H    0.388889\n",
       "I    0.444444\n",
       "J    0.500000\n",
       "K    0.555556\n",
       "L    0.611111\n",
       "M    0.666667\n",
       "N    0.722222\n",
       "O    0.777778\n",
       "P    0.833333\n",
       "Q    0.888889\n",
       "R    0.944444\n",
       "S    1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ValueApproxFunction():\n",
    "    \n",
    "    def __init__(self, game):\n",
    "\n",
    "        self.states = [s for s in game.states if not s in game.terminal_states]\n",
    "        self.n_states = len(self.states)\n",
    "        self.weights = np.zeros(2)  # Linear function approximator\n",
    "        self.z = np.full_like(self.weights, 0)  # Eligibility trace vector\n",
    "\n",
    "    def input_value(self, state_key):\n",
    "        \"\"\"Convert discrete state into a linear feature\n",
    "        value\n",
    "        \n",
    "        returns:\n",
    "            x (float): (0 <= x <= 1).\n",
    "        \"\"\"\n",
    "        return self.states.index(state_key) / (self.n_states - 1)\n",
    "    \n",
    "    def value(self, state_key):\n",
    "        \n",
    "        x = self.input_value(state_key)\n",
    "        return self.weights[0] + self.weights[1] * x\n",
    "\n",
    "    def dw(self, state_key):\n",
    "        \"\"\"Partial derivatives of value function w.r.t.\n",
    "        weights at given state.\"\"\"\n",
    "        \n",
    "        x = self.input_value(state_key)\n",
    "        return np.array([1.0, x])\n",
    "\n",
    "value_function = ValueApproxFunction(game)\n",
    "pd.Series({s: value_function.input_value(s) for s in value_function.states})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_function.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_function.value('J')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1. , 0.5])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_function.dw('J')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_lambda_update(value_function, prev_state, state, reward, z, t, lam, gamma, show=False):\n",
    "    \"\"\"Updates the value in value_function for the past state\n",
    "    using the TD-Lambda algorithm.\n",
    "    \n",
    "    Args:\n",
    "        value_function (ValueApproxFunction): Value function approximator.\n",
    "        state (str): State key.\n",
    "        reward (float): Reward value.\n",
    "        z (array): Eligibility trace vector.\n",
    "        t (int): The timestep of the state that will have\n",
    "            its value updated.\n",
    "        gamma (float): Discount factor.\n",
    "    \"\"\"\n",
    "\n",
    "    value_function.z = gamma * lam * value_function.z + value_function.dw(state_key)\n",
    "    td_error = reward + gamma * value_function.value(prev_state) - value_function.value(state)\n",
    "    value_function.weights += learning_rate * td_error * self.z\n",
    "\n",
    "    if show:\n",
    "        print(f\"value_function[{state_key.__repr__()}]: {state_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1 - Walk Right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) N-Step TD Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-bb774d4e554b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Update the value for state in timestep tau\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mtau\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mtd_n_step_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-d742e46a9c06>\u001b[0m in \u001b[0;36mtd_n_step_update\u001b[0;34m(value_function, prev_states, prev_rewards, tau, gamma, t_max, show)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtau\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mt_max\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalue_function\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprev_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtau\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# Update value of state at timestep tau\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# General TD parameters\n",
    "gamma = 1.0\n",
    "learning_rate = 0.1\n",
    "initial_value = 0.0\n",
    "\n",
    "# N-step TD parameters\n",
    "n = 9\n",
    "value_function = {state_key: initial_value for state_key in game.states}\n",
    "\n",
    "# Environment setup\n",
    "game = RandomWalkGame(size=19, terminal_rewards={'T1': -1.0, 'T2': 1.0})\n",
    "actions = ['r']*10\n",
    "role = 1\n",
    "\n",
    "prev_states = []\n",
    "prev_rewards = []\n",
    "t = 0\n",
    "while not game.game_over:\n",
    "\n",
    "    # Behaviour policy\n",
    "    move = (role, actions[t])\n",
    "    game.make_move(move)\n",
    "\n",
    "    # Get rewards\n",
    "    #reward = game.get_rewards()[1]\n",
    "    if not game.game_over:\n",
    "        reward = game.get_rewards()[role]\n",
    "    else:\n",
    "        reward = game.get_terminal_rewards()[role]\n",
    "\n",
    "    state_key = game.generate_state_key(game.state, role)\n",
    "    prev_states.append(state_key)\n",
    "    prev_rewards.append(reward)\n",
    "\n",
    "    if not game.game_over:\n",
    "        # Update the value for state in timestep tau\n",
    "        tau = t - n + 1\n",
    "        td_n_step_update(value_function, prev_states, prev_rewards, tau, gamma)\n",
    "\n",
    "    else:\n",
    "        # Complete final state-value updates for timesteps tau\n",
    "        # to current (terminal) timestep\n",
    "        T = t + 1\n",
    "        for tau in range(t - n + 1, T):\n",
    "            td_n_step_update(value_function, prev_states, prev_rewards, tau, gamma,\n",
    "                             t_max=T)\n",
    "\n",
    "    # Update timestep\n",
    "    t += 1\n",
    "\n",
    "\n",
    "assert game.game_over\n",
    "\n",
    "pd.Series(value_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) TD($\\lambda$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5529e0372f5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_state_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrole\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mprev_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprev_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mprev_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprev_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# General TD parameters\n",
    "gamma = 1.0\n",
    "learning_rate = 0.1\n",
    "initial_value = 0.0\n",
    "\n",
    "# TD-Lambda parameters\n",
    "value_function = ValueApproxFunction(game)\n",
    "lam = 0.5\n",
    "\n",
    "# Environment setup\n",
    "game = RandomWalkGame(size=19, terminal_rewards={'T1': -1.0, 'T2': 1.0})\n",
    "actions = ['r']*10\n",
    "role = 1\n",
    "\n",
    "prev_states = []\n",
    "prev_rewards = []\n",
    "t = 0\n",
    "while not game.game_over:\n",
    "\n",
    "    # Behaviour policy\n",
    "    move = (role, actions[t])\n",
    "    game.make_move(move)\n",
    "\n",
    "    # Get rewards\n",
    "    #reward = game.get_rewards()[1]\n",
    "    if not game.game_over:\n",
    "        reward = game.get_rewards()[role]\n",
    "    else:\n",
    "        reward = game.get_terminal_rewards()[role]\n",
    "\n",
    "    state = game.generate_state_key(game.state, role)\n",
    "    prev_state = prev_states[-1]\n",
    "    prev_states.append(state_key)\n",
    "    prev_rewards.append(reward)\n",
    "\n",
    "    if not game.game_over:\n",
    "        # Update the value for state in timestep tau\n",
    "        td_lambda_update(value_function, prev_state, state, reward, z, t, lam, gamma)\n",
    "\n",
    "    else:\n",
    "        # Complete state-value update for final timestep (to terminal state)\n",
    "        T = t + 1\n",
    "        td_lambda_update(value_function, prev_state, state, reward, z, T, lam, gamma)\n",
    "\n",
    "    # Update timestep\n",
    "    t += 1\n",
    "\n",
    "\n",
    "assert game.game_over\n",
    "\n",
    "pd.Series(value_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
