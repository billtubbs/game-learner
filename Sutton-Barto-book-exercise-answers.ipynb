{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sutton & Barto Reinforcement Learning Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.6'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from platform import python_version\n",
    "python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: Dynamic Programming\n",
    "\n",
    "## Example 4.1 Demonstration of iterative policy evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<IMG SRC=\"images/gridworld.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First set up the GridWorld environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \n",
    "    name = 'GridWorld'\n",
    "    roles = [1]\n",
    "    possible_n_players = [1]\n",
    "    help_text = {\n",
    "        'Move format': \"(row, col)\",\n",
    "        'Move not available': \"That action is not available.\",\n",
    "        'Number of players': \"This game is for 1 player.\"\n",
    "    }\n",
    "    actions = {\n",
    "        'l': (0, -1),\n",
    "        'u': (-1, 0),\n",
    "        'r': (0, 1),\n",
    "        'd': (1, 0)\n",
    "    }\n",
    "\n",
    "    def __init__(self, size=(4, 4), moves=None):\n",
    "\n",
    "        self.size = size\n",
    "\n",
    "        # Create all states\n",
    "        self.terminal_states = [(0, 0), (3, 3)]\n",
    "        self.states = [(row, col) for row in range(size[0])\n",
    "                       for col in range(size[1])\n",
    "                       if (row, col) not in self.terminal_states]\n",
    "\n",
    "        # Start in random state\n",
    "        self.start_state = random.choice(self.states)\n",
    "\n",
    "        # Store rewards for all possible possible rewards\n",
    "        self.rewards = {**{s: -1.0 for s in self.states}, \n",
    "                        **{s: 0.0 for s in self.terminal_states}}\n",
    "\n",
    "        # Store next state and reward for all possible\n",
    "        # state transitions\n",
    "        self.turn = self.roles[0]\n",
    "        self.transitions = {(s, a): (self.next_state(s, (self.turn, a)),\n",
    "                                     self.rewards[s])\n",
    "                            for s in self.states \n",
    "                            for a in self.actions}\n",
    "        self.n_players = 1\n",
    "        self.winner = None\n",
    "        self.game_over = False\n",
    "        self.reset()\n",
    "        if moves is not None:\n",
    "            for move in moves:\n",
    "                self.make_move(move)\n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        self.moves = []\n",
    "        self.state = self.start_state\n",
    "        self.winner = None\n",
    "        self.game_over = False\n",
    "\n",
    "    def show_state(self):\n",
    "\n",
    "        x = np.array(['_']*np.prod(game.size)).reshape(game.size)\n",
    "        i_terminal_states = np.array(game.terminal_states).T\n",
    "        x[i_terminal_states[0], i_terminal_states[1]] = 'X'\n",
    "        x[game.state] = 'O'\n",
    "        for row in x:\n",
    "            print(\" \".join(row))\n",
    "\n",
    "    def available_moves(self, state=None):\n",
    "\n",
    "        return self.actions\n",
    "\n",
    "    def update_state(self, move):\n",
    "        \n",
    "        self.state = self.next_state(self.state, move)\n",
    "\n",
    "    def next_state(self, state, move):\n",
    "\n",
    "        role, action = move\n",
    "\n",
    "        next_state = tuple(np.array(state) +\n",
    "                           np.array(self.actions[action]))\n",
    "\n",
    "        # Check if out of bounds\n",
    "        if (next_state[0] < 0) or (next_state[0] >= self.size[0]) or \\\n",
    "            (next_state[1] < 0) or (next_state[1] >= self.size[1]):\n",
    "            next_state = state\n",
    "\n",
    "        if state in self.terminal_states:\n",
    "            next_state = terminal_states[0]\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def make_move(self, move, show=False):\n",
    "        \n",
    "        assert self.game_over is False, \"Game is over\"\n",
    "\n",
    "        self.update_state(move)\n",
    "        self.moves.append(move)\n",
    "\n",
    "        if show:\n",
    "            role, action = move\n",
    "            print(\"Player %s made move %s\" % (str(role), str(action)))\n",
    "\n",
    "        self.check_if_game_over()\n",
    "\n",
    "    def get_rewards(self):\n",
    "        \"\"\"Returns reward resulting from the last action.\"\"\"\n",
    "\n",
    "        return {1: -1.0}\n",
    "\n",
    "    def get_terminal_rewards(self):\n",
    "        \"\"\"Returns the reward after the terminal state.\"\"\"\n",
    "\n",
    "        assert self.game_over\n",
    "\n",
    "        return {1: 0.0}   \n",
    "\n",
    "    def check_if_game_over(self):\n",
    "\n",
    "        if self.state in self.terminal_states:\n",
    "            self.game_over, self.winner = True, self.turn\n",
    "\n",
    "        return self.game_over\n",
    "\n",
    "    def generate_state_key(self, state, role):\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    def __repr__(self):\n",
    "\n",
    "        return \"GridWorld(%s)\" % self.size.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridWorld((4, 4))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game = GridWorld()\n",
    "game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1], [1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.roles, game.possible_n_players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l': (0, -1), 'u': (-1, 0), 'r': (0, 1), 'd': (1, 0)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.available_moves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, 1), -1.0),\n",
       " ((0, 2), -1.0),\n",
       " ((0, 3), -1.0),\n",
       " ((1, 0), -1.0),\n",
       " ((1, 1), -1.0),\n",
       " ((1, 2), -1.0),\n",
       " ((1, 3), -1.0),\n",
       " ((2, 0), -1.0),\n",
       " ((2, 1), -1.0),\n",
       " ((2, 2), -1.0),\n",
       " ((2, 3), -1.0),\n",
       " ((3, 0), -1.0),\n",
       " ((3, 1), -1.0),\n",
       " ((3, 2), -1.0),\n",
       " ((0, 0), 0.0),\n",
       " ((3, 3), 0.0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(game.rewards.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(((0, 1), 'l'), ((0, 0), -1.0)),\n",
       " (((0, 1), 'u'), ((0, 1), -1.0)),\n",
       " (((0, 1), 'r'), ((0, 2), -1.0)),\n",
       " (((0, 1), 'd'), ((1, 1), -1.0)),\n",
       " (((0, 2), 'l'), ((0, 1), -1.0)),\n",
       " (((0, 2), 'u'), ((0, 2), -1.0)),\n",
       " (((0, 2), 'r'), ((0, 3), -1.0)),\n",
       " (((0, 2), 'd'), ((1, 2), -1.0))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(game.transitions.items())[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X _ _ _\n",
      "_ _ _ _\n",
      "_ _ _ _\n",
      "_ _ O X\n"
     ]
    }
   ],
   "source": [
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X _ _ _\n",
      "_ _ _ _\n",
      "_ _ _ _\n",
      "_ _ _ O\n"
     ]
    }
   ],
   "source": [
    "game.make_move((1, 'r'))\n",
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: -1.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.get_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "while not game.game_over:\n",
    "    game.make_move((1, random.choice(list(game.actions.keys()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.get_terminal_rewards()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create value functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(0, 1): 0.0,\n",
       " (0, 2): 0.0,\n",
       " (0, 3): 0.0,\n",
       " (1, 0): 0.0,\n",
       " (1, 1): 0.0,\n",
       " (1, 2): 0.0,\n",
       " (1, 3): 0.0,\n",
       " (2, 0): 0.0,\n",
       " (2, 1): 0.0,\n",
       " (2, 2): 0.0,\n",
       " (2, 3): 0.0,\n",
       " (3, 0): 0.0,\n",
       " (3, 1): 0.0,\n",
       " (3, 2): 0.0,\n",
       " (0, 0): 0.0,\n",
       " (3, 3): 0.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# State value-function\n",
    "values = {s: 0.0 for s in game.states + game.terminal_states}\n",
    "print(\"Length:\", len(values))\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def show_values(values, decimals=1):\n",
    "\n",
    "    rows = max([k[0] for k, v in values.items()]) + 1\n",
    "    cols = max([k[1] for k, v in values.items()]) + 1\n",
    "    x = np.full((rows, cols), np.nan)\n",
    "    for k, v in values.items():\n",
    "        x[k] = round(v, decimals)\n",
    "    print(x)\n",
    "\n",
    "show_values(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 56\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(((0, 1), 'l'), 0.0),\n",
       " (((0, 1), 'u'), 0.0),\n",
       " (((0, 1), 'r'), 0.0),\n",
       " (((0, 1), 'd'), 0.0),\n",
       " (((0, 2), 'l'), 0.0),\n",
       " (((0, 2), 'u'), 0.0),\n",
       " (((0, 2), 'r'), 0.0),\n",
       " (((0, 2), 'd'), 0.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q (state, action) value function\n",
    "q_values = {}\n",
    "for s in game.states:\n",
    "    for a in game.actions:\n",
    "        q_values[(s, a)] = 0.0\n",
    "print(\"Length:\", len(q_values))\n",
    "list(q_values.items())[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def greedy_policy_with_state_values(game, state, action, values):\n",
    "    \"\"\"Returns the probability of taking the action in\n",
    "    given state for a greedy policy with given state value\n",
    "    function values.\"\"\"\n",
    "\n",
    "    actions = game.available_moves(state)\n",
    "\n",
    "    # Requires the game's state-transition matrix\n",
    "    action_values = {a: values[game.transitions[(state, a)][0]] \n",
    "                     for a in actions}\n",
    "    v_max = max(action_values.items(), key=lambda x: x[1])\n",
    "\n",
    "    return 1.0 if action == v_max[0] else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test above function\n",
    "state, action = (1, 1), 'u'\n",
    "greedy_policy_with_state_values(game, state, action, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def greedy_policy_with_q_values(game, state, action, q_values):\n",
    "    \"\"\"Returns the probability of taking the action in\n",
    "    given state for a greedy policy with given state-action\n",
    "    value function q_values.\"\"\"\n",
    "    \n",
    "    actions = game.available_moves(state)\n",
    "    action_values = {a: q_values[(state, a)] for a in actions}\n",
    "    v_max = max(action_values.items(), key=lambda x: x[1])\n",
    "\n",
    "    return 1.0 if action == v_max[0] else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test above function\n",
    "state, action = (1, 1), 'u'\n",
    "greedy_policy_with_q_values(game, state, action, q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_policy(game, state, action, values=None):\n",
    "\n",
    "    return 1.0/len(game.actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_action(game, policy, state, values):\n",
    "\n",
    "    actions = game.available_moves(state)\n",
    "    probs = [policy(game, state, a, values) for a in actions]\n",
    "\n",
    "    return np.random.choice(list(actions.keys()), p=probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['l', 'r', 'r', 'l', 'r', 'l', 'd', 'l']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test above function\n",
    "[policy_action(game, random_policy, (2, 2), values) for i in range(8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bellman_equation(game, policy, values, lr):\n",
    "    \"\"\"Updates the value function using the Bellman\n",
    "    equation (4.5).\"\"\"\n",
    "\n",
    "    v = values.copy()\n",
    "\n",
    "    for s in game.states:\n",
    "\n",
    "        actions = game.available_moves(s)\n",
    "        sum_values = 0.0\n",
    "\n",
    "        for a in actions:\n",
    "\n",
    "            p = policy(game, s, a, values)\n",
    "            s2, r = game.transitions[(s, a)]\n",
    "            if s2 in game.states:\n",
    "                sum_values += p*(r + lr*values[s2])\n",
    "            elif s2 in game.terminal_states:\n",
    "                sum_values += p*r\n",
    "        v[s] = sum_values\n",
    "\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, game, values, lr=1.0, theta=0.01, \n",
    "                    max_iter=1000, show=True, show_values=None):\n",
    "\n",
    "    iteration = 0\n",
    "    if show:\n",
    "        print(\"\\nk =\", iteration)\n",
    "        if show_values:\n",
    "            show_values(values)\n",
    "\n",
    "    while iteration < max_iter:\n",
    "\n",
    "        updated_values = bellman_equation(game, policy, values, lr=lr)\n",
    "\n",
    "        delta = np.abs(\n",
    "            np.array(list(updated_values.values())) -\n",
    "            np.array(list(values.values()))\n",
    "        ).max()\n",
    "        \n",
    "        values = updated_values\n",
    "        iteration += 1\n",
    "        \n",
    "        if show:\n",
    "            print(\"\\nk =\", iteration)\n",
    "            if show_values:\n",
    "                show_values(values)\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    if iteration == max_iter:\n",
    "        print(\"\\nMaximum iterations reached.\")\n",
    "    else:\n",
    "        print(\"\\nConverged to delta < %f after %d iterations\" % \n",
    "              (theta, iteration))\n",
    "    \n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Estimate optimal values using equi-probable random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values = {s: 0.0 for s in game.states + game.terminal_states}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "k = 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "k = 1\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "\n",
      "k = 2\n",
      "[[ 0.  -1.8 -2.  -2. ]\n",
      " [-1.8 -2.  -2.  -2. ]\n",
      " [-2.  -2.  -2.  -1.8]\n",
      " [-2.  -2.  -1.8  0. ]]\n",
      "\n",
      "k = 3\n",
      "[[ 0.  -2.4 -2.9 -3. ]\n",
      " [-2.4 -2.9 -3.  -2.9]\n",
      " [-2.9 -3.  -2.9 -2.4]\n",
      " [-3.  -2.9 -2.4  0. ]]\n",
      "\n",
      "k = 4\n",
      "[[ 0.  -3.1 -3.8 -4. ]\n",
      " [-3.1 -3.7 -3.9 -3.8]\n",
      " [-3.8 -3.9 -3.7 -3.1]\n",
      " [-4.  -3.8 -3.1  0. ]]\n",
      "\n",
      "k = 5\n",
      "[[ 0.  -3.7 -4.7 -4.9]\n",
      " [-3.7 -4.5 -4.8 -4.7]\n",
      " [-4.7 -4.8 -4.5 -3.7]\n",
      " [-4.9 -4.7 -3.7  0. ]]\n",
      "\n",
      "k = 6\n",
      "[[ 0.  -4.2 -5.5 -5.8]\n",
      " [-4.2 -5.2 -5.6 -5.5]\n",
      " [-5.5 -5.6 -5.2 -4.2]\n",
      " [-5.8 -5.5 -4.2  0. ]]\n",
      "\n",
      "k = 7\n",
      "[[ 0.  -4.7 -6.3 -6.7]\n",
      " [-4.7 -5.9 -6.4 -6.3]\n",
      " [-6.3 -6.4 -5.9 -4.7]\n",
      " [-6.7 -6.3 -4.7  0. ]]\n",
      "\n",
      "k = 8\n",
      "[[ 0.  -5.2 -7.  -7.5]\n",
      " [-5.2 -6.5 -7.1 -7. ]\n",
      " [-7.  -7.1 -6.5 -5.2]\n",
      " [-7.5 -7.  -5.2  0. ]]\n",
      "\n",
      "k = 9\n",
      "[[ 0.  -5.7 -7.7 -8.2]\n",
      " [-5.7 -7.2 -7.8 -7.7]\n",
      " [-7.7 -7.8 -7.2 -5.7]\n",
      " [-8.2 -7.7 -5.7  0. ]]\n",
      "\n",
      "k = 10\n",
      "[[ 0.  -6.1 -8.4 -9. ]\n",
      " [-6.1 -7.7 -8.4 -8.4]\n",
      " [-8.4 -8.4 -7.7 -6.1]\n",
      " [-9.  -8.4 -6.1  0. ]]\n",
      "\n",
      "Maximum iterations reached.\n"
     ]
    }
   ],
   "source": [
    "values = evaluate_policy(random_policy, game, values, max_iter=10, \n",
    "                         show=True, show_values=show_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 1): -6.137969970703125,\n",
       " (0, 2): -8.35235595703125,\n",
       " (0, 3): -8.967315673828125,\n",
       " (1, 0): -6.137969970703125,\n",
       " (1, 1): -7.737396240234375,\n",
       " (1, 2): -8.427825927734375,\n",
       " (1, 3): -8.35235595703125,\n",
       " (2, 0): -8.35235595703125,\n",
       " (2, 1): -8.427825927734375,\n",
       " (2, 2): -7.737396240234375,\n",
       " (2, 3): -6.137969970703125,\n",
       " (3, 0): -8.967315673828125,\n",
       " (3, 1): -8.35235595703125,\n",
       " (3, 2): -6.137969970703125,\n",
       " (0, 0): 0.0,\n",
       " (3, 3): 0.0}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([['_', 'l', 'l', 'l'],\n",
      "       ['u', 'l', 'l', 'd'],\n",
      "       ['u', 'u', 'r', 'd'],\n",
      "       ['u', 'r', 'r', '_']], dtype='<U1')\n"
     ]
    }
   ],
   "source": [
    "def show_actions(game, policy, values):\n",
    "\n",
    "    x = np.array(['_']*np.prod(game.size), dtype='<U1').reshape(game.size)\n",
    "    for s in game.states:\n",
    "        x[s] = policy_action(game, policy, s, values)\n",
    "    \n",
    "    print(x.__repr__())\n",
    "\n",
    "# Optimal actions\n",
    "show_actions(game, greedy_policy_with_state_values, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (0, 2),\n",
       " (0, 3),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (2, 0),\n",
       " (2, 1),\n",
       " (2, 2),\n",
       " (2, 3),\n",
       " (3, 0),\n",
       " (3, 1),\n",
       " (3, 2)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 1): 'l',\n",
       " (0, 2): 'l',\n",
       " (0, 3): 'ld',\n",
       " (1, 0): 'u',\n",
       " (1, 1): 'lu',\n",
       " (1, 2): 'lurd',\n",
       " (1, 3): 'd',\n",
       " (2, 0): 'u',\n",
       " (2, 1): 'lurd',\n",
       " (2, 2): 'rd',\n",
       " (2, 3): 'd',\n",
       " (3, 0): 'ur',\n",
       " (3, 1): 'r',\n",
       " (3, 2): 'r'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define true optimal actions so we can confirm\n",
    "# if a policy is optimal\n",
    "array_of_actions = np.array([\n",
    "    ['', 'l', 'l', 'ld'],\n",
    "    ['u', 'lu', 'lurd', 'd'],\n",
    "    ['u', 'lurd', 'rd', 'd'],\n",
    "    ['ur', 'r', 'r', '']\n",
    "], dtype='<U4')\n",
    "optimal_actions = {index: x for index, x in \n",
    "                   np.ndenumerate(array_of_actions)\n",
    "                   if index not in game.terminal_states}\n",
    "optimal_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_policy(game, policy, values, optimal_actions):\n",
    "    \"\"\"Returns an accuracy score for policy according to\n",
    "    how many optimal actions it makes in each state based\n",
    "    on optimal_actions.\n",
    "    \"\"\"\n",
    "\n",
    "    a_array = np.zeros(game.size, dtype=bool)\n",
    "    for s, opt in optimal_actions.items():\n",
    "        a = policy_action(game, policy, s, values)\n",
    "        if a in opt:\n",
    "            a_array[s] = True\n",
    "        else:\n",
    "            a_array[s] = False\n",
    "    \n",
    "    return a_array.sum()/len(game.states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_policy(game, greedy_policy_with_state_values, values, optimal_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Calculate Q-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def q_values_from_values(game, values, lr=1.0):\n",
    "\n",
    "    q_values = {}\n",
    "    for s in game.states:\n",
    "\n",
    "        sum_values = 0\n",
    "        actions = game.available_moves(s)\n",
    "        for a in actions:\n",
    "            next_state, reward = game.transitions[(s, a)]\n",
    "\n",
    "            # Q-value is value of next state + reward (?)\n",
    "            v_next = values[next_state] + reward\n",
    "            q_values[(s, a)] = v_next\n",
    "\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 0), -1.0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.transitions[((1, 0), 'l')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{((0, 1), 'l'): -1.0,\n",
       " ((0, 1), 'u'): -7.137969970703125,\n",
       " ((0, 1), 'r'): -9.35235595703125,\n",
       " ((0, 1), 'd'): -8.737396240234375,\n",
       " ((0, 2), 'l'): -7.137969970703125,\n",
       " ((0, 2), 'u'): -9.35235595703125,\n",
       " ((0, 2), 'r'): -9.967315673828125,\n",
       " ((0, 2), 'd'): -9.427825927734375,\n",
       " ((0, 3), 'l'): -9.35235595703125,\n",
       " ((0, 3), 'u'): -9.967315673828125,\n",
       " ((0, 3), 'r'): -9.967315673828125,\n",
       " ((0, 3), 'd'): -9.35235595703125,\n",
       " ((1, 0), 'l'): -7.137969970703125,\n",
       " ((1, 0), 'u'): -1.0,\n",
       " ((1, 0), 'r'): -8.737396240234375,\n",
       " ((1, 0), 'd'): -9.35235595703125,\n",
       " ((1, 1), 'l'): -7.137969970703125,\n",
       " ((1, 1), 'u'): -7.137969970703125,\n",
       " ((1, 1), 'r'): -9.427825927734375,\n",
       " ((1, 1), 'd'): -9.427825927734375,\n",
       " ((1, 2), 'l'): -8.737396240234375,\n",
       " ((1, 2), 'u'): -9.35235595703125,\n",
       " ((1, 2), 'r'): -9.35235595703125,\n",
       " ((1, 2), 'd'): -8.737396240234375,\n",
       " ((1, 3), 'l'): -9.427825927734375,\n",
       " ((1, 3), 'u'): -9.967315673828125,\n",
       " ((1, 3), 'r'): -9.35235595703125,\n",
       " ((1, 3), 'd'): -7.137969970703125,\n",
       " ((2, 0), 'l'): -9.35235595703125,\n",
       " ((2, 0), 'u'): -7.137969970703125,\n",
       " ((2, 0), 'r'): -9.427825927734375,\n",
       " ((2, 0), 'd'): -9.967315673828125,\n",
       " ((2, 1), 'l'): -9.35235595703125,\n",
       " ((2, 1), 'u'): -8.737396240234375,\n",
       " ((2, 1), 'r'): -8.737396240234375,\n",
       " ((2, 1), 'd'): -9.35235595703125,\n",
       " ((2, 2), 'l'): -9.427825927734375,\n",
       " ((2, 2), 'u'): -9.427825927734375,\n",
       " ((2, 2), 'r'): -7.137969970703125,\n",
       " ((2, 2), 'd'): -7.137969970703125,\n",
       " ((2, 3), 'l'): -8.737396240234375,\n",
       " ((2, 3), 'u'): -9.35235595703125,\n",
       " ((2, 3), 'r'): -7.137969970703125,\n",
       " ((2, 3), 'd'): -1.0,\n",
       " ((3, 0), 'l'): -9.967315673828125,\n",
       " ((3, 0), 'u'): -9.35235595703125,\n",
       " ((3, 0), 'r'): -9.35235595703125,\n",
       " ((3, 0), 'd'): -9.967315673828125,\n",
       " ((3, 1), 'l'): -9.967315673828125,\n",
       " ((3, 1), 'u'): -9.427825927734375,\n",
       " ((3, 1), 'r'): -7.137969970703125,\n",
       " ((3, 1), 'd'): -9.35235595703125,\n",
       " ((3, 2), 'l'): -9.35235595703125,\n",
       " ((3, 2), 'u'): -8.737396240234375,\n",
       " ((3, 2), 'r'): -1.0,\n",
       " ((3, 2), 'd'): -7.137969970703125}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values = q_values_from_values(game, values)\n",
    "q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_policy_with_q_values(game, (1, 0), 'r', q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_policy_with_q_values(game, (1, 0), 'u', q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_policy(game, greedy_policy_with_q_values, q_values, optimal_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1\n",
    "\n",
    "In Example 4.1, if $\\pi$ is the equiprobable random policy,\n",
    "\n",
    "- What is $q_\\pi(11,down)$?\n",
    "- What is $q_\\pi(7,down)$?\n",
    "\n",
    "$q_\\pi(11,down)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values[((2, 3), 'd')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$q_\\pi(7,down)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.137969970703125"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values[((1, 3), 'd')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4.2 Jackâ€™s Car Rental\n",
    "\n",
    "Jack manages two locations for a nationwide car rental company. Each day, some number of customers arrive at each location to rent cars. If Jack has a car available, he rents it out and is credited \\$10 by the national company. If he is out of cars at that location, then the business is lost. Cars become available for renting the day after they are returned. To help ensure that cars are available where they are needed, Jack can move them between the two locations overnight, at a cost of \\$2 per car moved. We assume that the number of cars requested and returned at each location are Poisson random variables, meaning that the probability that the number is\n",
    "$n$ is $\\frac{\\lambda^n}{n!}\\mathrm{e}^{-\\lambda}$, where $\\lambda$ is the expected number.\n",
    "\n",
    "Suppose $\\lambda$ is 3 and 4 for rental requests at the first and second locations and 3 and 2 for returns. To simplify the problem slightly, we assume that there can be no more than 20 cars at each location (any additional cars are returned to the nationwide company, and thus disappear from the problem) and a maximum of five cars can be moved from one location to the other in one night. We take the discount rate to be $\\gamma = 0.9$ and formulate this as a continuing finite MDP, where the time steps are days, the state is the number of cars at each location at the end of the day, and the actions are the net numbers of cars moved between the two locations overnight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import factorial\n",
    "lam1, lam2 = 3, 4\n",
    "\n",
    "def prob_request(n, lam):\n",
    "    \n",
    "    return lam**n/factorial(n)*np.exp(-lam)\n",
    "\n",
    "n_max = 20\n",
    "n = np.arange(0, n_max + 1)\n",
    "probs = prob_request(n, lam1), prob_request(n, lam2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot pdf\n",
    "ax = plt.subplot(111)\n",
    "ax.bar(n-0.2, probs[0], width=0.4, color='m', align='center', label=\"location 1\")\n",
    "ax.bar(n+0.2, probs[1], width=0.4, color='y', align='center', label=\"location 2\")\n",
    "\n",
    "plt.xlabel(\"Number of cars requested/returned\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "states = []\n",
    "for c1 in range(n_max + 1):\n",
    "    for c2 in range(n_max + 1):\n",
    "        states.append((c1, c2))\n",
    "print(states[0:3])\n",
    "len(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# State value-function\n",
    "values = {s: 0 for s in states}\n",
    "list(values.items())[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def available_actions(state):\n",
    "    \"\"\"Determine how many cars are available for moving.\n",
    "    Return as a list of number of cars movable.\n",
    "    \"\"\"\n",
    "    \n",
    "    s1, s2 = state\n",
    "    \n",
    "    actions = list(range(-min(s2, n_max - s1), \n",
    "                         min(s1, n_max - s2) + 1))\n",
    "    \n",
    "    return actions\n",
    "\n",
    "def next_state(state, policy, values):\n",
    "    \n",
    "    s1, s2 = state\n",
    "    \n",
    "    actions = available_actions(state)\n",
    "    cars_moved = policy_action(policy, state, actions, values)\n",
    "\n",
    "    cars_rented = [np.random.choice(n, p=p) for p in probs]\n",
    "    cars_returned = [np.random.choice(n, p=p) for p in probs]\n",
    "\n",
    "    for i in range(2):\n",
    "        \n",
    "        # Rent cars if available\n",
    "        cars_rented[i] = min(state[i], cars_rented[i])\n",
    "        \n",
    "        # Return cars if space available\n",
    "        cars_returned[i] = min(n_max - state[i], cars_returned[i])\n",
    "    \n",
    "    s1 = s1 + cars_rented[0] - cars_returned[0] - cars_moved\n",
    "    s2 = s2 + cars_rented[1] - cars_returned[1] + cars_moved\n",
    "\n",
    "    cost = 2*cars_moved - 10*sum(cars_rented)\n",
    "    \n",
    "    return (s1, s2), cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "next_state((10, 10), random_policy, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "states[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_values(values, r=1):\n",
    "    \"\"\"Displays the values rounded to r decimal places.\n",
    "    \"\"\"\n",
    "    \n",
    "    v_array  = np.zeros((n_max+1, n_max+1))\n",
    "    for s, v in values.items():\n",
    "        v_array[s] = v\n",
    "    print(v_array.round(r))\n",
    "\n",
    "show_values(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: This is not working...\n",
    "#evaluate_policy(random_policy, states, actions, transitions=None,  \n",
    "#                values=values, lr=1.0, theta=0.01, max_iter=10,\n",
    "#                show=True, show_values=show_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6: TD Learning\n",
    "\n",
    "### Example 6.2 Random walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gamelearner import GameController, RandomPlayer, TDLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomWalkGame():\n",
    "    \n",
    "    name = 'Random Walk'\n",
    "    terminal_states = ['T1', 'T2']\n",
    "    roles = [1]\n",
    "    possible_n_players = [1]\n",
    "    help_text = {\n",
    "        'Move format': \"l/r\",\n",
    "        'Move not available': \"That action is not available.\",\n",
    "        'Number of players': \"This game is for 1 player.\"\n",
    "    }\n",
    "\n",
    "    def __init__(self, size=5, moves=None):\n",
    "        \n",
    "        self.size = size\n",
    "        assert 1 < size <= 26\n",
    "        \n",
    "        # Create states\n",
    "        self.states = [self.terminal_states[0]] + \\\n",
    "                       list(string.ascii_uppercase[:size]) + \\\n",
    "                       [self.terminal_states[1]]\n",
    "        \n",
    "        # Start in middle position\n",
    "        self.start_state = self.states[self.size//2 + 1]\n",
    "        self.rewards = {'T1': 0.0, 'T2': 1.0}\n",
    "        self.dynamics = {}\n",
    "        for i in range(1, self.size + 1):\n",
    "            s_left = self.states[i - 1]\n",
    "            s_right = self.states[i + 1]\n",
    "            self.dynamics[self.states[i]] = {'l': s_left, 'r': s_right}\n",
    "        self.n_players = 1\n",
    "        self.turn = 1\n",
    "        self.winner = None\n",
    "        self.game_over = False\n",
    "        self.reset()\n",
    "        if moves is not None:\n",
    "            for move in moves:\n",
    "                self.make_move(move)\n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        self.moves = []\n",
    "        self.state = self.start_state\n",
    "        self.winner = None\n",
    "        self.game_over = False\n",
    "\n",
    "    def show_state(self):\n",
    "\n",
    "        print(self.state)\n",
    "\n",
    "    def available_moves(self, state=None):\n",
    "\n",
    "        if state is None:\n",
    "            state = self.state\n",
    "\n",
    "        return list(self.dynamics[state].keys())\n",
    "\n",
    "    def update_state(self, move):\n",
    "        \n",
    "        self.state = self.next_state(self.state, move)\n",
    "\n",
    "    def next_state(self, state, move):\n",
    "\n",
    "        role, action = move\n",
    "\n",
    "        return self.dynamics[state][action]\n",
    "\n",
    "    def make_move(self, move, show=False):\n",
    "        \n",
    "        assert self.game_over is False, \"Game is over\"\n",
    "\n",
    "        self.update_state(move)\n",
    "        self.moves.append(move)\n",
    "\n",
    "        if show:\n",
    "            role, action = move\n",
    "            print(\"Player %s made move %s\" % (str(role), str(action)))\n",
    "\n",
    "        self.check_if_game_over()\n",
    "\n",
    "    def get_rewards(self):\n",
    "        \"\"\"Returns any rewards at the current time step.  In\n",
    "        RandomWalk, there are no rewards until the end of the\n",
    "        game so send a zero reward.\"\"\"\n",
    "\n",
    "        return {1: 0.0}\n",
    "\n",
    "    def get_terminal_rewards(self):\n",
    "        \"\"\"Returns the reward after the terminal state was\n",
    "        reached.\"\"\"\n",
    "\n",
    "        assert self.game_over\n",
    "\n",
    "        return {1: self.rewards.get(game.state, 0.0)}   \n",
    "\n",
    "    def check_if_game_over(self):\n",
    "\n",
    "        if self.state in self.terminal_states:\n",
    "            self.game_over, self.winner = True, 1\n",
    "\n",
    "        return self.game_over\n",
    "\n",
    "    def generate_state_key(self, state, role):\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    def __repr__(self):\n",
    "\n",
    "        return \"RandomWalkGame(%d)\" % self.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game = RandomWalkGame()\n",
    "game.possible_n_players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.make_move((1, 'r'), show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.get_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.make_move((1, 'r'))\n",
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.get_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.make_move((1, 'l'))\n",
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.make_move((1, 'r'))\n",
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.make_move((1, 'r'))\n",
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.game_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.get_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.get_terminal_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game = RandomWalkGame()\n",
    "player = RandomPlayer()\n",
    "ctrl = GameController(game, [player])\n",
    "ctrl.play(show=False)\n",
    "ctrl.announce_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.reset()\n",
    "player = TDLearner(initial_value=0.5)\n",
    "ctrl = GameController(game, [player])\n",
    "ctrl.play(show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD(0) Updates on Random Walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation (6.2)\n",
    "\n",
    "$V(S_t) \\leftarrow V(S_t) + \\alpha [R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = 5\n",
    "game = RandomWalkGame(size=size)\n",
    "gamma = 1.0\n",
    "learning_rate = 0.1\n",
    "\n",
    "initial_value = 0.5\n",
    "value_function = {s: initial_value for s in game.states}\n",
    "value_function['T1'] = 0.0\n",
    "value_function['T2'] = 0.0\n",
    "\n",
    "episodes_to_record = [0, 1, 10, 100]\n",
    "value_sets = {}\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "n_iters = 1001\n",
    "for iteration in range(n_iters):\n",
    "    \n",
    "    if iteration in episodes_to_record:\n",
    "        value_sets[iteration] = value_function.copy()\n",
    "\n",
    "    past_states = [game.state]\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # Behaviour policy\n",
    "        move = np.random.choice(game.available_moves())\n",
    "        game.make_move([1, move])\n",
    "        past_states.append(game.state)\n",
    "\n",
    "        reward = game.get_rewards()[1]\n",
    "\n",
    "        # TD(0) update\n",
    "        if not game.game_over:\n",
    "            value_function[past_states[-2]] += \\\n",
    "                    learning_rate*(\n",
    "                            reward + gamma*value_function[past_states[-1]] -\n",
    "                            value_function[past_states[-2]]\n",
    "                    )\n",
    "        else:\n",
    "            reward = game.get_terminal_rewards()[1]\n",
    "            value_function[past_states[-2]] += \\\n",
    "                learning_rate*(reward - value_function[past_states[-2]])\n",
    "            break\n",
    "\n",
    "    game.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "states = game.states[1:size+1]\n",
    "true_values = pd.Series([x*(1/(size+1)) for x in range(size+2)][1:size+1], \n",
    "                       index=states)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i, value_function in value_sets.items():\n",
    "    values = list(value_function.values())[1:size+1]\n",
    "    ax.plot(states, values, marker='o', label=str(i))\n",
    "ax.plot(true_values.index, true_values, linestyle='--', label='True values')\n",
    "plt.title('TD(0) Value Updates on Random Walk')\n",
    "ax.set_xlabel('State')\n",
    "ax.set_ylabel('Value')\n",
    "plt.legend()\n",
    "plt.savefig(\"random_walk_td0.pdf\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 7: n-step Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-step TD\n",
    "\n",
    "<IMG SRC=\"tdlambda.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 7.1 Random walk\n",
    "\n",
    "n-step TD Updates on Random Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = 19\n",
    "game = RandomWalkGame(size=size)\n",
    "\n",
    "# Change rewards for this exercise\n",
    "game.rewards = {'T1': -1.0, 'T2': 1.0}\n",
    "\n",
    "# Initialization of parameters\n",
    "gamma = 1.0\n",
    "learning_rate = 0.1\n",
    "initial_value = 0.0\n",
    "n = 5\n",
    "\n",
    "# Initialise value function\n",
    "value_function = {\n",
    "    s: 0.0 if s in game.terminal_states else initial_value\n",
    "    for s in game.states\n",
    "}\n",
    "\n",
    "episodes_to_record = [0, 1, 10, 100]\n",
    "value_sets = {}\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "n_iters = 101\n",
    "for iteration in range(n_iters):\n",
    "    \n",
    "    if iteration in episodes_to_record:\n",
    "        value_sets[iteration] = value_function.copy()\n",
    "\n",
    "    past_states = [game.state]\n",
    "    past_rewards = [None]\n",
    "    \n",
    "    T = 999999\n",
    "    t = 0\n",
    "    while True:\n",
    "\n",
    "        # Behaviour policy\n",
    "        if t < T:\n",
    "            move = np.random.choice(game.available_moves())\n",
    "            game.make_move([1, move])\n",
    "            past_states.append(game.state)\n",
    "            if not game.game_over:\n",
    "                reward = game.get_rewards()[1]\n",
    "            else:\n",
    "                reward = game.get_terminal_rewards()[1]\n",
    "            past_rewards.append(reward)\n",
    "            if game.game_over:\n",
    "                T = t + 1\n",
    "\n",
    "        #import pdb; pdb.set_trace()\n",
    "\n",
    "        # State to be updated\n",
    "        tau = t - n + 1\n",
    "        \n",
    "        # TD(Î») update\n",
    "        if tau >= 0:\n",
    "            g = 0\n",
    "            for i in range(tau + 1, min(tau + n, T) + 1):\n",
    "                g += gamma**(i - tau - 1)*past_rewards[i]\n",
    "            if tau + n < T:\n",
    "                g += gamma**n*value_function[past_states[tau + n]]\n",
    "\n",
    "            value_function[past_states[tau]] += \\\n",
    "                learning_rate*(g - value_function[past_states[tau]])\n",
    "        \n",
    "        t += 1\n",
    "        if tau == T - 1:\n",
    "            break\n",
    "\n",
    "    game.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "states = game.states[1:size+1]\n",
    "true_values = pd.Series([(2*x*(1/(size+1)) - 1)\n",
    "                         for x in range(size+2)][1:size+1], \n",
    "                        index=states)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i, value_function in value_sets.items():\n",
    "    values = list(value_function.values())[1:size+1]\n",
    "    ax.plot(states, values, marker='o', label=str(i))\n",
    "ax.plot(true_values.index, true_values, linestyle='--', \n",
    "        label='True values')\n",
    "plt.title('TD(%d) Value Updates on Random Walk' % n)\n",
    "ax.set_xlabel('State')\n",
    "ax.set_ylabel('Value')\n",
    "plt.legend()\n",
    "plt.savefig(\"random_walk_td%d_19.pdf\" % n)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(value_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMS error metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rms_error(x, true_values):\n",
    "    \"\"\"Root-mean-squared error in values\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.sqrt(np.sum((x - true_values)**2)/len(x))\n",
    "\n",
    "true_values = np.array(\n",
    "    [(game.rewards['T1']*(1 - x) + game.rewards['T2']*x)\n",
    "     for x in np.linspace(0.0, 1.0, size + 2)]\n",
    ")[1:size+1]\n",
    "\n",
    "values = np.array(list(value_sets[100].values()))[1:size+1]\n",
    "rms_error(values, true_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-step TD - effect of variation in n (and alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = 19\n",
    "game = RandomWalkGame(size=size)\n",
    "game.rewards = {'T1': -1.0, 'T2': 1.0}\n",
    "\n",
    "true_values = np.array(\n",
    "    [(game.rewards['T1']*(1 - x) + game.rewards['T2']*x) for x in np.linspace(0.0, 1.0, size + 2)]\n",
    ")[1:size+1]\n",
    "\n",
    "# Initialization of parameters\n",
    "initial_value = 0.0\n",
    "gamma = 1.0\n",
    "\n",
    "# Iterate over learning_rates and n_values\n",
    "learning_rates = np.linspace(0.01, 1.0, 100)\n",
    "n_values = [2**i for i in range(8)]\n",
    "\n",
    "rms_error_sets = {}\n",
    "\n",
    "# TODO: Should be repeating the test 100 times for each\n",
    "# pair of n, alpha values and averaging the results.\n",
    "for n in n_values:\n",
    "\n",
    "    rms_results = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "\n",
    "        # Initialise value function\n",
    "        value_function = {\n",
    "            s: 0.0 if s in game.terminal_states else initial_value\n",
    "            for s in game.states\n",
    "        }\n",
    "\n",
    "        np.random.seed(5)\n",
    "\n",
    "        n_iters = 10\n",
    "        for iteration in range(n_iters):\n",
    "\n",
    "            past_states = [game.state]\n",
    "            past_rewards = [None]\n",
    "\n",
    "            T = 999999\n",
    "            t = 0\n",
    "            while True:\n",
    "\n",
    "                # Behaviour policy\n",
    "                if t < T:\n",
    "                    move = np.random.choice(game.available_moves())\n",
    "                    game.make_move([1, move])\n",
    "                    past_states.append(game.state)\n",
    "                    if not game.game_over:\n",
    "                        reward = game.get_rewards()[1]\n",
    "                    else:\n",
    "                        reward = game.get_terminal_rewards()[1]\n",
    "                    past_rewards.append(reward)\n",
    "                    if game.game_over:\n",
    "                        T = t + 1\n",
    "\n",
    "                #import pdb; pdb.set_trace()\n",
    "\n",
    "                # State to be updated\n",
    "                tau = t - n + 1\n",
    "\n",
    "                # TD(Î») update\n",
    "                if tau >= 0:\n",
    "                    g = 0\n",
    "                    for i in range(tau + 1, min(tau + n, T) + 1):\n",
    "                        g += gamma**(i - tau - 1)*past_rewards[i]\n",
    "                    if tau + n < T:\n",
    "                        g += gamma**n*value_function[past_states[tau + n]]\n",
    "\n",
    "                    value_function[past_states[tau]] += \\\n",
    "                                learning_rate*(g - value_function[past_states[tau]])\n",
    "\n",
    "                t += 1\n",
    "                if tau == T - 1:\n",
    "                    break\n",
    "\n",
    "            game.reset()\n",
    "\n",
    "        values = np.array(list(value_function.values())[1:size+1])\n",
    "        rms_results.append(rms_error(values, true_values))\n",
    "\n",
    "    rms_error_sets[n] = rms_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot results\n",
    "states = game.states[1:size+1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for n, rms_errors in rms_error_sets.items():\n",
    "    ax.plot(learning_rates, rms_errors, label='n = %d' % n)\n",
    "plt.title('n-step TD value error on random walk (%d states)' % game.size)\n",
    "ax.set_xlabel('Learning rate (alpha)')\n",
    "ax.set_ylabel('RMS Error')\n",
    "ax.set_ylim(0.0, 0.55)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(\"random_walk_tdl_%d.pdf\" % game.size)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7 Real-Time Dynamic Programming\n",
    "\n",
    "### Exercise 8.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RandomStatesExperiment():\n",
    "\n",
    "    name = 'Random States Experiment'\n",
    "    roles = [1]\n",
    "    possible_n_players = [1]\n",
    "    help_text = {\n",
    "        'Move format': \"0 or 1\",\n",
    "        'Move not available': \"That action is not available.\",\n",
    "        'Number of players': \"This game is for 1 player.\"\n",
    "    }\n",
    "\n",
    "    def __init__(self, b, size=1000, moves=None, \n",
    "                 terminal_reward=0.0, seed=1):\n",
    "        \"\"\"Experiment described in section 8.6 of Sutton\n",
    "        and Barto book to test Trajectory Sampling\n",
    "        methods.\n",
    "\n",
    "        Args:\n",
    "            b (int): Branching factor.\n",
    "            size (int): Number of states.\n",
    "            moves (list): Optional. Provide a list of completed\n",
    "                moves. Each move should be a tuple of length 2\n",
    "                where the first item is the player role (1) and\n",
    "                the second is the action (0 or 1).\n",
    "            terminal_reward (float): Final reward when terminal\n",
    "                state is reached.\n",
    "            seed (int): Change this to produce a different\n",
    "                random environment.\n",
    "        \"\"\"\n",
    "\n",
    "        self.b = b\n",
    "        self.size = size\n",
    "        self.seed = seed\n",
    "\n",
    "        self.states = np.arange(0, size + 1)\n",
    "\n",
    "        # Last state is terminal state\n",
    "        self.terminal_states = [self.states[-1]]\n",
    "\n",
    "        # Initialize a random number generator for\n",
    "        # generating random state transitions and\n",
    "        # rewards\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "\n",
    "        # First, create random state transitions\n",
    "        self.branches = {}\n",
    "        for state in self.states:\n",
    "            self.branches[state] = {\n",
    "                0: self.rng.choice(self.states, size=b), \n",
    "                1: self.rng.choice(self.states, size=b)\n",
    "            }\n",
    "        self.start_state = self.rng.choice(self.states)\n",
    "        \n",
    "        self.terminal_reward = terminal_reward\n",
    "        \n",
    "        self.n_players = 1\n",
    "        self.turn = 1\n",
    "        self.winner = None\n",
    "        self.game_over = False\n",
    "        self.states = None\n",
    "        self.moves = None\n",
    "        self.reset()\n",
    "        if moves is not None:\n",
    "            for move in moves:\n",
    "                self.make_move(move)\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        self.states = []\n",
    "        self.moves = []\n",
    "        self.state = self.start_state\n",
    "        self.winner = None\n",
    "        self.game_over = False\n",
    "\n",
    "    def show_state(self):\n",
    "\n",
    "        print(self.state)\n",
    "\n",
    "    def available_moves(self):\n",
    "\n",
    "        return (0, 1)\n",
    "\n",
    "    def next_state(self, state, move):\n",
    "\n",
    "        if np.random.random() < 0.1:\n",
    "            next_state = self.terminal_states[0]\n",
    "        else:\n",
    "            role, action = move\n",
    "            next_state = np.random.choice(\n",
    "                self.branches[state][action]\n",
    "            )\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def update_state(self, move):\n",
    "\n",
    "        self.state = self.next_state(self.state, move)\n",
    "\n",
    "    def make_move(self, move, show=False):\n",
    "        \n",
    "        assert self.game_over is False, \"Game is over\"\n",
    "\n",
    "        self.states.append(self.state)\n",
    "        self.update_state(move)\n",
    "        self.moves.append(move)\n",
    "\n",
    "        if show:\n",
    "            role, action = move\n",
    "            print(\"Player %s made move %s\" % (str(role), str(action)))\n",
    "\n",
    "        self.check_if_game_over()\n",
    "\n",
    "    def get_rewards(self):\n",
    "        \"\"\"Returns the reward at the current time step.  In\n",
    "        this experiment, the reward of each transition is\n",
    "        different and selected from a Gaussian distribution.\n",
    "        Rather than store all the fixed rewards in memory,\n",
    "        we regenerate them deterministically when needed using\n",
    "        a seeded random number generator.\n",
    "        \"\"\"\n",
    "\n",
    "        # Use previous state + seed to set RNG state\n",
    "        # so that rewards are deterministic for any\n",
    "        # state transition\n",
    "        self.rng.seed(self.states[-1] + self.seed)\n",
    "        last_action = self.moves[-1][1]\n",
    "\n",
    "        # Return the reward for the last action taken\n",
    "        reward = self.rng.normal(size=self.b)[last_action]\n",
    "\n",
    "        return {1: reward}\n",
    "\n",
    "    def get_terminal_rewards(self):\n",
    "        \"\"\"Returns the reward after the terminal state was\n",
    "        reached.\"\"\"\n",
    "\n",
    "        assert self.game_over\n",
    "\n",
    "        return {1: 0.0}   \n",
    "\n",
    "    def check_if_game_over(self):\n",
    "\n",
    "        if self.state in self.terminal_states:\n",
    "            self.game_over, self.winner = True, 1\n",
    "\n",
    "        return self.game_over\n",
    "\n",
    "    def generate_state_key(self, state, role):\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    def __repr__(self):\n",
    "\n",
    "        return \"RandomWalkGame(%d)\" % self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "game = RandomStatesExperiment(b=5, size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.start_state, game.terminal_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.available_moves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find all possible next states with action 0\n",
    "next_states = set()\n",
    "for i in range(100):\n",
    "    next_states.add(game.next_state(game.state, (1, 0)))\n",
    "next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find all possible next states with action 1\n",
    "next_states = set()\n",
    "for i in range(100):\n",
    "    next_states.add(game.next_state(game.state, (1, 1)))\n",
    "next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.roles, game.turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.make_move([1, 0], show=True)\n",
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gamelearner import RandomPlayer, TDLearner, GameController\n",
    "td_player = TDLearner()\n",
    "td_player.learning_rate, td_player.off_policy_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.reset()\n",
    "ctrl = GameController(game, [td_player])\n",
    "ctrl.play(show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(td_player.value_function.items())[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "td_player.value_function[game.start_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
