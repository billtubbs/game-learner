{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sutton & Barto Reinforcement Learning Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.6'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from platform import python_version\n",
    "python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: Dynamic Programming\n",
    "\n",
    "## Example 4.1 Demonstration of iterative policy evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<IMG SRC=\"images/gridworld.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First set up the GridWorld environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \n",
    "    name = 'GridWorld'\n",
    "    roles = [1]\n",
    "    possible_n_players = [1]\n",
    "    help_text = {\n",
    "        'Move format': \"(row, col)\",\n",
    "        'Move not available': \"That action is not available.\",\n",
    "        'Number of players': \"This game is for 1 player.\"\n",
    "    }\n",
    "    actions = {\n",
    "        'l': (0, -1),\n",
    "        'u': (-1, 0),\n",
    "        'r': (0, 1),\n",
    "        'd': (1, 0)\n",
    "    }\n",
    "\n",
    "    def __init__(self, size=(4, 4), moves=None):\n",
    "\n",
    "        self.size = size\n",
    "\n",
    "        # Create all states\n",
    "        self.terminal_states = [(0, 0), (3, 3)]\n",
    "        self.states = [(row, col) for row in range(size[0])\n",
    "                       for col in range(size[1])\n",
    "                       if (row, col) not in self.terminal_states]\n",
    "\n",
    "        # Start in random state\n",
    "        self.start_state = random.choice(self.states)\n",
    "\n",
    "        # Store rewards for all possible possible rewards\n",
    "        self.rewards = {**{s: -1.0 for s in self.states}, \n",
    "                        **{s: 0.0 for s in self.terminal_states}}\n",
    "\n",
    "        # Store next state and reward for all possible\n",
    "        # state transitions\n",
    "        self.turn = self.roles[0]\n",
    "        self.transitions = {(s, a): (self.next_state(s, (self.turn, a)),\n",
    "                                     self.rewards[s])\n",
    "                            for s in self.states \n",
    "                            for a in self.actions}\n",
    "        self.n_players = 1\n",
    "        self.winner = None\n",
    "        self.game_over = False\n",
    "        self.reset()\n",
    "        if moves is not None:\n",
    "            for move in moves:\n",
    "                self.make_move(move)\n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        self.moves = []\n",
    "        self.state = self.start_state\n",
    "        self.winner = None\n",
    "        self.game_over = False\n",
    "\n",
    "    def show_state(self):\n",
    "\n",
    "        x = np.array(['_']*np.prod(game.size)).reshape(game.size)\n",
    "        i_terminal_states = np.array(game.terminal_states).T\n",
    "        x[i_terminal_states[0], i_terminal_states[1]] = 'X'\n",
    "        x[game.state] = 'O'\n",
    "        for row in x:\n",
    "            print(\" \".join(row))\n",
    "\n",
    "    def available_moves(self, state=None):\n",
    "\n",
    "        return list(self.actions.keys())\n",
    "\n",
    "    def update_state(self, move):\n",
    "        \n",
    "        self.state = self.next_state(self.state, move)\n",
    "\n",
    "    def next_state(self, state, move):\n",
    "\n",
    "        role, action = move\n",
    "\n",
    "        next_state = tuple(np.array(state) +\n",
    "                           np.array(self.actions[action]))\n",
    "\n",
    "        # Check if out of bounds\n",
    "        if (next_state[0] < 0) or (next_state[0] >= self.size[0]) or \\\n",
    "            (next_state[1] < 0) or (next_state[1] >= self.size[1]):\n",
    "            next_state = state\n",
    "\n",
    "        if state in self.terminal_states:\n",
    "            next_state = terminal_states[0]\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def make_move(self, move, show=False):\n",
    "        \n",
    "        assert self.game_over is False, \"Game is over\"\n",
    "\n",
    "        self.update_state(move)\n",
    "        self.moves.append(move)\n",
    "\n",
    "        if show:\n",
    "            role, action = move\n",
    "            print(\"Player %s made move %s\" % (str(role), str(action)))\n",
    "\n",
    "        self.check_if_game_over()\n",
    "\n",
    "    def get_rewards(self):\n",
    "        \"\"\"Returns reward resulting from the last action.\"\"\"\n",
    "\n",
    "        return {1: -1.0}\n",
    "\n",
    "    def get_terminal_rewards(self):\n",
    "        \"\"\"Returns the reward after the terminal state.\"\"\"\n",
    "\n",
    "        assert self.game_over\n",
    "\n",
    "        return {1: 0.0}   \n",
    "\n",
    "    def check_if_game_over(self):\n",
    "\n",
    "        if self.state in self.terminal_states:\n",
    "            self.game_over, self.winner = True, self.turn\n",
    "\n",
    "        return self.game_over\n",
    "\n",
    "    def generate_state_key(self, state, role):\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    def __repr__(self):\n",
    "\n",
    "        return \"GridWorld(%s)\" % self.size.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridWorld((4, 4))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game = GridWorld()\n",
    "game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1], [1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.roles, game.possible_n_players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['l', 'u', 'r', 'd']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.available_moves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, 1), -1.0),\n",
       " ((0, 2), -1.0),\n",
       " ((0, 3), -1.0),\n",
       " ((1, 0), -1.0),\n",
       " ((1, 1), -1.0),\n",
       " ((1, 2), -1.0),\n",
       " ((1, 3), -1.0),\n",
       " ((2, 0), -1.0),\n",
       " ((2, 1), -1.0),\n",
       " ((2, 2), -1.0),\n",
       " ((2, 3), -1.0),\n",
       " ((3, 0), -1.0),\n",
       " ((3, 1), -1.0),\n",
       " ((3, 2), -1.0),\n",
       " ((0, 0), 0.0),\n",
       " ((3, 3), 0.0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(game.rewards.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(((0, 1), 'l'), ((0, 0), -1.0)),\n",
       " (((0, 1), 'u'), ((0, 1), -1.0)),\n",
       " (((0, 1), 'r'), ((0, 2), -1.0)),\n",
       " (((0, 1), 'd'), ((1, 1), -1.0)),\n",
       " (((0, 2), 'l'), ((0, 1), -1.0)),\n",
       " (((0, 2), 'u'), ((0, 2), -1.0)),\n",
       " (((0, 2), 'r'), ((0, 3), -1.0)),\n",
       " (((0, 2), 'd'), ((1, 2), -1.0))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(game.transitions.items())[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X _ _ _\n",
      "_ _ _ O\n",
      "_ _ _ _\n",
      "_ _ _ X\n"
     ]
    }
   ],
   "source": [
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X _ _ _\n",
      "_ _ _ O\n",
      "_ _ _ _\n",
      "_ _ _ X\n"
     ]
    }
   ],
   "source": [
    "game.make_move((1, 'r'))\n",
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: -1.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.get_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "while not game.game_over:\n",
    "    game.make_move((1, random.choice(game.available_moves())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.get_terminal_rewards()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create value functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(0, 1): 0.0,\n",
       " (0, 2): 0.0,\n",
       " (0, 3): 0.0,\n",
       " (1, 0): 0.0,\n",
       " (1, 1): 0.0,\n",
       " (1, 2): 0.0,\n",
       " (1, 3): 0.0,\n",
       " (2, 0): 0.0,\n",
       " (2, 1): 0.0,\n",
       " (2, 2): 0.0,\n",
       " (2, 3): 0.0,\n",
       " (3, 0): 0.0,\n",
       " (3, 1): 0.0,\n",
       " (3, 2): 0.0,\n",
       " (0, 0): 0.0,\n",
       " (3, 3): 0.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# State value-function\n",
    "values = {s: 0.0 for s in game.states + game.terminal_states}\n",
    "print(\"Length:\", len(values))\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def show_values(values, decimals=1):\n",
    "\n",
    "    rows = max([k[0] for k, v in values.items()]) + 1\n",
    "    cols = max([k[1] for k, v in values.items()]) + 1\n",
    "    x = np.full((rows, cols), np.nan)\n",
    "    for k, v in values.items():\n",
    "        x[k] = round(v, decimals)\n",
    "    print(x)\n",
    "\n",
    "show_values(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 56\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(((0, 1), 'l'), 0.0),\n",
       " (((0, 1), 'u'), 0.0),\n",
       " (((0, 1), 'r'), 0.0),\n",
       " (((0, 1), 'd'), 0.0),\n",
       " (((0, 2), 'l'), 0.0),\n",
       " (((0, 2), 'u'), 0.0),\n",
       " (((0, 2), 'r'), 0.0),\n",
       " (((0, 2), 'd'), 0.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q (state, action) value function\n",
    "q_values = {}\n",
    "for s in game.states:\n",
    "    for a in game.actions:\n",
    "        q_values[(s, a)] = 0.0\n",
    "print(\"Length:\", len(q_values))\n",
    "list(q_values.items())[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def greedy_policy_with_state_values(game, state, action, values):\n",
    "    \"\"\"Returns the probability of taking the action in\n",
    "    given state for a greedy policy with given state value\n",
    "    function values.\"\"\"\n",
    "\n",
    "    actions = game.available_moves(state)\n",
    "\n",
    "    # Requires the game's state-transition matrix\n",
    "    action_values = {a: values[game.transitions[(state, a)][0]] \n",
    "                     for a in actions}\n",
    "    v_max = max(action_values.items(), key=lambda x: x[1])\n",
    "\n",
    "    return 1.0 if action == v_max[0] else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x144 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test above function\n",
    "state = (1, 1)\n",
    "actions = ['l', 'u', 'r', 'd']\n",
    "test_values = {\n",
    " (1, 0): 1.0,\n",
    " (0, 1): 7.0,\n",
    " (1, 2): 9.0,\n",
    " (2, 1): 8.0\n",
    "}\n",
    "pd.Series([greedy_policy_with_state_values(game, state, a, test_values)\n",
    "          for a in actions], index=actions).plot.bar(figsize=(3, 2))\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def greedy_policy_with_q_values(game, state, action, q_values):\n",
    "    \"\"\"Returns the probability of taking the action in\n",
    "    given state for a greedy policy with given state-action\n",
    "    value function q_values.\"\"\"\n",
    "    \n",
    "    actions = game.available_moves(state)\n",
    "    action_values = {a: q_values[(state, a)] for a in actions}\n",
    "    v_max = max(action_values.items(), key=lambda x: x[1])\n",
    "\n",
    "    return 1.0 if action == v_max[0] else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x144 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test above function\n",
    "state = (1, 1)\n",
    "actions = ['l', 'u', 'r', 'd']\n",
    "test_q_values = {\n",
    " ((1, 1), 'l'): 1.0,\n",
    " ((1, 1), 'u'): 7.0,\n",
    " ((1, 1), 'r'): 9.0,\n",
    " ((1, 1), 'd'): 8.0\n",
    "}\n",
    "pd.Series([greedy_policy_with_q_values(game, state, a, test_q_values)\n",
    "          for a in actions], index=actions).plot.bar(figsize=(3, 2))\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_policy(game, state, action, values=None):\n",
    "\n",
    "    return 1.0/len(game.actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x144 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test above function\n",
    "state = (1, 1)\n",
    "actions = ['l', 'u', 'r', 'd']\n",
    "pd.Series([random_policy(game, state, a)\n",
    "          for a in actions], index=actions).plot.bar(figsize=(3, 2))\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_action(game, policy, state, values):\n",
    "\n",
    "    actions = game.available_moves(state)\n",
    "    probs = [policy(game, state, a, values) for a in actions]\n",
    "\n",
    "    return np.random.choice(actions, p=probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['r', 'r', 'u', 'u', 'r', 'u', 'r', 'l']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test above function\n",
    "[policy_action(game, random_policy, (1, 1), values) for i in range(8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bellman_equation(game, policy, values, lr):\n",
    "    \"\"\"Updates the value function using the Bellman\n",
    "    equation (4.5).\"\"\"\n",
    "\n",
    "    v = values.copy()\n",
    "\n",
    "    for s in game.states:\n",
    "\n",
    "        actions = game.available_moves(s)\n",
    "        sum_values = 0.0\n",
    "\n",
    "        for a in actions:\n",
    "\n",
    "            p = policy(game, s, a, values)\n",
    "            s2, r = game.transitions[(s, a)]\n",
    "            if s2 in game.states:\n",
    "                sum_values += p*(r + lr*values[s2])\n",
    "            elif s2 in game.terminal_states:\n",
    "                sum_values += p*r\n",
    "        v[s] = sum_values\n",
    "\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, game, values, lr=1.0, theta=0.01, \n",
    "                    max_iter=1000, show=True, show_values=None):\n",
    "\n",
    "    iteration = 0\n",
    "    if show:\n",
    "        print(\"\\nk =\", iteration)\n",
    "        if show_values:\n",
    "            show_values(values)\n",
    "\n",
    "    while iteration < max_iter:\n",
    "\n",
    "        updated_values = bellman_equation(game, policy, values, lr=lr)\n",
    "\n",
    "        delta = np.abs(\n",
    "            np.array(list(updated_values.values())) -\n",
    "            np.array(list(values.values()))\n",
    "        ).max()\n",
    "        \n",
    "        values = updated_values\n",
    "        iteration += 1\n",
    "        \n",
    "        if show:\n",
    "            print(\"\\nk =\", iteration)\n",
    "            if show_values:\n",
    "                show_values(values)\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    if iteration == max_iter:\n",
    "        print(\"\\nMaximum iterations reached.\")\n",
    "    else:\n",
    "        print(\"\\nConverged to delta < %f after %d iterations\" % \n",
    "              (theta, iteration))\n",
    "    \n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Estimate optimal values using equi-probable random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values = {s: 0.0 for s in game.states + game.terminal_states}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "k = 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "k = 1\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "\n",
      "k = 2\n",
      "[[ 0.  -1.8 -2.  -2. ]\n",
      " [-1.8 -2.  -2.  -2. ]\n",
      " [-2.  -2.  -2.  -1.8]\n",
      " [-2.  -2.  -1.8  0. ]]\n",
      "\n",
      "k = 3\n",
      "[[ 0.  -2.4 -2.9 -3. ]\n",
      " [-2.4 -2.9 -3.  -2.9]\n",
      " [-2.9 -3.  -2.9 -2.4]\n",
      " [-3.  -2.9 -2.4  0. ]]\n",
      "\n",
      "k = 4\n",
      "[[ 0.  -3.1 -3.8 -4. ]\n",
      " [-3.1 -3.7 -3.9 -3.8]\n",
      " [-3.8 -3.9 -3.7 -3.1]\n",
      " [-4.  -3.8 -3.1  0. ]]\n",
      "\n",
      "k = 5\n",
      "[[ 0.  -3.7 -4.7 -4.9]\n",
      " [-3.7 -4.5 -4.8 -4.7]\n",
      " [-4.7 -4.8 -4.5 -3.7]\n",
      " [-4.9 -4.7 -3.7  0. ]]\n",
      "\n",
      "k = 6\n",
      "[[ 0.  -4.2 -5.5 -5.8]\n",
      " [-4.2 -5.2 -5.6 -5.5]\n",
      " [-5.5 -5.6 -5.2 -4.2]\n",
      " [-5.8 -5.5 -4.2  0. ]]\n",
      "\n",
      "k = 7\n",
      "[[ 0.  -4.7 -6.3 -6.7]\n",
      " [-4.7 -5.9 -6.4 -6.3]\n",
      " [-6.3 -6.4 -5.9 -4.7]\n",
      " [-6.7 -6.3 -4.7  0. ]]\n",
      "\n",
      "k = 8\n",
      "[[ 0.  -5.2 -7.  -7.5]\n",
      " [-5.2 -6.5 -7.1 -7. ]\n",
      " [-7.  -7.1 -6.5 -5.2]\n",
      " [-7.5 -7.  -5.2  0. ]]\n",
      "\n",
      "k = 9\n",
      "[[ 0.  -5.7 -7.7 -8.2]\n",
      " [-5.7 -7.2 -7.8 -7.7]\n",
      " [-7.7 -7.8 -7.2 -5.7]\n",
      " [-8.2 -7.7 -5.7  0. ]]\n",
      "\n",
      "k = 10\n",
      "[[ 0.  -6.1 -8.4 -9. ]\n",
      " [-6.1 -7.7 -8.4 -8.4]\n",
      " [-8.4 -8.4 -7.7 -6.1]\n",
      " [-9.  -8.4 -6.1  0. ]]\n",
      "\n",
      "Maximum iterations reached.\n"
     ]
    }
   ],
   "source": [
    "values = evaluate_policy(random_policy, game, values, max_iter=10, \n",
    "                         show=True, show_values=show_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 1): -6.137969970703125,\n",
       " (0, 2): -8.35235595703125,\n",
       " (0, 3): -8.967315673828125,\n",
       " (1, 0): -6.137969970703125,\n",
       " (1, 1): -7.737396240234375,\n",
       " (1, 2): -8.427825927734375,\n",
       " (1, 3): -8.35235595703125,\n",
       " (2, 0): -8.35235595703125,\n",
       " (2, 1): -8.427825927734375,\n",
       " (2, 2): -7.737396240234375,\n",
       " (2, 3): -6.137969970703125,\n",
       " (3, 0): -8.967315673828125,\n",
       " (3, 1): -8.35235595703125,\n",
       " (3, 2): -6.137969970703125,\n",
       " (0, 0): 0.0,\n",
       " (3, 3): 0.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([['_', 'l', 'l', 'l'],\n",
      "       ['u', 'l', 'l', 'd'],\n",
      "       ['u', 'u', 'r', 'd'],\n",
      "       ['u', 'r', 'r', '_']], dtype='<U1')\n"
     ]
    }
   ],
   "source": [
    "def show_actions(game, policy, values):\n",
    "\n",
    "    x = np.array(['_']*np.prod(game.size), dtype='<U1').reshape(game.size)\n",
    "    for s in game.states:\n",
    "        x[s] = policy_action(game, policy, s, values)\n",
    "    \n",
    "    print(x.__repr__())\n",
    "\n",
    "# Optimal actions\n",
    "show_actions(game, greedy_policy_with_state_values, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (0, 2),\n",
       " (0, 3),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (2, 0),\n",
       " (2, 1),\n",
       " (2, 2),\n",
       " (2, 3),\n",
       " (3, 0),\n",
       " (3, 1),\n",
       " (3, 2)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 1): 'l',\n",
       " (0, 2): 'l',\n",
       " (0, 3): 'ld',\n",
       " (1, 0): 'u',\n",
       " (1, 1): 'lu',\n",
       " (1, 2): 'lurd',\n",
       " (1, 3): 'd',\n",
       " (2, 0): 'u',\n",
       " (2, 1): 'lurd',\n",
       " (2, 2): 'rd',\n",
       " (2, 3): 'd',\n",
       " (3, 0): 'ur',\n",
       " (3, 1): 'r',\n",
       " (3, 2): 'r'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define true optimal actions so we can confirm\n",
    "# if a policy is optimal\n",
    "array_of_actions = np.array([\n",
    "    ['', 'l', 'l', 'ld'],\n",
    "    ['u', 'lu', 'lurd', 'd'],\n",
    "    ['u', 'lurd', 'rd', 'd'],\n",
    "    ['ur', 'r', 'r', '']\n",
    "], dtype='<U4')\n",
    "optimal_actions = {index: x for index, x in \n",
    "                   np.ndenumerate(array_of_actions)\n",
    "                   if index not in game.terminal_states}\n",
    "optimal_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_policy(game, policy, values, optimal_actions):\n",
    "    \"\"\"Returns an accuracy score for policy according to\n",
    "    how many optimal actions it makes in each state based\n",
    "    on optimal_actions.\n",
    "    \"\"\"\n",
    "\n",
    "    a_array = np.zeros(game.size, dtype=bool)\n",
    "    for s, opt in optimal_actions.items():\n",
    "        a = policy_action(game, policy, s, values)\n",
    "        if a in opt:\n",
    "            a_array[s] = True\n",
    "        else:\n",
    "            a_array[s] = False\n",
    "    \n",
    "    return a_array.sum()/len(game.states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_policy(game, greedy_policy_with_state_values, values, optimal_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Calculate Q-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def q_values_from_values(game, values, lr=1.0):\n",
    "\n",
    "    q_values = {}\n",
    "    for s in game.states:\n",
    "\n",
    "        sum_values = 0\n",
    "        actions = game.available_moves(s)\n",
    "        for a in actions:\n",
    "            next_state, reward = game.transitions[(s, a)]\n",
    "\n",
    "            # Q-value is value of next state + reward (?)\n",
    "            v_next = values[next_state] + reward\n",
    "            q_values[(s, a)] = v_next\n",
    "\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 0), -1.0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.transitions[((1, 0), 'l')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{((0, 1), 'l'): -1.0,\n",
       " ((0, 1), 'u'): -7.137969970703125,\n",
       " ((0, 1), 'r'): -9.35235595703125,\n",
       " ((0, 1), 'd'): -8.737396240234375,\n",
       " ((0, 2), 'l'): -7.137969970703125,\n",
       " ((0, 2), 'u'): -9.35235595703125,\n",
       " ((0, 2), 'r'): -9.967315673828125,\n",
       " ((0, 2), 'd'): -9.427825927734375,\n",
       " ((0, 3), 'l'): -9.35235595703125,\n",
       " ((0, 3), 'u'): -9.967315673828125,\n",
       " ((0, 3), 'r'): -9.967315673828125,\n",
       " ((0, 3), 'd'): -9.35235595703125,\n",
       " ((1, 0), 'l'): -7.137969970703125,\n",
       " ((1, 0), 'u'): -1.0,\n",
       " ((1, 0), 'r'): -8.737396240234375,\n",
       " ((1, 0), 'd'): -9.35235595703125,\n",
       " ((1, 1), 'l'): -7.137969970703125,\n",
       " ((1, 1), 'u'): -7.137969970703125,\n",
       " ((1, 1), 'r'): -9.427825927734375,\n",
       " ((1, 1), 'd'): -9.427825927734375,\n",
       " ((1, 2), 'l'): -8.737396240234375,\n",
       " ((1, 2), 'u'): -9.35235595703125,\n",
       " ((1, 2), 'r'): -9.35235595703125,\n",
       " ((1, 2), 'd'): -8.737396240234375,\n",
       " ((1, 3), 'l'): -9.427825927734375,\n",
       " ((1, 3), 'u'): -9.967315673828125,\n",
       " ((1, 3), 'r'): -9.35235595703125,\n",
       " ((1, 3), 'd'): -7.137969970703125,\n",
       " ((2, 0), 'l'): -9.35235595703125,\n",
       " ((2, 0), 'u'): -7.137969970703125,\n",
       " ((2, 0), 'r'): -9.427825927734375,\n",
       " ((2, 0), 'd'): -9.967315673828125,\n",
       " ((2, 1), 'l'): -9.35235595703125,\n",
       " ((2, 1), 'u'): -8.737396240234375,\n",
       " ((2, 1), 'r'): -8.737396240234375,\n",
       " ((2, 1), 'd'): -9.35235595703125,\n",
       " ((2, 2), 'l'): -9.427825927734375,\n",
       " ((2, 2), 'u'): -9.427825927734375,\n",
       " ((2, 2), 'r'): -7.137969970703125,\n",
       " ((2, 2), 'd'): -7.137969970703125,\n",
       " ((2, 3), 'l'): -8.737396240234375,\n",
       " ((2, 3), 'u'): -9.35235595703125,\n",
       " ((2, 3), 'r'): -7.137969970703125,\n",
       " ((2, 3), 'd'): -1.0,\n",
       " ((3, 0), 'l'): -9.967315673828125,\n",
       " ((3, 0), 'u'): -9.35235595703125,\n",
       " ((3, 0), 'r'): -9.35235595703125,\n",
       " ((3, 0), 'd'): -9.967315673828125,\n",
       " ((3, 1), 'l'): -9.967315673828125,\n",
       " ((3, 1), 'u'): -9.427825927734375,\n",
       " ((3, 1), 'r'): -7.137969970703125,\n",
       " ((3, 1), 'd'): -9.35235595703125,\n",
       " ((3, 2), 'l'): -9.35235595703125,\n",
       " ((3, 2), 'u'): -8.737396240234375,\n",
       " ((3, 2), 'r'): -1.0,\n",
       " ((3, 2), 'd'): -7.137969970703125}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values = q_values_from_values(game, values)\n",
    "q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_policy_with_q_values(game, (1, 0), 'r', q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_policy_with_q_values(game, (1, 0), 'u', q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_policy(game, greedy_policy_with_q_values, q_values, optimal_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1\n",
    "\n",
    "In Example 4.1, if $\\pi$ is the equiprobable random policy,\n",
    "\n",
    "- What is $q_\\pi(11,down)$?\n",
    "- What is $q_\\pi(7,down)$?\n",
    "\n",
    "$q_\\pi(11,down)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values[((2, 3), 'd')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$q_\\pi(7,down)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.137969970703125"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values[((1, 3), 'd')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4.2 Jack’s Car Rental\n",
    "\n",
    "Jack manages two locations for a nationwide car rental company. Each day, some number of customers arrive at each location to rent cars. If Jack has a car available, he rents it out and is credited \\$10 by the national company. If he is out of cars at that location, then the business is lost. Cars become available for renting the day after they are returned. To help ensure that cars are available where they are needed, Jack can move them between the two locations overnight, at a cost of \\$2 per car moved. We assume that the number of cars requested and returned at each location are Poisson random variables, meaning that the probability that the number is\n",
    "$n$ is $\\frac{\\lambda^n}{n!}\\mathrm{e}^{-\\lambda}$, where $\\lambda$ is the expected number.\n",
    "\n",
    "Suppose $\\lambda$ is 3 and 4 for rental requests at the first and second locations and 3 and 2 for returns. To simplify the problem slightly, we assume that there can be no more than 20 cars at each location (any additional cars are returned to the nationwide company, and thus disappear from the problem) and a maximum of five cars can be moved from one location to the other in one night. We take the discount rate to be $\\gamma = 0.9$ and formulate this as a continuing finite MDP, where the time steps are days, the state is the number of cars at each location at the end of the day, and the actions are the net numbers of cars moved between the two locations overnight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import factorial\n",
    "lam1, lam2 = 3, 4\n",
    "\n",
    "def prob_request(n, lam):\n",
    "    \n",
    "    return lam**n/factorial(n)*np.exp(-lam)\n",
    "\n",
    "n_max = 20\n",
    "n = np.arange(0, n_max + 1)\n",
    "probs = prob_request(n, lam1), prob_request(n, lam2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot pdf\n",
    "ax = plt.subplot(111)\n",
    "ax.bar(n-0.2, probs[0], width=0.4, color='m', align='center', label=\"location 1\")\n",
    "ax.bar(n+0.2, probs[1], width=0.4, color='y', align='center', label=\"location 2\")\n",
    "\n",
    "plt.xlabel(\"Number of cars requested/returned\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 1), (0, 2)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "441"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states = []\n",
    "for c1 in range(n_max + 1):\n",
    "    for c2 in range(n_max + 1):\n",
    "        states.append((c1, c2))\n",
    "print(states[0:3])\n",
    "len(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, 0), 0), ((0, 1), 0), ((0, 2), 0)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# State value-function\n",
    "values = {s: 0 for s in states}\n",
    "list(values.items())[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def available_actions(state):\n",
    "    \"\"\"Determine how many cars are available for moving.\n",
    "    Return as a list of number of cars movable.\n",
    "    \"\"\"\n",
    "    \n",
    "    s1, s2 = state\n",
    "    \n",
    "    actions = list(range(-min(s2, n_max - s1), \n",
    "                         min(s1, n_max - s2) + 1))\n",
    "    \n",
    "    return actions\n",
    "\n",
    "def next_state(state, policy, values):\n",
    "    \n",
    "    s1, s2 = state\n",
    "    \n",
    "    actions = available_actions(state)\n",
    "    cars_moved = policy_action(policy, state, actions, values)\n",
    "\n",
    "    cars_rented = [np.random.choice(n, p=p) for p in probs]\n",
    "    cars_returned = [np.random.choice(n, p=p) for p in probs]\n",
    "\n",
    "    for i in range(2):\n",
    "        \n",
    "        # Rent cars if available\n",
    "        cars_rented[i] = min(state[i], cars_rented[i])\n",
    "        \n",
    "        # Return cars if space available\n",
    "        cars_returned[i] = min(n_max - state[i], cars_returned[i])\n",
    "    \n",
    "    s1 = s1 + cars_rented[0] - cars_returned[0] - cars_moved\n",
    "    s2 = s2 + cars_rented[1] - cars_returned[1] + cars_moved\n",
    "\n",
    "    cost = 2*cars_moved - 10*sum(cars_rented)\n",
    "    \n",
    "    return (s1, s2), cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'available_moves'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-2ac953903617>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnext_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-bfc4ef87cdb4>\u001b[0m in \u001b[0;36mnext_state\u001b[0;34m(state, policy, values)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavailable_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mcars_moved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mcars_rented\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-baff5f4c6e72>\u001b[0m in \u001b[0;36mpolicy_action\u001b[0;34m(game, policy, state, values)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpolicy_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavailable_moves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'available_moves'"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "next_state((10, 10), random_policy, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "states[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_values(values, r=1):\n",
    "    \"\"\"Displays the values rounded to r decimal places.\n",
    "    \"\"\"\n",
    "    \n",
    "    v_array  = np.zeros((n_max+1, n_max+1))\n",
    "    for s, v in values.items():\n",
    "        v_array[s] = v\n",
    "    print(v_array.round(r))\n",
    "\n",
    "show_values(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: This is not working...\n",
    "#evaluate_policy(random_policy, states, actions, transitions=None,  \n",
    "#                values=values, lr=1.0, theta=0.01, max_iter=10,\n",
    "#                show=True, show_values=show_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6: TD Learning\n",
    "\n",
    "### Example 6.2 Random walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gamelearner import GameController, RandomPlayer, TDLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomWalkGame():\n",
    "    \n",
    "    name = 'Random Walk'\n",
    "    terminal_states = ['T1', 'T2']\n",
    "    roles = [1]\n",
    "    possible_n_players = [1]\n",
    "    help_text = {\n",
    "        'Move format': \"l/r\",\n",
    "        'Move not available': \"That action is not available.\",\n",
    "        'Number of players': \"This game is for 1 player.\"\n",
    "    }\n",
    "\n",
    "    def __init__(self, size=5, moves=None):\n",
    "        \n",
    "        self.size = size\n",
    "        assert 1 < size <= 26\n",
    "        \n",
    "        # Create states\n",
    "        self.states = [self.terminal_states[0]] + \\\n",
    "                       list(string.ascii_uppercase[:size]) + \\\n",
    "                       [self.terminal_states[1]]\n",
    "        \n",
    "        # Start in middle position\n",
    "        self.start_state = self.states[self.size//2 + 1]\n",
    "        self.rewards = {'T1': 0.0, 'T2': 1.0}\n",
    "        self.dynamics = {}\n",
    "        for i in range(1, self.size + 1):\n",
    "            s_left = self.states[i - 1]\n",
    "            s_right = self.states[i + 1]\n",
    "            self.dynamics[self.states[i]] = {'l': s_left, 'r': s_right}\n",
    "        self.n_players = 1\n",
    "        self.turn = 1\n",
    "        self.winner = None\n",
    "        self.game_over = False\n",
    "        self.reset()\n",
    "        if moves is not None:\n",
    "            for move in moves:\n",
    "                self.make_move(move)\n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        self.moves = []\n",
    "        self.state = self.start_state\n",
    "        self.winner = None\n",
    "        self.game_over = False\n",
    "\n",
    "    def show_state(self):\n",
    "\n",
    "        print(self.state)\n",
    "\n",
    "    def available_moves(self, state=None):\n",
    "\n",
    "        if state is None:\n",
    "            state = self.state\n",
    "\n",
    "        return list(self.dynamics[state].keys())\n",
    "\n",
    "    def update_state(self, move):\n",
    "        \n",
    "        self.state = self.next_state(self.state, move)\n",
    "\n",
    "    def next_state(self, state, move):\n",
    "\n",
    "        role, action = move\n",
    "\n",
    "        return self.dynamics[state][action]\n",
    "\n",
    "    def make_move(self, move, show=False):\n",
    "        \n",
    "        assert self.game_over is False, \"Game is over\"\n",
    "\n",
    "        self.update_state(move)\n",
    "        self.moves.append(move)\n",
    "\n",
    "        if show:\n",
    "            role, action = move\n",
    "            print(\"Player %s made move %s\" % (str(role), str(action)))\n",
    "\n",
    "        self.check_if_game_over()\n",
    "\n",
    "    def get_rewards(self):\n",
    "        \"\"\"Returns any rewards at the current time step.  In\n",
    "        RandomWalk, there are no rewards until the end of the\n",
    "        game so send a zero reward.\"\"\"\n",
    "\n",
    "        return {1: 0.0}\n",
    "\n",
    "    def get_terminal_rewards(self):\n",
    "        \"\"\"Returns the reward after the terminal state was\n",
    "        reached.\"\"\"\n",
    "\n",
    "        assert self.game_over\n",
    "\n",
    "        return {1: self.rewards.get(game.state, 0.0)}   \n",
    "\n",
    "    def check_if_game_over(self):\n",
    "\n",
    "        if self.state in self.terminal_states:\n",
    "            self.game_over, self.winner = True, 1\n",
    "\n",
    "        return self.game_over\n",
    "\n",
    "    def generate_state_key(self, state, role):\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    def __repr__(self):\n",
    "\n",
    "        return \"RandomWalkGame(%d)\" % self.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game = RandomWalkGame()\n",
    "game.possible_n_players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.make_move((1, 'r'), show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.get_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.make_move((1, 'r'))\n",
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.get_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.make_move((1, 'l'))\n",
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.make_move((1, 'r'))\n",
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.make_move((1, 'r'))\n",
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.game_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.get_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.get_terminal_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game = RandomWalkGame()\n",
    "player = RandomPlayer()\n",
    "ctrl = GameController(game, [player])\n",
    "ctrl.play(show=False)\n",
    "ctrl.announce_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.reset()\n",
    "player = TDLearner(initial_value=0.5)\n",
    "ctrl = GameController(game, [player])\n",
    "ctrl.play(show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD(0) Updates on Random Walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation (6.2)\n",
    "\n",
    "$V(S_t) \\leftarrow V(S_t) + \\alpha [R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = 5\n",
    "game = RandomWalkGame(size=size)\n",
    "gamma = 1.0\n",
    "learning_rate = 0.1\n",
    "\n",
    "initial_value = 0.5\n",
    "value_function = {s: initial_value for s in game.states}\n",
    "value_function['T1'] = 0.0\n",
    "value_function['T2'] = 0.0\n",
    "\n",
    "episodes_to_record = [0, 1, 10, 100]\n",
    "value_sets = {}\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "n_iters = 1001\n",
    "for iteration in range(n_iters):\n",
    "    \n",
    "    if iteration in episodes_to_record:\n",
    "        value_sets[iteration] = value_function.copy()\n",
    "\n",
    "    past_states = [game.state]\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # Behaviour policy\n",
    "        move = np.random.choice(game.available_moves())\n",
    "        game.make_move([1, move])\n",
    "        past_states.append(game.state)\n",
    "\n",
    "        reward = game.get_rewards()[1]\n",
    "\n",
    "        # TD(0) update\n",
    "        if not game.game_over:\n",
    "            value_function[past_states[-2]] += \\\n",
    "                    learning_rate*(\n",
    "                            reward + gamma*value_function[past_states[-1]] -\n",
    "                            value_function[past_states[-2]]\n",
    "                    )\n",
    "        else:\n",
    "            reward = game.get_terminal_rewards()[1]\n",
    "            value_function[past_states[-2]] += \\\n",
    "                learning_rate*(reward - value_function[past_states[-2]])\n",
    "            break\n",
    "\n",
    "    game.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "states = game.states[1:size+1]\n",
    "true_values = pd.Series([x*(1/(size+1)) for x in range(size+2)][1:size+1], \n",
    "                       index=states)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i, value_function in value_sets.items():\n",
    "    values = list(value_function.values())[1:size+1]\n",
    "    ax.plot(states, values, marker='o', label=str(i))\n",
    "ax.plot(true_values.index, true_values, linestyle='--', label='True values')\n",
    "plt.title('TD(0) Value Updates on Random Walk')\n",
    "ax.set_xlabel('State')\n",
    "ax.set_ylabel('Value')\n",
    "plt.legend()\n",
    "plt.savefig(\"random_walk_td0.pdf\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 7: n-step Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-step TD\n",
    "\n",
    "<IMG SRC=\"tdlambda.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 7.1 Random walk\n",
    "\n",
    "n-step TD Updates on Random Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = 19\n",
    "game = RandomWalkGame(size=size)\n",
    "\n",
    "# Change rewards for this exercise\n",
    "game.rewards = {'T1': -1.0, 'T2': 1.0}\n",
    "\n",
    "# Initialization of parameters\n",
    "gamma = 1.0\n",
    "learning_rate = 0.1\n",
    "initial_value = 0.0\n",
    "n = 5\n",
    "\n",
    "# Initialise value function\n",
    "value_function = {\n",
    "    s: 0.0 if s in game.terminal_states else initial_value\n",
    "    for s in game.states\n",
    "}\n",
    "\n",
    "episodes_to_record = [0, 1, 10, 100]\n",
    "value_sets = {}\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "n_iters = 101\n",
    "for iteration in range(n_iters):\n",
    "    \n",
    "    if iteration in episodes_to_record:\n",
    "        value_sets[iteration] = value_function.copy()\n",
    "\n",
    "    past_states = [game.state]\n",
    "    past_rewards = [None]\n",
    "    \n",
    "    T = 999999\n",
    "    t = 0\n",
    "    while True:\n",
    "\n",
    "        # Behaviour policy\n",
    "        if t < T:\n",
    "            move = np.random.choice(game.available_moves())\n",
    "            game.make_move([1, move])\n",
    "            past_states.append(game.state)\n",
    "            if not game.game_over:\n",
    "                reward = game.get_rewards()[1]\n",
    "            else:\n",
    "                reward = game.get_terminal_rewards()[1]\n",
    "            past_rewards.append(reward)\n",
    "            if game.game_over:\n",
    "                T = t + 1\n",
    "\n",
    "        #import pdb; pdb.set_trace()\n",
    "\n",
    "        # State to be updated\n",
    "        tau = t - n + 1\n",
    "        \n",
    "        # TD(λ) update\n",
    "        if tau >= 0:\n",
    "            g = 0\n",
    "            for i in range(tau + 1, min(tau + n, T) + 1):\n",
    "                g += gamma**(i - tau - 1)*past_rewards[i]\n",
    "            if tau + n < T:\n",
    "                g += gamma**n*value_function[past_states[tau + n]]\n",
    "\n",
    "            value_function[past_states[tau]] += \\\n",
    "                learning_rate*(g - value_function[past_states[tau]])\n",
    "        \n",
    "        t += 1\n",
    "        if tau == T - 1:\n",
    "            break\n",
    "\n",
    "    game.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "states = game.states[1:size+1]\n",
    "true_values = pd.Series([(2*x*(1/(size+1)) - 1)\n",
    "                         for x in range(size+2)][1:size+1], \n",
    "                        index=states)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i, value_function in value_sets.items():\n",
    "    values = list(value_function.values())[1:size+1]\n",
    "    ax.plot(states, values, marker='o', label=str(i))\n",
    "ax.plot(true_values.index, true_values, linestyle='--', \n",
    "        label='True values')\n",
    "plt.title('TD(%d) Value Updates on Random Walk' % n)\n",
    "ax.set_xlabel('State')\n",
    "ax.set_ylabel('Value')\n",
    "plt.legend()\n",
    "plt.savefig(\"random_walk_td%d_19.pdf\" % n)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(value_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMS error metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rms_error(x, true_values):\n",
    "    \"\"\"Root-mean-squared error in values\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.sqrt(np.sum((x - true_values)**2)/len(x))\n",
    "\n",
    "true_values = np.array(\n",
    "    [(game.rewards['T1']*(1 - x) + game.rewards['T2']*x)\n",
    "     for x in np.linspace(0.0, 1.0, size + 2)]\n",
    ")[1:size+1]\n",
    "\n",
    "values = np.array(list(value_sets[100].values()))[1:size+1]\n",
    "rms_error(values, true_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-step TD - effect of variation in n (and alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = 19\n",
    "game = RandomWalkGame(size=size)\n",
    "game.rewards = {'T1': -1.0, 'T2': 1.0}\n",
    "\n",
    "true_values = np.array(\n",
    "    [(game.rewards['T1']*(1 - x) + game.rewards['T2']*x) for x in np.linspace(0.0, 1.0, size + 2)]\n",
    ")[1:size+1]\n",
    "\n",
    "# Initialization of parameters\n",
    "initial_value = 0.0\n",
    "gamma = 1.0\n",
    "\n",
    "# Iterate over learning_rates and n_values\n",
    "learning_rates = np.linspace(0.01, 1.0, 100)\n",
    "n_values = [2**i for i in range(8)]\n",
    "\n",
    "rms_error_sets = {}\n",
    "\n",
    "# TODO: Should be repeating the test 100 times for each\n",
    "# pair of n, alpha values and averaging the results.\n",
    "for n in n_values:\n",
    "\n",
    "    rms_results = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "\n",
    "        # Initialise value function\n",
    "        value_function = {\n",
    "            s: 0.0 if s in game.terminal_states else initial_value\n",
    "            for s in game.states\n",
    "        }\n",
    "\n",
    "        np.random.seed(5)\n",
    "\n",
    "        n_iters = 10\n",
    "        for iteration in range(n_iters):\n",
    "\n",
    "            past_states = [game.state]\n",
    "            past_rewards = [None]\n",
    "\n",
    "            T = 999999\n",
    "            t = 0\n",
    "            while True:\n",
    "\n",
    "                # Behaviour policy\n",
    "                if t < T:\n",
    "                    move = np.random.choice(game.available_moves())\n",
    "                    game.make_move([1, move])\n",
    "                    past_states.append(game.state)\n",
    "                    if not game.game_over:\n",
    "                        reward = game.get_rewards()[1]\n",
    "                    else:\n",
    "                        reward = game.get_terminal_rewards()[1]\n",
    "                    past_rewards.append(reward)\n",
    "                    if game.game_over:\n",
    "                        T = t + 1\n",
    "\n",
    "                #import pdb; pdb.set_trace()\n",
    "\n",
    "                # State to be updated\n",
    "                tau = t - n + 1\n",
    "\n",
    "                # TD(λ) update\n",
    "                if tau >= 0:\n",
    "                    g = 0\n",
    "                    for i in range(tau + 1, min(tau + n, T) + 1):\n",
    "                        g += gamma**(i - tau - 1)*past_rewards[i]\n",
    "                    if tau + n < T:\n",
    "                        g += gamma**n*value_function[past_states[tau + n]]\n",
    "\n",
    "                    value_function[past_states[tau]] += \\\n",
    "                                learning_rate*(g - value_function[past_states[tau]])\n",
    "\n",
    "                t += 1\n",
    "                if tau == T - 1:\n",
    "                    break\n",
    "\n",
    "            game.reset()\n",
    "\n",
    "        values = np.array(list(value_function.values())[1:size+1])\n",
    "        rms_results.append(rms_error(values, true_values))\n",
    "\n",
    "    rms_error_sets[n] = rms_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot results\n",
    "states = game.states[1:size+1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for n, rms_errors in rms_error_sets.items():\n",
    "    ax.plot(learning_rates, rms_errors, label='n = %d' % n)\n",
    "plt.title('n-step TD value error on random walk (%d states)' % game.size)\n",
    "ax.set_xlabel('Learning rate (alpha)')\n",
    "ax.set_ylabel('RMS Error')\n",
    "ax.set_ylim(0.0, 0.55)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(\"random_walk_tdl_%d.pdf\" % game.size)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7 Real-Time Dynamic Programming\n",
    "\n",
    "### Exercise 8.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RandomStatesExperiment():\n",
    "\n",
    "    name = 'Random States Experiment'\n",
    "    roles = [1]\n",
    "    possible_n_players = [1]\n",
    "    help_text = {\n",
    "        'Move format': \"0 or 1\",\n",
    "        'Move not available': \"That action is not available.\",\n",
    "        'Number of players': \"This game is for 1 player.\"\n",
    "    }\n",
    "\n",
    "    def __init__(self, b, size=1000, moves=None, \n",
    "                 terminal_reward=0.0, seed=1):\n",
    "        \"\"\"Experiment described in section 8.6 of Sutton\n",
    "        and Barto book to test Trajectory Sampling\n",
    "        methods.\n",
    "\n",
    "        Args:\n",
    "            b (int): Branching factor.\n",
    "            size (int): Number of states.\n",
    "            moves (list): Optional. Provide a list of completed\n",
    "                moves. Each move should be a tuple of length 2\n",
    "                where the first item is the player role (1) and\n",
    "                the second is the action (0 or 1).\n",
    "            terminal_reward (float): Final reward when terminal\n",
    "                state is reached.\n",
    "            seed (int): Change this to produce a different\n",
    "                random environment.\n",
    "        \"\"\"\n",
    "\n",
    "        self.b = b\n",
    "        self.size = size\n",
    "        self.seed = seed\n",
    "\n",
    "        self.states = np.arange(0, size + 1)\n",
    "\n",
    "        # Last state is terminal state\n",
    "        self.terminal_states = [self.states[-1]]\n",
    "\n",
    "        # Initialize a random number generator for\n",
    "        # generating random state transitions and\n",
    "        # rewards\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "\n",
    "        # First, create random state transitions\n",
    "        self.branches = {}\n",
    "        for state in self.states:\n",
    "            self.branches[state] = {\n",
    "                0: self.rng.choice(self.states, size=b), \n",
    "                1: self.rng.choice(self.states, size=b)\n",
    "            }\n",
    "        self.start_state = self.rng.choice(self.states)\n",
    "        \n",
    "        self.terminal_reward = terminal_reward\n",
    "        \n",
    "        self.n_players = 1\n",
    "        self.turn = 1\n",
    "        self.winner = None\n",
    "        self.game_over = False\n",
    "        self.states = None\n",
    "        self.moves = None\n",
    "        self.reset()\n",
    "        if moves is not None:\n",
    "            for move in moves:\n",
    "                self.make_move(move)\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        self.states = []\n",
    "        self.moves = []\n",
    "        self.state = self.start_state\n",
    "        self.winner = None\n",
    "        self.game_over = False\n",
    "\n",
    "    def show_state(self):\n",
    "\n",
    "        print(self.state)\n",
    "\n",
    "    def available_moves(self):\n",
    "\n",
    "        return (0, 1)\n",
    "\n",
    "    def next_state(self, state, move):\n",
    "\n",
    "        if np.random.random() < 0.1:\n",
    "            next_state = self.terminal_states[0]\n",
    "        else:\n",
    "            role, action = move\n",
    "            next_state = np.random.choice(\n",
    "                self.branches[state][action]\n",
    "            )\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def update_state(self, move):\n",
    "\n",
    "        self.state = self.next_state(self.state, move)\n",
    "\n",
    "    def make_move(self, move, show=False):\n",
    "        \n",
    "        assert self.game_over is False, \"Game is over\"\n",
    "\n",
    "        self.states.append(self.state)\n",
    "        self.update_state(move)\n",
    "        self.moves.append(move)\n",
    "\n",
    "        if show:\n",
    "            role, action = move\n",
    "            print(\"Player %s made move %s\" % (str(role), str(action)))\n",
    "\n",
    "        self.check_if_game_over()\n",
    "\n",
    "    def get_rewards(self):\n",
    "        \"\"\"Returns the reward at the current time step.  In\n",
    "        this experiment, the reward of each transition is\n",
    "        different and selected from a Gaussian distribution.\n",
    "        Rather than store all the fixed rewards in memory,\n",
    "        we regenerate them deterministically when needed using\n",
    "        a seeded random number generator.\n",
    "        \"\"\"\n",
    "\n",
    "        # Use previous state + seed to set RNG state\n",
    "        # so that rewards are deterministic for any\n",
    "        # state transition\n",
    "        self.rng.seed(self.states[-1] + self.seed)\n",
    "        last_action = self.moves[-1][1]\n",
    "\n",
    "        # Return the reward for the last action taken\n",
    "        reward = self.rng.normal(size=self.b)[last_action]\n",
    "\n",
    "        return {1: reward}\n",
    "\n",
    "    def get_terminal_rewards(self):\n",
    "        \"\"\"Returns the reward after the terminal state was\n",
    "        reached.\"\"\"\n",
    "\n",
    "        assert self.game_over\n",
    "\n",
    "        return {1: 0.0}   \n",
    "\n",
    "    def check_if_game_over(self):\n",
    "\n",
    "        if self.state in self.terminal_states:\n",
    "            self.game_over, self.winner = True, 1\n",
    "\n",
    "        return self.game_over\n",
    "\n",
    "    def generate_state_key(self, state, role):\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    def __repr__(self):\n",
    "\n",
    "        return \"RandomWalkGame(%d)\" % self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "game = RandomStatesExperiment(b=5, size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.start_state, game.terminal_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.available_moves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find all possible next states with action 0\n",
    "next_states = set()\n",
    "for i in range(100):\n",
    "    next_states.add(game.next_state(game.state, (1, 0)))\n",
    "next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find all possible next states with action 1\n",
    "next_states = set()\n",
    "for i in range(100):\n",
    "    next_states.add(game.next_state(game.state, (1, 1)))\n",
    "next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.roles, game.turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.make_move([1, 0], show=True)\n",
    "game.show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gamelearner import RandomPlayer, TDLearner, GameController\n",
    "td_player = TDLearner()\n",
    "td_player.learning_rate, td_player.off_policy_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game.reset()\n",
    "ctrl = GameController(game, [td_player])\n",
    "ctrl.play(show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(td_player.value_function.items())[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "td_player.value_function[game.start_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
